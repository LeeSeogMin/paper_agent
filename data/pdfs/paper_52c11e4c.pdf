<!DOCTYPE html>
<!--[if IE 8 ]>    <html class="ie8"> <![endif]-->
<!--[if IE 9 ]>    <html class="ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!-->
<html class="no-js" lang="en"> <!--<![endif]-->
<head>


    <!-- meta tags & title for page component -->
    <!-- system header start -->
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic? | Natural Language Processing | Cambridge Core</title>
    
    
      <script src="/core/vanilla/public/js/sentry.min.js"></script>
    
      <script>
        let isTerminated = false;
    
        window.addEventListener('pagehide', (event) => {
          isTerminated = !event.persisted;
        }, { capture: true });
    
        const EXTRA_KEY = "ROUTE_TO";
    
        Sentry.init({
          dsn: "https://074fed417c764caaafebad958d2c0c95@o1239501.ingest.sentry.io/6395238",
          release: "www.cambridge.org" + "@" + "unreadable",
          environment: "prod",
          ignoreErrors: [],
          integrations: [Sentry.moduleMetadataIntegration()],
          transport: Sentry.makeMultiplexedTransport(
            Sentry.makeFetchTransport,
            (args) => {
              const event = args.getEvent();
              if (
                event &&
                event.extra &&
                EXTRA_KEY in event.extra &&
                Array.isArray(event.extra[EXTRA_KEY])
              ) {
                return event.extra[EXTRA_KEY];
              }
              return [];
            },
          ),
          beforeSend: (event) => {
            if (isTerminated || (window.location.protocol !== 'http:' && window.location.protocol !== 'https:')) {
              return null;
            }
    
            if (event?.exception?.values?.[0].stacktrace?.frames) {
              const { frames } = event.exception.values[0].stacktrace;
    
              const routeTo = frames
                .filter((frame) => frame.module_metadata && frame.module_metadata.dsn)
                .map((v) => v.module_metadata)
                .slice(-1);
    
              if (routeTo.length) {
                event.extra = {
                  ...event.extra,
                  [EXTRA_KEY]: routeTo,
                };
              }
            }
    
            return event;
          },
        });
      </script>
    
    
    
    
    
    <!-- need to loop through block page defined header includes -->
    
    
    <!-- system header finish -->
    <meta property="og:site_name" content="Cambridge Core"/>
    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://www.cambridge.org/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"/>
    <meta property="og:title" content="Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic? | Natural Language Processing | Cambridge Core"/>
    <meta property="og:description" content="Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?"/>
    <meta property="og:image" content="https://static.cambridge.org/covers/NLP_0_0_0/natural_language processing.jpg?send-full-size-image&#x3D;true"/>
    
    
    
      <meta name="description" content="Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?">
      <link rel="canonical" href="https://www.cambridge.org/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30">
      <meta name="dc.identifier" content="doi:10.1017/nlp.2024.43">
    
        <meta name="citation_fulltext_world_readable" content="">
    
        <meta name="citation_journal_title"  content="Natural Language Processing">
        <meta name="citation_publisher"  content="Cambridge University Press">
        <meta name="citation_title"  content="Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?">
        <meta name="citation_author"  content="Vasudevan Nedumpozhimana">
        <meta name="citation_author_orcid"  content="0000-0001-5161-8925">
        <meta name="citation_author"  content="John D. Kelleher">
        <meta name="citation_online_date"  content="2024/10/25">
        <meta name="citation_firstpage"  content="1">
        <meta name="citation_lastpage"  content="29">
        <meta name="citation_issn"  content="2977-0424">
        <meta name="citation_keywords"  content="Semantics; topic modelling; idiom token identification; neural language model; transformer">
        <meta name="citation_pdf_url"  content="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0FFF33B18E284DAB8FE8DCF69A963A30/S2977042424000438a.pdf/div-class-title-topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic-div.pdf">
        <meta name="citation_reference"  content="citation_title=Probing for constituency structure in neural language models; citation_author=Arps, D.; citation_author=Samih, Y.; citation_author=Kallmeyer, L.; citation_author=Sajjad, H.; citation_publication_date=2022; citation_inbook=Findings of the Association for Computational Linguistics: EMNLP.; citation_firstpage=6738; citation_lastpage=6757">
        <meta name="citation_reference"  content="Balasubramanian, S., Jain, N., Jindal, G., Awasthi, A. and Sarawagi, S. (2020). What’s in a name? are BERT named entity representations just as good for any other name? In Proceedings of the 5th Workshop on Representation Learning for NLP, Online. Association for Computational Linguistics, pp. 205–214.">
        <meta name="citation_reference"  content="Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.">
        <meta name="citation_reference"  content="citation_title=Introduction to Natural Language Processing; citation_author=Eisenstein, J.; citation_publication_date=2019">
        <meta name="citation_reference"  content="Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.">
        <meta name="citation_reference"  content="citation_title=Bert &amp; family eat word salad: experiments with text understanding; citation_author=Gupta, A.; citation_author=Kvernadze, G.; citation_author=Srikumar, V.; citation_publication_date=2021; citation_journal_title=Proceedings of the AAAI Conference on Artificial Intelligence; citation_volume=35; citation_firstpage=12946; citation_lastpage=12954; citation_doi=10.1609/aaai.v35i14.17531">
        <meta name="citation_reference"  content="Haagsma, H., Bos, J. and Nissim, M. (2020). MAGPIE: A large corpus of potentially idiomatic expressions. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France: European Language Resources Association, pp. 279–287.">
        <meta name="citation_reference"  content="Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.">
        <meta name="citation_reference"  content="Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.">
        <meta name="citation_reference"  content="Kim, T., Choi, J., Edmiston, D. and goo Lee, S. (2020). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. ArXiv, abs/2002.00737.">
        <meta name="citation_reference"  content="citation_title=Skip-thought vectors; citation_author=Kiros, R.; citation_author=Zhu, Y.; citation_author=Salakhutdinov, R. R.; citation_author=Zemel, R.; citation_author=Urtasun, R.; citation_author=Torralba, A.; citation_author=Fidler, S.; citation_publication_date=2015; citation_inbook=Advances in Neural Information Processing Systems">
        <meta name="citation_reference"  content="Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.">
        <meta name="citation_reference"  content="Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. (2019b). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.">
        <meta name="citation_reference"  content="citation_title=Emergent linguistic structure in artificial neural networks trained by self-supervision; citation_author=Manning, C. D.; citation_author=Clark, K.; citation_author=Hewitt, J.; citation_author=Khandelwal, U.; citation_author=Levy, O.; citation_publication_date=2020; citation_journal_title=Proceedings of the National Academy of Sciences of the United States of America; citation_volume=117; citation_firstpage=30046; citation_lastpage=30054; citation_doi=10.1073/pnas.1907367117">
        <meta name="citation_reference"  content="Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.">
        <meta name="citation_reference"  content="citation_title=Scikit-learn: machine learning in python; citation_author=Pedregosa, F.; citation_author=Varoquaux, G.; citation_author=Gramfort, A.; citation_author=Michel, V.; citation_author=Thirion, B.; citation_author=Grisel, O.; citation_author=Blondel, M.; citation_author=Prettenhofer, P.; citation_author=Weiss, R.; citation_author=Dubourg, V.; citation_author=Vanderplas, J.; citation_author=Passos, A.; citation_author=Cournapeau, D.; citation_author=Brucher, M.; citation_author=Perrot, M.; citation_author=Duchesnay, E.; citation_publication_date=2011; citation_journal_title=Journal of Machine Learning Research; citation_volume=12; citation_firstpage=2825; citation_lastpage=2830">
        <meta name="citation_reference"  content="Ramisch, C., Cordeiro, S. R., Savary, A., Vincze, V., Barbu Mititelu, V., Bhatia, A., Buljan, M., Candito, M., Gantar, P., Giouli, V., Güngör, T., Hawwari, A., Iñurrieta, U., Kovalevskaitė, J., Krek, S., Lichte, T., Liebeskind, C., Monti, J., Parra Escartín, C., QasemiZadeh, B., Ramisch, R., Schneider, N., Stoyanova, I., Vaidya, A. and Walsh, A. (2018). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Santa Fe, New Mexico, USA: Association for Computational Linguistics, pp. 222–240.">
        <meta name="citation_reference"  content="citation_title=Visualizing and measuring the geometry of bert; citation_author=Reif, E.; citation_author=Yuan, A.; citation_author=Wattenberg, M.; citation_author=Viegas, F. B.; citation_author=Coenen, A.; citation_author=Pearce, A.; citation_author=Kim, B.; citation_publication_date=2019; citation_inbook=Advances in Neural Information Processing Systems">
        <meta name="citation_reference"  content="Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.">
        <meta name="citation_reference"  content="Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.">
        <meta name="citation_reference"  content="Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.">
        <meta name="citation_reference"  content="Devlin, J., Chang, M., Lee, K. and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. NAACL-HLT, pp. 4171–4186.">
        <meta name="citation_reference"  content="Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.">
        <meta name="citation_reference"  content="Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.">
        <meta name="citation_reference"  content="Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.">
        <meta name="citation_reference"  content="Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.">
        <meta name="citation_reference"  content="Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855.">
        <meta name="citation_reference"  content="citation_title=Foundations of Statistical Natural Language Processing; citation_author=Manning, C.; citation_author=Schutze, H.; citation_publication_date=1999">
        <meta name="citation_reference"  content="Mickus, T., Paperno, D., Constant, M. and van Deemter, K. (2020). What do you mean, BERT? In Proceedings of the Society for Computation in Linguistics., New York: Association for Computational Linguistics, pp. 279–290.">
        <meta name="citation_reference"  content="Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.">
        <meta name="citation_reference"  content="Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.">
        <meta name="citation_reference"  content="Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.">
        <meta name="citation_reference"  content="citation_title=Literal occurrences of multiword expressions: rare birds that cause a stir; citation_author=Savary, A.; citation_author=Cordeiro, S. R.; citation_author=Lichte, T.; citation_author=Ramisch, C.; citation_author=nurrieta, U. I.; citation_author=Giouli, V.; citation_publication_date=2019; citation_journal_title=The Prague Bulletin of Mathematical Linguistics; citation_volume=112; citation_firstpage=5; citation_lastpage=54; citation_doi=10.2478/pralin-2019-0001">
        <meta name="citation_reference"  content="Schneider, N., Hovy, D., Johannsen, A. and Carpuat, M. (2016). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California: Association for Computational Linguistics, pp. 546–559.">
        <meta name="citation_reference"  content="Cook, P., Fazly, A. and Stevenson, S. (2008). The VNC-tokens dataset. In Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), pp. 19–22.">
        <meta name="citation_reference"  content="citation_title=Unsupervised type and token identification of idiomatic expressions; citation_author=Fazly, A.; citation_author=Cook, P.; citation_author=Stevenson, S.; citation_publication_date=2009; citation_journal_title=Computational Linguistics; citation_volume=35; citation_firstpage=61; citation_lastpage=103; citation_doi=10.1162/coli.08-010-R1-07-048">
        <meta name="citation_reference"  content="citation_title=Automatic detection of idiomatic clauses; citation_author=Feldman, A.; citation_author=Peng, J.; citation_publication_date=2013; citation_inbook=Computational Linguistics and Intelligent Text Processing; citation_firstpage=435; citation_lastpage=446">
        <meta name="citation_reference"  content="Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.">
        <meta name="citation_reference"  content="Goldberg, Y. (2019). Assessing BERT’s Syntactic Abilities. CoRR, abs/1901.05287.">
        <meta name="citation_reference"  content="Hashempour, R. and Villavicencio, A. (2020). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In Proceedings of the Workshop on the Cognitive Aspects of the Lexicon, Online. Association for Computational Linguistics, pp. 72–80.">
        <meta name="citation_reference"  content="Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.">
        <meta name="citation_reference"  content="Li, L. and Sporleder, C. (2010b). Using Gaussian mixture models to detect figurative language in context. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California: Association for Computational Linguistics, pp. 297–300.">
        <meta name="citation_reference"  content="Pimentel, T., Valvoda, J., Stoehr, N. and Cotterell, R. (2022). The architectural bottleneck principle.">
        <meta name="citation_reference"  content="Rosa, R. and Mareček, D. (2019). Inducing syntactic trees from BERT representations, arXiv, abs/1906.11511.">
        <meta name="citation_reference"  content="Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.">
        <meta name="citation_reference"  content="Adi, Y., Kermany, E., Belinkov, Y., Lavi, O. and Goldberg, Y. (2017). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 5th International Conference on Learning Representations, ICLR. 2017, Toulon, France: Conference Track Proceedings, OpenReview.net.">
        <meta name="citation_reference"  content="Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.">
        <meta name="citation_reference"  content="Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.">
        <meta name="citation_reference"  content="citation_title=Survey: multiword expression processing: a survey; citation_author=Constant, M.; citation_author=Eryiğit, G.; citation_author=Monti, J.; citation_author=van der Plas, L.; citation_author=Ramisch, C.; citation_author=Rosner, M.; citation_author=Todirascu, A.; citation_publication_date=2017; citation_journal_title=Computational Linguistics; citation_volume=43; citation_firstpage=837; citation_lastpage=892; citation_doi=10.1162/COLI_a_00302">
        <meta name="citation_reference"  content="citation_title=Indexing by latent semantic analysis; citation_author=Deerwester, S. C.; citation_author=Dumais, S. T.; citation_author=Landauer, T. K.; citation_author=Furnas, G. W.; citation_author=Harshman, R. A.; citation_publication_date=1990; citation_journal_title=Journal of the American Society of Information Science; citation_volume=41; citation_firstpage=391; citation_lastpage=407; citation_doi=10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9">
        <meta name="citation_reference"  content="citation_title=What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models; citation_author=Ettinger, A.; citation_publication_date=2020; citation_journal_title=Transactions of the Association for Computational Linguistics; citation_volume=8; citation_firstpage=34; citation_lastpage=48; citation_doi=10.1162/tacl_a_00298">
        <meta name="citation_reference"  content="Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Minnesota: Association for Computational Linguistics, vol 1, pp. 4129–4138, (Long and Short Papers).">
        <meta name="citation_reference"  content="Jawahar, G., Sagot, B. and Seddah, D. (2019b). What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, Florence, Italy: Association for Computational Linguistics, vol 1, pp. 3651–3657.">
        <meta name="citation_reference"  content="citation_title=Capturing and measuring thematic relatedness; citation_author=Kacmajor, M.; citation_author=Kelleher, J. D.; citation_publication_date=2020; citation_journal_title=Language Resources and Evaluation; citation_volume=54; citation_firstpage=645; citation_lastpage=682; citation_doi=10.1007/s10579-019-09452-w">
        <meta name="citation_reference"  content="Li, L. and Sporleder, C. (2010a). Linguistic cues for distinguishing literal and non-literal usages. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 683–691.">
        <meta name="citation_reference"  content="Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.">
        <meta name="citation_reference"  content="citation_title=Shapley idioms: analysing BERT sentence embeddings for general idiom token identification; citation_author=Nedumpozhimana, V.; citation_author=Klubička, F.; citation_author=Kelleher, J. D.; citation_publication_date=2022; citation_journal_title=Frontiers in Artificial Intelligence; citation_volume=5; citation_doi=10.3389/frai.2022.813967">
        <meta name="citation_reference"  content="Niu, J., Lu, W. and Penn, G. (2022). Does BERT rediscover a classical NLP pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea: International Committee on Computational Linguistics, 3143–3153.">
        <meta name="citation_reference"  content="Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.">
        <meta name="citation_reference"  content="citation_title=Out of order: how important is the sequential order of words in a sentence in natural language understanding tasks?; citation_author=Pham, T.; citation_author=Bui, T.; citation_author=Mai, L.; citation_author=Nguyen, A.; citation_publication_date=2021; citation_inbook=Findings of the Association for Computational Linguistics; citation_firstpage=1145; citation_lastpage=1160">
        <meta name="citation_reference"  content="Ramisch, C., Savary, A., Guillaume, B., Waszczuk, J., Candito, M., Vaidya, A., Barbu Mititelu, V., Bhatia, A., Iñurrieta, U., Giouli, V., Güngör, T., Jiang, M., Lichte, T., Liebeskind, C., Monti, J., Ramisch, R., Stymne, S., Walsh, A. and Xu, H. (2020). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons, Association for Computational Linguistics, pp. 107–118.">
        <meta name="citation_reference"  content="citation_title=A primer in BERTology: what we know about how BERT works; citation_author=Rogers, A.; citation_author=Kovaleva, O.; citation_author=Rumshisky, A.; citation_publication_date=2020; citation_journal_title=Transactions of the Association for Computational Linguistics; citation_volume=8; citation_firstpage=842; citation_lastpage=866; citation_doi=10.1162/tacl_a_00349">
        <meta name="citation_reference"  content="Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.">
        <meta name="citation_reference"  content="Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.">
        <meta name="citation_reference"  content="Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA: Association for Computational Linguistics, pp. 1631–1642.">
        <meta name="citation_reference"  content="citation_title=Idioms in context: the idix corpus; citation_author=Sporleder, C.; citation_author=Li, L.; citation_author=Gorinski, P.; citation_author=Koch, X.; citation_publication_date=2010; citation_inbook=LREC">
        <meta name="citation_reference"  content="Wang, W., Bi, B., Yan, M., Wu, C., Xia, J., Bao, Z., Peng, L. and Si, L. (2020). Structbert: Incorporating language structures into pre-training for deep language understanding. In 8th International Conference on Learning Representations, ICLR, Ethiopia: Addis Ababa, OpenReview.net,">
        <meta name="citation_reference"  content="Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D. and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316.">
        <meta name="citation_reference"  content="Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence">
        <meta name="citation_reference"  content="Tayyar Madabushi, H., Gow-Smith, E., Garcia, M., Scarton, C., Idiart, M. and Villavicencio, A. (2022). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Seattle, United States. Association for Computational Linguistics, pp. 107–121.">
        <meta name="citation_reference"  content="Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.">
        <meta name="citation_reference"  content="Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.">
        <meta name="citation_abstract"  content="Transformer-based neural language models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models’ (BERT and RoBERTa’s) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call topic-aware probing. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers, but also that the facility of these models to distinguish idiomatic usage is primarily based on their ability to identify and encode topic. Furthermore, our analysis of these models’ performance on other standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for these models.">
        <meta name="citation_doi"  content="10.1017/nlp.2024.43">
    
    <link rel="alternate" href="/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30" hreflang="en" />
    
    

    <link rel="icon" href="/core/cambridge-core/public/images/favicon.ico" type="image/x-icon"/>
    <link rel="shortcut icon" href="/core/cambridge-core/public/images/favicon.ico" type="image/x-icon"/>
    <link href='//fonts.googleapis.com/css?family=Noto+Sans:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <!--[if (gte IE 10)|!(IE)]><!-->
    <link rel="stylesheet" href="/core/cambridge-core/public/css/app.css?version=v7.403.1"/>

      <link rel="dns-prefetch" href="https://usage.prod.aop.cambridge.org/v1/events">

    <link rel="stylesheet" href="/core/cambridge-core/public/bower_components/font-awesome/css/font-awesome.min.css"/>
    <script src="/core/system/public/bower_components/jquery/dist/jquery.min.js"></script>
    <script src="/core/system/public/bower_components/jquery-migrate-3/jquery-migrate.min.js"></script>

    <!--<![endif]-->

    <!-- IE8/9 Fixes -->
    <!-- Only supports version 1 of jquery -->
    <!-- Has a CSS limitation of 4096 directives per file, so we need to split it up into multiple files -->
    <!--[if lt IE 10]>
    <link rel="stylesheet" href="/core/cambridge-core/public/css/ie-8-9.css" />
    <script src="/core/cambridge-core/public/js/ie8-head.min.js"></script>
    <script src="/core/cambridge-core/public/js/jquery-1.11.2.min.js"></script>
    <![endif]-->


    <script src="/core/cambridge-core/public/bower_components/modernizr/modernizr.js"></script>

        <!-- Google Analytics -->
        <script>
            (function(a,s,y,n,c,h,i,d,e){s.className+=' '+y;h.start=1*new Date;
                h.end=i=function(){s.className=s.className.replace(RegExp(' ?'+y),'')};
                (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
            })(window,document.documentElement,'async-hide','dataLayer',4000,
                    {'GTM-PWNSR3B':true});
    
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    
            ga('create', 'UA-86516288-1', 'auto');
            ga('require', 'GTM-PWNSR3B');
            ga('send', 'pageview');
        </script>
        <!-- End Google Analytics -->
        <!-- Google Tag Manager -->
        <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
                j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.src=
                'https://www.googletagmanager.com/gtm.js?id='+i+dl;
                f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-NTX72TG');</script>
        <!-- End Google Tag Manager -->
        <!-- Site24x7 -->
        <script type="text/javascript">
        var rumMOKey='bd7888531793d453d3bdf2268eea537b';
        (function(){
        if(window.performance && window.performance.timing && window.performance.navigation) {
            var site24x7_rum_beacon=document.createElement('script');
            site24x7_rum_beacon.async=true;
            site24x7_rum_beacon.setAttribute('src','//static.site24x7rum.eu/beacon/site24x7rum-min.js?appKey='+rumMOKey);
            document.getElementsByTagName('head')[0].appendChild(site24x7_rum_beacon);
        }
        })(window)
        </script>
        <!-- Site24x7 -->

    

  <script>
    var AOP = AOP || {};
    AOP.uiLanguage = 'en' || 'en';
    AOP.uiLanguageSuggested = '';
    AOP.leaveTranslationSuggested = '';
    AOP.isMultilanguageEnabled = 'true';
    AOP.clientSideMultilingualString = '%5B%7B%22_id%22%3A%225a7c10d5ab0af516063ac94b%22%2C%22name%22%3A%22Copy%20and%20paste%20a%20formatted%20citation%20or%20use%20one%20of%20the%20options%20to%20export%20in%20your%20chosen%20format%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Copy%20and%20paste%20a%20formatted%20citation%20or%20use%20one%20of%20the%20options%20to%20export%20in%20your%20chosen%20format%22%2C%22fr%22%3A%22Copiez-collez%20la%20citation%20ou%20utilisez%20une%20des%20options%20ci-dessous%20pour%20l&#x27;exporter%20dans%20le%20format%20de%20votre%20choix%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac94c%22%2C%22name%22%3A%22Selected%20format%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Selected%20format%22%2C%22fr%22%3A%22Format%20s%C3%A9lectionn%C3%A9%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac94d%22%2C%22name%22%3A%22Change%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Change%22%2C%22fr%22%3A%22Modifier%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac950%22%2C%22name%22%3A%22Export%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Export%22%2C%22fr%22%3A%22Exporter%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac94f%22%2C%22name%22%3A%22Download%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Download%22%2C%22fr%22%3A%22T%C3%A9l%C3%A9charger%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac94e%22%2C%22name%22%3A%22Copy%20to%20clipboard%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Copy%20to%20clipboard%22%2C%22fr%22%3A%22Copier%20dans%20le%20presse-papier%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d6%22%2C%22name%22%3A%22Sep%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Sep%22%2C%22fr%22%3A%22Sep.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9cd%22%2C%22name%22%3A%22decline%20option%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22No%20thanks%22%2C%22fr%22%3A%22Non%20merci%20(view%20site%20in%20English)%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d7%22%2C%22name%22%3A%22Oct%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Oct%22%2C%22fr%22%3A%22Oct.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d2%22%2C%22name%22%3A%22May%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22May%22%2C%22fr%22%3A%22Mai%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9c5%22%2C%22name%22%3A%22Collapse%20list%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22Collapse%20list%22%2C%22fr%22%3A%22Replier%20la%20liste%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9dc%22%2C%22name%22%3A%22Sorted%20by%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Sorted%20by%22%2C%22fr%22%3A%22Tri%20par%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9df%22%2C%22name%22%3A%22Type%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Type%22%2C%22fr%22%3A%22Type%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e1%22%2C%22name%22%3A%22Publication%20date%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Publication%20date%22%2C%22fr%22%3A%22Date%20de%20publication%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9cf%22%2C%22name%22%3A%22Feb%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Feb%22%2C%22fr%22%3A%22F%C3%A9v.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9cc%22%2C%22name%22%3A%22accept%20option%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22Yes%2C%20please%20switch%20to%20%3Clang%3E%22%2C%22fr%22%3A%22Oui%2C%20voir%20le%20site%20en%20fran%C3%A7ais%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e0%22%2C%22name%22%3A%22Online%20publication%20date%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Online%20publication%20date%22%2C%22fr%22%3A%22Date%20de%20publication%20en%20ligne%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d3%22%2C%22name%22%3A%22Jun%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Jun%22%2C%22fr%22%3A%22Juin%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9de%22%2C%22name%22%3A%22Title%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Title%22%2C%22fr%22%3A%22Titre%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e2%22%2C%22name%22%3A%22View%20Altmetric%20attention%20score%20details%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20entry%22%2C%22category%22%3A%22alt%20text%22%2C%22en%22%3A%22View%20Altmetric%20attention%20score%20details%22%2C%22fr%22%3A%22Voir%20le%20d%C3%A9tail%20du%20score%20d&#x27;attention%20Altmetric%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9c9%22%2C%22name%22%3A%22language%20prompt%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22%3Cp%3EWe%20notice%20that%20%3Clang%3E%20is%20the%20default%20language%20in%20your%20browser%20settings.%20Would%20you%20like%20to%20view%20this%20site%20in%20%3Clang%3E%3F%3C%2Fp%3E%22%2C%22fr%22%3A%22%3Cp%3EVotre%20navigateur%20indique%20le%20fran%C3%A7ais%20comme%20langue%20par%20defaut.%20Souhaitez%20vous%20consulter%20le%20site%20en%20fran%C3%A7ais%3F%3C%2Fp%3E%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d4%22%2C%22name%22%3A%22Jul%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Jul%22%2C%22fr%22%3A%22Juil.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d8%22%2C%22name%22%3A%22Nov%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Nov%22%2C%22fr%22%3A%22Nov.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9c6%22%2C%22name%22%3A%22Expand%20full%20list%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22Expand%20full%20list%22%2C%22fr%22%3A%22D%C3%A9plier%20la%20liste%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d0%22%2C%22name%22%3A%22Mar%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Mar%22%2C%22fr%22%3A%22Mars%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d9%22%2C%22name%22%3A%22Dec%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Dec%22%2C%22fr%22%3A%22D%C3%A9c.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9cb%22%2C%22name%22%3A%22close%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22Close%22%2C%22fr%22%3A%22Fermer%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9c8%22%2C%22name%22%3A%22Hide%20all%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22Hide%20All%22%2C%22fr%22%3A%22Dissimuler%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9ca%22%2C%22name%22%3A%22leaving%20language%20prompt%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22%3Cp%3EThe%20%3Clang%3E%20version%20of%20Cambridge%20Core%20is%20not%20currently%20available%20through%20the%20entire%20site.%20The%20page%20you%20are%20visiting%20is%20not%20available%20in%20%3Clang%3E%20and%20will%20be%20displayed%20in%20English.%3C%2Fp%3E%22%2C%22fr%22%3A%22%3Cp%3ELa%20version%20fran%C3%A7aise%20de%20Cambridge%20Core%20n&#x27;est%20actuellement%20pas%20disponible%20%C3%A0%20travers%20l&#x27;int%C3%A9gralit%C3%A9%20du%20site.%20La%20page%20vers%20laquelle%20vous%20vous%20rendez%20n&#x27;est%20pas%20disponible%20en%20fran%C3%A7ais%20et%20sera%20affich%C3%A9e%20en%20anglais.%3C%2Fp%3E%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9db%22%2C%22name%22%3A%22Show%20fewer%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Refine%20listing%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Show%20fewer%22%2C%22fr%22%3A%22Afficher%20moins%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9dd%22%2C%22name%22%3A%22Page%2FArticle%20number%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Page%2FArticle%20number%22%2C%22fr%22%3A%22Page%2Fnum%C3%A9ro%20d%E2%80%99article%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9da%22%2C%22name%22%3A%22Show%20more%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Refine%20listing%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Show%20more%22%2C%22fr%22%3A%22Afficher%20plus%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d1%22%2C%22name%22%3A%22Apr%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Apr%22%2C%22fr%22%3A%22Avr.%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9d5%22%2C%22name%22%3A%22Aug%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Aug%22%2C%22fr%22%3A%22Ao%C3%BBt%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9c7%22%2C%22name%22%3A%22View%20all%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22View%20All%22%2C%22fr%22%3A%22Voir%20tout%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9ce%22%2C%22name%22%3A%22Jan%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Metrics%20tab%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Jan%22%2C%22fr%22%3A%22Jan%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e7%22%2C%22name%22%3A%22Search%20society%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22contextual%20search%22%2C%22category%22%3A%22searchbox%20placeholder%20text%22%2C%22en%22%3A%22Search%20%3CSocietyName%3E%22%2C%22fr%22%3A%22Rechercher%20dans%20%3CSocietyName%3E%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9f1%22%2C%22name%22%3A%22Show%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20eLetters%20tab%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Show%22%2C%22fr%22%3A%22Afficher%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2b%22%2C%22name%22%3A%22Anyone%20you%20share%20the%20following%20link%20with%20will%20be%20able%20to%20freely%20read%20this%20content.%20For%20more%20information%2C%20please%20view%20the%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Anyone%20you%20share%20the%20following%20link%20with%20will%20be%20able%20to%20freely%20read%20this%20content.%20For%20more%20information%2C%20please%20view%20the%22%2C%22fr%22%3A%22Tous%20ceux%20avec%20qui%20vous%20partagez%20ce%20lien%20obtiendront%20un%20acc%C3%A8s%20gratuit%20%C3%A0%20cet%20article.%20Pour%20plus%20d&#x27;information%2C%20veuillez%20svp%20consulter%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca30%22%2C%22name%22%3A%22The%20content%20link%20has%20been%20copied%20to%20your%20clipboard%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20view%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22The%20content%20link%20has%20been%20copied%20to%20your%20clipboard%22%2C%22fr%22%3A%22Le%20lien%20a%20%C3%A9t%C3%A9%20copi%C3%A9%20dans%20votre%20presse-papier%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2f%22%2C%22name%22%3A%22Cancel%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Cancel%22%2C%22fr%22%3A%22Annuler%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2e%22%2C%22name%22%3A%22Share%20via%20email%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Share%20via%20email%22%2C%22fr%22%3A%22Partager%20par%20e-mail%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e6%22%2C%22name%22%3A%22Search%20journal%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22contextual%20search%22%2C%22category%22%3A%22searchbox%20placeholder%20text%22%2C%22en%22%3A%22Search%20%3CJnlTitle%3E%22%2C%22fr%22%3A%22Chercher%20dans%20%3CJnlTitle%3E%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9f5%22%2C%22name%22%3A%22replies%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20eLetters%20tab%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22replies%22%2C%22fr%22%3A%22r%C3%A9ponses%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2c%22%2C%22name%22%3A%22Cambridge%20University%20Press%20content%20sharing%20policy%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Cambridge%20University%20Press%20content%20sharing%20policy.%22%2C%22fr%22%3A%22la%20politique%20de%20Cambridge%20University%20Press%20sur%20le%20partage%20(en%20anglais).%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2a%22%2C%22name%22%3A%22Copy%20and%20paste%20the%20content%20link%20or%20use%20the%20option%20below%20to%20share%20it%20via%20email%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Copy%20and%20paste%20the%20content%20link%20or%20use%20the%20option%20below%20to%20share%20it%20via%20email%22%2C%22fr%22%3A%22Copiez-collez%20ce%20lien%20ou%20cliquez%20le%20bouton%20ci-dessous%20pour%20envoyer%20un%20email%20contenant%20le%20lien%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e8%22%2C%22name%22%3A%22Relevance%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Sorting%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Relevance%22%2C%22fr%22%3A%22Pertinence%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e9%22%2C%22name%22%3A%22Delete%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Recommend%20to%20librarian%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Delete%22%2C%22fr%22%3A%22Supprimer%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e4%22%2C%22name%22%3A%22View%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20entry%22%2C%22category%22%3A%22label%20with%20hyperlink%22%2C%22en%22%3A%22View%22%2C%22fr%22%3A%22Voir%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e5%22%2C%22name%22%3A%22Hide%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20entry%22%2C%22category%22%3A%22label%20with%20hyperlink%22%2C%22en%22%3A%22Hide%22%2C%22fr%22%3A%22Dissimuler%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9ea%22%2C%22name%22%3A%22Access%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Refine%20listing%20options%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Access%22%2C%22fr%22%3A%22Acc%C3%A8s%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9f8%22%2C%22name%22%3A%22Hide%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20eLetters%20tab%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22Hide%22%2C%22fr%22%3A%22Dissimuler%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca2d%22%2C%22name%22%3A%22OR%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22OR%22%2C%22fr%22%3A%22OU%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9e3%22%2C%22name%22%3A%22Citation%20Tools%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Citation%20Tools%22%2C%22fr%22%3A%22Outils%20bibliographiques%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063ac9f6%22%2C%22name%22%3A%22reply%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Article%20eLetters%20tab%22%2C%22category%22%3A%22button%20label%22%2C%22en%22%3A%22reply%22%2C%22fr%22%3A%22r%C3%A9ponse%22%7D%2C%7B%22_id%22%3A%225a7c10d5ab0af516063aca29%22%2C%22name%22%3A%22Share%20content%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22label%22%2C%22en%22%3A%22Share%20content%22%2C%22fr%22%3A%22Partager%20cet%20article%22%7D%2C%7B%22_id%22%3A%225b714525e54e3d290be053d4%22%2C%22name%22%3A%22This%20alert%20has%20been%20successfully%20added.%20Our%20records%20show%20that%20there%20is%20an%20error%20with%20the%20email%20address%20you%20have%20provided%20and%20therefore%20we%20are%20currently%20unable%20to%20send%20content%20alerts%20to%20you.%20The%20email%20address%20we%20are%20attempting%20to%20send%20your%20alerts%20to%20is%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22This%20alert%20has%20been%20successfully%20added.%20Our%20records%20show%20that%20there%20is%20an%20error%20with%20the%20email%20address%20you%20have%20provided%20and%20therefore%20we%20are%20currently%20unable%20to%20send%20content%20alerts%20to%20you.%20The%20email%20address%20we%20are%20attempting%20to%20send%20your%20alerts%20to%20is%22%2C%22fr%22%3A%22Alerte%20ajout%C3%A9e%20avec%20succ%C3%A8s.%20D%C3%BB%20a%20un%20probl%C3%A8me%20avec%20votre%20adresse%20e-mail%20nous%20ne%20sommes%20actuellement%20pas%20en%20mesure%20de%20vous%20envoyer%20des%20alertes%20par%20e-mail.%20L&#x27;adresse%20e-mail%20%C3%A0%20laquelle%20nous%20n&#x27;arrivons%20pas%20%C3%A0%20envoyer%20les%20alertes%20est%22%7D%2C%7B%22_id%22%3A%225b714525e54e3d290be053d5%22%2C%22name%22%3A%22If%20you%20have%20any%20queries%20about%20this%20please%20contact%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22If%20you%20have%20any%20queries%20about%20this%20please%20contact%22%2C%22fr%22%3A%22Pour%20toutes%20questions%20veuillez%20svp%20contacter%22%7D%2C%7B%22_id%22%3A%225b714525e54e3d290be053d6%22%2C%22name%22%3A%22To%20manage%20your%20alert%20preferences%20and%20update%20your%20details%20go%20to%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22%22%2C%22category%22%3A%22%22%2C%22en%22%3A%22To%20manage%20your%20alert%20preferences%20and%20update%20your%20details%20go%20to%22%2C%22fr%22%3A%22Pour%20g%C3%A9rer%20vos%20pr%C3%A9ferences%20d&#x27;alerte%20ou%20mettre%20%C3%A0%20jour%20vos%20details%20veuillez%20svp%20vous%20rendre%20sur%20la%20page%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be68639%22%2C%22name%22%3A%22Anyone%20you%20share%20the%20following%20link%20with%20will%20be%20able%20to%20freely%20read%20this%20content%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Anyone%20you%20share%20the%20following%20link%20with%20will%20be%20able%20to%20freely%20read%20this%20content%22%2C%22fr%22%3A%22%EF%BB%BFToute%20personne%20avec%20qui%20vous%20partagez%20le%20lien%20suivant%20pourra%20acc%C3%A9der%20gratuitement%20%C3%A0%20cet%20article%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863a%22%2C%22name%22%3A%22Copy%20and%20paste%20the%20link%20or%20use%20the%20option%20below%20to%20share%20it%20via%20email%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Copy%20and%20paste%20the%20link%20or%20use%20the%20option%20below%20to%20share%20it%20via%20email%22%2C%22fr%22%3A%22Pour%20cela%2C%20copiez-collez%20ce%20lien%20ou%20bien%20utilisez%20l&#x27;option%20ci-dessous%20pour%20le%20partager%20par%20email%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863b%22%2C%22name%22%3A%22Alternatively%20you%20can%20download%20a%20PDF%20containing%20the%20link%20which%20can%20be%20freely%20shared%20online%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Alternatively%20you%20can%20download%20a%20PDF%20containing%20the%20link%20which%20can%20be%20freely%20shared%20online%22%2C%22fr%22%3A%22Vous%20pouvez%20aussi%20t%C3%A9l%C3%A9charger%20le%20PDF%20contenant%20ce%20lien%20et%20le%20partager%20librement%20en%20ligne%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863c%22%2C%22name%22%3A%22For%20more%20information%2C%20please%20view%20our%20content%20sharing%20policy%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22For%20more%20information%2C%20please%20view%20our%20content%20sharing%20policy%22%2C%22fr%22%3A%22Pour%20plus%20d&#x27;information%2C%20veuillez-vous%20r%C3%A9f%C3%A9rer%20%C3%A0%20notre%20politique%20de%20partage%20de%20contenus%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863d%22%2C%22name%22%3A%22For%20more%20information%2C%20please%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22For%20more%20information%2C%20please%22%2C%22fr%22%3A%22Pour%20plus%20d&#x27;information%2C%20veuillez-vous%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863e%22%2C%22name%22%3A%22view%20our%20content%20sharing%20policy%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22view%20our%20content%20sharing%20policy%22%2C%22fr%22%3A%22r%C3%A9f%C3%A9rer%20%C3%A0%20notre%20politique%20de%20partage%20de%20contenus%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be6863f%22%2C%22name%22%3A%22Share%20this%20link%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Share%20this%20link%22%2C%22fr%22%3A%22Partager%20ce%20lien%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be68640%22%2C%22name%22%3A%22Via%20email%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Via%20email%22%2C%22fr%22%3A%22Par%20email%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be68641%22%2C%22name%22%3A%22Within%20PDF%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Within%20PDF%22%2C%22fr%22%3A%22Par%20PDF%22%7D%2C%7B%22_id%22%3A%225b754bedf620e6600be68642%22%2C%22name%22%3A%22Within%20PDF%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Share%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Within%20PDF%22%2C%22fr%22%3A%22Par%20PDF%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab50718369f%22%2C%22name%22%3A%22January%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22January%22%2C%22fr%22%3A%22Janvier%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a0%22%2C%22name%22%3A%22February%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22February%22%2C%22fr%22%3A%22F%C3%A9vrier%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a1%22%2C%22name%22%3A%22March%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22March%22%2C%22fr%22%3A%22Mars%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a2%22%2C%22name%22%3A%22April%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22April%22%2C%22fr%22%3A%22Avril%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a3%22%2C%22name%22%3A%22May%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22May%22%2C%22fr%22%3A%22Mai%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a4%22%2C%22name%22%3A%22June%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22June%22%2C%22fr%22%3A%22Juin%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a5%22%2C%22name%22%3A%22July%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22July%22%2C%22fr%22%3A%22Juillet%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a6%22%2C%22name%22%3A%22August%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22August%22%2C%22fr%22%3A%22Ao%C3%BBt%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a7%22%2C%22name%22%3A%22September%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22September%22%2C%22fr%22%3A%22Septembre%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a8%22%2C%22name%22%3A%22October%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22October%22%2C%22fr%22%3A%22Octobre%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836a9%22%2C%22name%22%3A%22November%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22November%22%2C%22fr%22%3A%22Novembre%22%7D%2C%7B%22_id%22%3A%225bb361659f569ab5071836aa%22%2C%22name%22%3A%22December%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22dates%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22December%22%2C%22fr%22%3A%22D%C3%A9cembre%22%7D%2C%7B%22_id%22%3A%226568350e9421515f267b9929%22%2C%22name%22%3A%22Copy%20and%20paste%20a%20formatted%20citation%20or%20download%20in%20your%20chosen%20format%22%2C%22area%22%3A%22Client%20side%22%2C%22element%22%3A%22Export%20citation%20modal%22%2C%22category%22%3A%22text%22%2C%22en%22%3A%22Copy%20and%20paste%20a%20formatted%20citation%20or%20download%20in%20your%20chosen%20format%22%2C%22fr%22%3A%22Copiez%20et%20collez%20la%20citation%20format%C3%A9e%20ou%20t%C3%A9l%C3%A9chargez-la%20dans%20le%20format%20de%20votre%20choix%22%7D%5D';
  </script>

  <script type="text/javascript">
      function handleImageError(element) {
          const $element = $(element);
          $element.css('pointer-events', 'none');
          $element.attr('tabindex', '-1');
      }
  </script>
  <meta name="shareaholic:site_id" content="b60ec523a5bee2ad04c630bf0d3aa388" />
  <link rel="preload" href="https://cdn.shareaholic.net/assets/pub/shareaholic.js" as="script" />
  <script data-cfasync="false" async="true" src="https://cdn.shareaholic.net/assets/pub/shareaholic.js"></script>
  <script>
    $(document).ready(function () {
        AOP.loadScript(AOP.baseUrl + '/cambridge-core/public/js/a11y-1.0.40.min.js', function () {
          if(!window.cambridgeA11y) {
            console.error('cambridgeA11y not loaded')
            return
          }
          const { ShareaholicAccessibilityHandler, FocusTrapKeydownHandler } = window.cambridgeA11y
          new ShareaholicAccessibilityHandler(new FocusTrapKeydownHandler()).fixAccessibilityForShareaholicModal();
      })
    })
  </script>
  <script type="text/javascript">
      var AOP = AOP || {};
      AOP.focusTrap = function (modalId, firstFocusableElement) {
        var modalElement = document.getElementById(modalId)
        modalElement.removeAttribute('tabindex')
  
        if (!modalElement) {
          return;
        }
  
        // Get all focusable elements inside the modal
        var focusableElements = Array.from(modalElement.querySelectorAll(
          'a[href], area[href], input:not([disabled]):not([type="hidden"]), select:not([disabled]), textarea:not([disabled]), button:not([disabled]), [tabindex="0"]'
        ))
  
        focusableElements = focusableElements.filter(el => {
          const ancestorElement = el.offsetParent || el.parentElement
          const visibleElement = el.style.visibility !== 'hidden'
  
          return ancestorElement && visibleElement
        })
  
        if (!focusableElements) {
          return;
        }
  
        var firstFocusable = firstFocusableElement || focusableElements[0]
        var lastFocusable = focusableElements[focusableElements.length - 1]
        var TAB_KEY_CODE = 'Tab'
  
        firstFocusable.focus()
  
        $(firstFocusable).on('keydown', function (event) {
          var isTabPressed = event.key === TAB_KEY_CODE
  
          if (!isTabPressed) {
            return
          }
          if (event.shiftKey) {
            event.preventDefault()
            lastFocusable.focus()
          }
        })
  
        $(`#${modalId}`).on('keydown', function (event) {
          var isTabPressed = event.key === TAB_KEY_CODE
  
          if (!isTabPressed) {
            return
          }
          if (event.shiftKey) {
            if (document.activeElement === firstFocusable || document.activeElement === modalElement) {
              event.preventDefault()
              lastFocusable.focus()
            }
          } else {
            if (document.activeElement === lastFocusable || document.activeElement === modalElement) {
              event.preventDefault()
              firstFocusable.focus()
            }
          }
        })
      }
    </script></head>

<body>
<header>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NTX72TG"
                      height="0" width="0" style="display:none;visibility:hidden" sandbox="allow-scripts"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->


<a class="skiptocontent" href="#maincontent">
  Skip to main content
</a>
<a class="skiptocontent" href="/core/accessibility">
 Accessibility help
</a>

  <div class="cookie-message">
      <div class="row">
          <div class="small-11 columns">
              <p>We use cookies to distinguish you from other users and to provide you with a better experience on our websites. Close this message to accept cookies or find out how to <a href="https://www.cambridge.org/about-us/legal-notices/cookies-policy/" target="_blank">manage your cookie settings</a>.</p>
          </div>
          <div class="small-1 columns">
              <a href="#" onclick="closeMessage();" class="cookie-close">
                  <!--[if IE 8 ]>
                  <img alt = "Close cookie message" title = "Close cookie message" src = "/core/cambridge-core/public/images/icn_circle__btn_close_white.png">
                  <![endif]-->
                  <!--[if (gte IE 9)|!(IE)]><!-->
                  <img alt = "Close cookie message" title = "Close cookie message" src = "/core/cambridge-core/public/images/icn_circle__btn_close_white.svg">
                  <!--<![endif]-->
              </a>
          </div>
      </div>
  </div>
  
  <script>
  var cookieMessage = $('.cookie-message');
  var cookieName = 'EULAW';
  var closeMessage = function () {
      $.cookie(cookieName, true, {expires: 365, path:'/'});
      cookieMessage.slideUp().promise().done(function () {
          cookieMessage.remove();
      });
  };
  $(document).ready(function () {
      if ($.cookie(cookieName) !== 'true') {
          cookieMessage.slideDown();
      }
  });
  </script>
  <div id="login-modal" class="reveal-modal small" data-reveal role="dialog" aria-labelledby="loginModalHeader">
    <div class="header">
      <h2 class="heading_07 margin-bottom" id="loginModalHeader">Login Alert</h2>
    </div>
    <div class="wrapper large-margin-top-for-small-only">
      <div class="row margin-top">
        <div class="large-12 columns">
          <div class="panel callout message"></div>
        </div>
      </div>
      <div class="row margin-top">
        <div class="small-6 large-6 columns"><a href="#" onclick="$('#login-modal').foundation('reveal', 'close');return false;" class="right small button radius transparent cancel">Cancel</a></div>
        <div class="small-6 large-6 columns">
          <a href="/core/login?ref&#x3D;/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"  class="left small radius button blue confirm login">Log in</a>
        </div>
      </div>
      <a href="#" class="close-reveal-modal" aria-label="Close login notification"><span aria-hidden="true">×</span></a>
    </div>
  </div>
<div id="multilingual-suggestion-modal" class="reveal-modal small" data-reveal>
    <div class="wrapper large-margin-top-for-small-only">
        <div class="row margin-top">
            <div class="large-12 columns">
                <div id="multilingual-suggestion-modal-message" class="panel callout message"></div>
            </div>
        </div>
        <div class="row margin-top">
            <div class="small-6 large-6 columns"><a id="multilingual-suggestion-modal-decline-option" href="#" class="right small button radius transparent cancel"></a></div>
            <div class="small-6 large-6 columns">
                <a id="multilingual-suggestion-modal-accept-option" href="#" class="left small radius button blue confirm login"></a>
            </div>
        </div>
        <a class="close-reveal-modal">&#215;</a>
    </div>
</div>


      <div class="global-header-wrapper">
        <div class="global-header-spacer"></div>
        <div id="global-header" class="global-header">
          <div class="__shared-elements-html ShEl"><div class="__shared-elements-head">

<link rel="stylesheet" href="/aca/shared-elements/_nuxt/entry.D9LY0ri8.css">

<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-404.B06nACMW.css">


<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-500.WGRfNq7F.css">

</div><div class="__shared-elements-body"><div id="__sharedElements-fmh4p"><!--[--><!----><!----><div class="shared-elements"><div class="gh-headerContainer"><div class="gh-globalHeader"><div class="gh-logoContainer"><a href="https://www.cambridge.org/academic" class="gh-cup-logo"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="149" height="30" viewbox="0 0 149 30" fill="none" aria-label="Homepage Cambridge University Press" class="gh-logo" data-v-7c4b1471><g clip-path="url(#A)" data-v-7c4b1471><g class="B" data-v-7c4b1471><path d="M147.065 1.786l.036 2.785-.445.114c-.1-1.217-.376-2.211-2.324-2.211h-2.618v5.147h2.247c1.103 0 1.402-.616 1.527-1.679h.447v4.304l-.446.007c-.069-1.029-.311-1.829-1.518-1.829h-2.257v4.52c0 1.314 1.133 1.314 2.332 1.314 2.151 0 3.149-.135 3.75-2.31l.441.098-.602 2.901h-9.774v-.482h.186c.75 0 1.443-.14 1.443-1.162V3.43c0-1.022-.693-1.163-1.443-1.163h-.186v-.481h9.204zm-53.267 0h-5.895v.481h.26c.486 0 1.388 0 1.388 1.525v9.569c0 .815-.748 1.104-1.388 1.104h-.26v.482h5.724v-.482h-.261c-.932 0-1.591-.115-1.591-1.6V8.766h1.454l3.918 6.18h3.371v-.492c-.889-.048-1.311-.356-1.828-1.153l-3.23-4.915.085-.035c1.152-.466 2.615-1.414 2.615-3.286 0-2.207-1.447-3.28-4.363-3.28zm-.749.536c1.685 0 2.804 1.123 2.804 2.858 0 2.138-.903 3.051-2.953 3.051h-1.125l.001-5.826a8.87 8.87 0 0 1 1.273-.083zm13.772-.539h-5.501v.481h.186c.751 0 1.443.142 1.443 1.163V13.3c0 1.021-.692 1.162-1.443 1.162h-.186v.481h5.501v-.481h-.186c-.75 0-1.444-.14-1.444-1.162V3.427c0-1.021.694-1.163 1.444-1.163h.186v-.481zM31.488 8.355c0 4.435 2.709 6.939 6.956 6.939 2.089 0 3.924-.818 5.112-2.354l-.345-.644c-1.055 1.355-2.452 2.089-4.412 2.089-3.487 0-4.927-3.074-4.927-6.324 0-3.652 1.781-5.919 4.647-5.919 2.79 0 3.616 1.617 4.112 3.383l.433-.056-.139-3.046c-1.373-.641-2.776-.945-4.46-.945-1.88 0-3.545.638-4.814 1.775-1.367 1.224-2.162 3.146-2.162 5.102zM59.883 4.33l4.761 10.706h.397l4.553-10.921.401 8.416.019.944c0 .893-.643.931-1.533.984l-.036.002v.475h5.464v-.475c-1.008-.038-1.544-.13-1.622-1.151l-.509-8.729c-.112-1.86.151-2.32 1.632-2.32h.07l.001-.485h-3.654l-4.231 9.975-4.472-9.974h-3.727v.479h.152c1.046 0 1.669.644 1.602 1.779l-.516 8.32c-.069 1.215-.161 2.046-1.657 2.106h-.169v.478h4.439v-.478h-.113c-1.21-.045-1.768-.203-1.689-2.003l.437-8.128zm55.196 10.61c1.826 0 3.63-.436 4.919-1.712 1.28-1.27 2.015-3.056 2.015-4.9 0-1.974-.668-3.636-1.93-4.81-1.28-1.191-3.113-1.741-5.301-1.741l-5.887.005v.482h.187c.749 0 1.443.141 1.443 1.162v9.871c0 1.023-.694 1.163-1.443 1.163h-.187v.483l6.184-.004zM114.1 2.349c3.704 0 5.505 1.986 5.505 6.347 0 2.806-1.514 5.669-4.841 5.669-1.413 0-2.003-.225-2.003-1.621l.001-10.334a17.99 17.99 0 0 1 1.338-.062zm19.119 11.918c-.493.224-1.326.347-2.132.33-3.496 0-5.062-3.303-5.062-6.636 0-3.628 1.855-5.882 4.914-5.882 2.267 0 3.277.887 3.823 3.15l.441-.028-.209-3.141c-1.018-.34-2.623-.651-4.184-.651-3.487 0-7.192 2.112-7.192 6.746 0 4.357 2.81 7.173 7.156 7.173 1.801 0 3.422-.307 4.63-.695l.002-3.592c0-.706.374-1.079 1.082-1.079h.153v-.47h-5.062v.47h.154c.973 0 1.487.374 1.487 1.079l-.001 3.226zM80.916 8.129c2.063 0 3.294 1.147 3.294 3.168 0 1.797-1.21 3.117-2.814 3.117-1.336 0-1.746-.368-1.746-1.654V8.129h1.265zm-.073-5.86c1.733 0 2.612.865 2.612 2.723 0 1.682-.999 2.562-2.813 2.562h-.991V2.269h1.192zm1.507 5.51c1.616-.261 3.451-1.032 3.451-3.117 0-1.883-1.511-2.878-4.256-2.878l-5.846.003v.481h.261c.484 0 1.387 0 1.387 1.524v9.548c0 .817-.748 1.121-1.387 1.121h-.261v.477l6.451.001c2.463 0 4.358-1.655 4.358-3.72 0-.866-.303-1.609-.877-2.195-.742-.759-1.624-1.124-3.281-1.245zM50.387 1.784l4.289 11.418c.318.903.783 1.258 1.463 1.258h.04l-.001.486H51.08v-.486h.116c.991 0 1.38-.415 1.082-1.327L51.24 10.24h-4.149l-.956 2.654c-.42 1.245.237 1.553 1.201 1.553h.174l.001.494h-4.008l.002-.494h.045c.811 0 1.358-.498 1.668-1.305 0-.001 4.18-11.36 4.18-11.36l.992.002zm-3.065 7.81h3.684l-1.84-5.127-1.845 5.127zM32.257 20.335c.66 0 .829.18.829.672v4.573c0 1.56 1.153 2.449 3.111 2.449 1.982 0 3.063-1.02 3.063-2.677v-4.141c0-.744.18-.876.793-.876v-.348l-1.093.024a30.18 30.18 0 0 1-1.237-.024v.348c.625.012.925.012.925 1.116v3.997c0 1.272-.757 2.077-2.102 2.077-1.333 0-2.054-.84-2.054-2.089v-4.43c0-.612.264-.672.781-.672h.084v-.348l-1.573.024c-.409 0-.841-.012-1.526-.024v.348zm9.028 0c.829.024 1.237.312 1.237.732v5.33c0 .78-.252 1.056-1.177 1.068v.348l1.429-.024a43.1 43.1 0 0 1 1.453.024v-.348h-.048c-.877 0-1.165-.336-1.165-1.128v-4.982l5.909 6.686h.312v-6.578c0-.6.216-1.128.853-1.128h.264v-.348l-1.429.024c-.288 0-.757-.012-1.393-.024v.348c.853 0 1.213.192 1.213 1.14v4.189l-5.045-5.678-1.081.024c-.432 0-.877-.012-1.333-.024v.348h0zm13.441 7.13c-.685-.012-.853-.12-.853-.66v-5.678c0-.672.168-.792.877-.792v-.348l-1.501.024c-.564 0-1.177-.012-1.633-.024v.348c.673 0 .865.18.865.696v5.462c0 .78-.168.972-.901.972v.348c.36-.012.709-.024 1.622-.024a56.45 56.45 0 0 1 1.525.024v-.348h0zm1.293-7.13c.601.012.829.072.973.432l2.931 7.238h.372l2.739-6.878c.132-.336.216-.492.288-.588.096-.12.18-.204.444-.204h.072v-.348l-1.057.024c-.325 0-.565-.012-1.057-.024v.348c.517.012.853.012.853.3 0 .144-.06.336-.156.6L60.631 26l-1.886-4.681c-.144-.348-.18-.528-.18-.66 0-.312.324-.312.829-.324v-.348l-1.742.024c-.541 0-.973-.012-1.633-.024v.348h0zm8.959 0c.877 0 1.009.084 1.009.744v5.642c0 .54-.132.732-1.009.744v.348l2.774-.024 3.375.012.42-1.789h-.312c-.204.552-.336.78-.468.924a1.14 1.14 0 0 1-.589.312c-.252.06-.709.096-1.381.096-.541 0-.865-.048-1.069-.156-.216-.108-.325-.312-.325-.732V23.96h1.55c.492 0 .829.156.829.816v.192l.3-.06c-.012-.324-.036-.648-.036-1.428l.012-.996h-.3c0 .708-.192.948-.877.948h-1.478v-2.977l1.502-.036c.829 0 1.021.084 1.201.24.168.156.288.348.372 1.02l.312-.12c-.036-.456-.048-.9-.048-1.116 0-.156.012-.348.012-.456l-3.231.024c-.853 0-1.694-.012-2.546-.024v.348h0zm7.686 0h.06c.745 0 .937.108.937.792v5.522c0 .612-.144.816-.949.816h-.06v.348a76.17 76.17 0 0 1 1.778-.024l1.682.024v-.348h-.084c-.757 0-.961-.192-.961-.624V24.14h.757l.588 1.068a22.31 22.31 0 0 0 .697 1.092l.672 1.032c.264.408.409.48.613.48.252 0 .793-.024 1.189-.024.276 0 .516.012.817.024v-.348a1.26 1.26 0 0 1-.913-.336c-.228-.204-.516-.552-.841-1.044L77.18 23.9c1.057-.396 1.621-1.044 1.621-1.957 0-1.26-.961-1.957-2.895-1.957l-1.982.024c-.288 0-.757-.012-1.261-.024v.348h0zm2.402.084c.192-.06.421-.084.613-.084 1.057 0 1.681.6 1.681 1.753 0 .828-.36 1.224-.673 1.416-.24.144-.516.228-.961.228h-.661v-3.313zm9.104-.624c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.814 2.341.865.432 1.609.804 1.609 1.693 0 .66-.492 1.404-1.526 1.404-1.057 0-1.501-.684-1.826-1.765l-.3.048c.036.156.096.648.168 1.62.697.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.48-1.704-1.802-2.389-1.153-.6-1.694-.936-1.694-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.312-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zm6.391 7.671c-.685-.012-.853-.12-.853-.66v-5.678c0-.672.168-.792.877-.792v-.348l-1.502.024c-.564 0-1.177-.012-1.633-.024v.348c.673 0 .865.18.865.696v5.462c0 .78-.168.972-.901.972v.348c.36-.012.709-.024 1.621-.024a56.9 56.9 0 0 1 1.525.024v-.348zm1.517-5.966c.144-.792.409-1.044 1.333-1.044h1.633v6.002c0 .936-.192 1.008-.901 1.008h-.156v.348l1.79-.024 1.766.024v-.348c-.865-.012-1.105-.156-1.105-.864v-6.146h1.67c.432 0 .721.072.865.216.132.132.204.264.24.9l.276-.108c0-.408 0-.852.144-1.789l-.204-.096c-.18.3-.288.396-.673.396h-5.933c-.348 0-.529-.132-.613-.396h-.252a10.45 10.45 0 0 1-.216 1.921h.336zm8.746-1.164c.433 0 .745.048.913.276l2.558 3.469v2.653c0 .672-.216.72-1.045.732v.348l1.826-.024 1.694.024v-.348c-.853 0-1.057-.144-1.057-.624v-3.085l2.126-2.977c.252-.348.504-.444.828-.444v-.348l-1.045.024c-.252 0-.672-.012-1.213-.024v.348c.493.012.793.024.793.24 0 .12-.144.348-.24.492l-1.477 2.221-1.73-2.329c-.156-.216-.18-.324-.18-.384 0-.204.288-.228.624-.24v-.348l-1.693.024c-.565 0-.889-.012-1.682-.024v.348zm14.689.036c.12-.012.36-.036.636-.036 1.405 0 1.453 1.729 1.453 1.981 0 1.116-.492 1.717-1.465 1.717h-.06v.312h.18c1.393 0 1.874-.408 2.198-.744.264-.276.565-.768.565-1.488 0-1.236-.805-2.125-2.607-2.125l-1.717.024c-.385 0-.985-.012-1.442-.024v.348c.565 0 .853.036.853.708v5.63c0 .672-.24.792-.865.792v.348l1.586-.024 1.838.024v-.348c-.649 0-1.154 0-1.154-.756l.001-6.338zm4.466-.036h.06c.744 0 .937.108.937.792v5.522c0 .612-.145.816-.949.816h-.06v.348c.42-.012 1.225-.024 1.777-.024l1.682.024v-.348h-.084c-.757 0-.961-.192-.961-.624V24.14h.757l.588 1.068c.216.372.529.84.697 1.092l.672 1.032c.265.408.409.48.613.48.252 0 .793-.024 1.189-.024.276 0 .517.012.817.024v-.348a1.26 1.26 0 0 1-.913-.336c-.228-.204-.516-.552-.841-1.044l-1.465-2.185c1.057-.396 1.621-1.044 1.621-1.957 0-1.26-.96-1.957-2.894-1.957l-1.982.024c-.288 0-.757-.012-1.261-.024v.348h0zm2.402.084a2.15 2.15 0 0 1 .613-.084c1.057 0 1.681.6 1.681 1.753 0 .828-.36 1.224-.673 1.416-.24.144-.516.228-.96.228h-.661V20.42zm6.516-.084c.877 0 1.009.084 1.009.744v5.642c0 .54-.132.732-1.009.744v.348l2.775-.024 3.375.012.42-1.789h-.312c-.204.552-.336.78-.469.924-.144.144-.3.24-.588.312-.252.06-.709.096-1.381.096-.541 0-.865-.048-1.069-.156-.217-.108-.325-.312-.325-.732V23.96h1.55c.492 0 .828.156.828.816v.192l.301-.06c-.012-.324-.036-.648-.036-1.428l.012-.996h-.301c0 .708-.192.948-.876.948h-1.478v-2.977l1.502-.036c.828 0 1.021.084 1.201.24.168.156.288.348.372 1.02l.312-.12c-.036-.456-.048-.9-.048-1.116 0-.156.012-.348.012-.456l-3.231.024c-.852 0-1.693-.012-2.546-.024v.348zm10.51-.54c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.813 2.341.865.432 1.61.804 1.61 1.693 0 .66-.492 1.404-1.526 1.404-1.056 0-1.501-.684-1.825-1.765l-.3.048c.036.156.096.648.168 1.62.696.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.481-1.704-1.802-2.389-1.153-.6-1.693-.936-1.693-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.313-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zm6.233 0c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.813 2.341.865.432 1.61.804 1.61 1.693 0 .66-.493 1.404-1.526 1.404-1.056 0-1.501-.684-1.825-1.765l-.3.048c.036.156.096.648.168 1.62.696.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.48-1.704-1.802-2.389-1.153-.6-1.693-.936-1.693-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.313-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zM0 0v15.599c0 7.67 4.03 9.859 5.967 10.911.356.193 6.333 3.162 6.715 3.352l.258.138.257-.137 6.716-3.352C21.85 25.459 25.88 23.27 25.88 15.6V0H0z" data-v-7c4b1471></path></g><path d="M12.939 29.365l2.633-1.309V15.762h9.746l.001-.163V10.5h-9.747V.559h-5.266V10.5H.559v5.099l.001.163h9.746v12.294l2.633 1.309z" fill="#fff" data-v-7c4b1471></path><g fill="#e73337" data-v-7c4b1471><path d="M.559.559h9.747V10.5H.559V.559zm15.015 0h9.747V10.5h-9.747V.559zm-5.269 15.205H.559c.056 7.126 3.712 9.191 5.674 10.256.199.108 2.281 1.146 4.073 2.038V15.764zm5.269 0v12.294l4.073-2.038c1.961-1.065 5.618-3.13 5.673-10.256h-9.746z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M19.073 13.234l.201-.239-.254-.302-.255.302.202.239c.003.12.01 1.175-.611 1.552 0 0 .207.04.486-.014a3.1 3.1 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.59-1.791l.201-.239-.254-.302-.255.302.202.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.09 3.09 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.053.486.014.486.014-.622-.377-.614-1.431-.611-1.551zm1.586 1.791l.201-.239-.254-.302-.254.302.201.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.485-.014a3.11 3.11 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.053.485.014.485.014-.622-.377-.614-1.431-.611-1.551zm-6.348 0l.201-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.551zM3.733 13.234l.201-.239-.254-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.59-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.612 1.551 0 0 .207.04.486-.014 0 0 .048.275.178.588.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586 1.791l.201-.239-.254-.302-.255.302.202.239c.003.12.01 1.175-.611 1.552 0 0 .207.04.485-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.485-.014 0 0 .049.274.179.588.13-.314.179-.588.179-.588.278.053.486.014.486.014-.622-.377-.614-1.431-.611-1.551zm-6.348 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.12 3.12 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.551zm10.844 5.803l.201-.239-.254-.302-.254.302.202.239c.003.121.01 1.175-.612 1.551 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.551zm0 5.387l.201-.239-.254-.302-.254.302.202.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.48-2.694l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .178.589c.131-.314.179-.589.179-.589.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.09 3.09 0 0 0 .179.589c.131-.314.179-.589.179-.589.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm2.957 5.387l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .178.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.477-20.59l.201-.239-.254-.302-.254.302.202.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.12 3.12 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.48-2.693l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014 0 0 .048.275.178.588.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.13 3.13 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm2.957 5.387l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.1 3.1 0 0 0 .178.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm11.221 13.502c.144.19.346.314.512.314a1.06 1.06 0 0 0 .223-.034.43.43 0 0 1 .11-.016c.04 0 .057.015.057.052a.56.56 0 0 1-.043.155c-.026.062-.043.112-.049.148l-.033.173-.083.148-.115.107-.033.034v.074.037c0 .202-.06.316-.223.425-.013.008-.033.022-.058.041l-.017.067-.082.288a1.73 1.73 0 0 1-.157.231c-.1.131-.165.237-.206.338-.034.083-.06.107-.114.107-.081 0-.162-.102-.176-.222a.42.42 0 0 1-.041.05l-.098.066-.158.107-.131.099-.091.091c-.016.016-.033.025-.047.025-.042 0-.085-.071-.126-.207l-.082-.271c-.022-.073-.034-.163-.034-.267 0-.137.009-.185.058-.295-.165.017-.212.027-.297.058a.29.29 0 0 1-.072.016c-.027 0-.044-.019-.044-.051a.51.51 0 0 1 .033-.122l.108-.28a.7.7 0 0 1 .566-.416.72.72 0 0 1 .211.044.45.45 0 0 0 .125.026c.096 0 .172-.07.172-.158 0-.044-.029-.091-.066-.108-.014-.007-.113-.037-.297-.091-.139-.041-.208-.092-.364-.273-.113-.131-.158-.168-.199-.168-.032 0-.049.021-.049.058l.008.11.003.051c0 .229-.068.448-.234.75l-.232.42c-.11.2-.157.352-.157.504 0 .166.072.313.165.339l.124.032a.09.09 0 0 1 .041.026c.011.013.026.034.026.047s-.015.046-.042.076c-.015.016-.064.08-.149.189-.142.186-.232.253-.405.305-.1.242-.263.383-.554.479l-.033.057c-.098.174-.317.331-.456.331h-.032c-.018.151-.126.308-.289.421l-.19.139a.34.34 0 0 0-.041.075c-.037.082-.072.116-.118.116-.088 0-.161-.082-.212-.24l-.075.049-.157.107c-.038.018-.135.047-.289.083-.203.047-.378.133-.422.207l-.074.123-.034.014c-.04 0-.087-.057-.107-.13a2.03 2.03 0 0 1-.075-.464c0-.293.097-.484.397-.783l-.001-.071c0-.456.348-.765.86-.765l.083.002.281.017.026.001c.152 0 .292-.058.444-.183.202-.165.261-.266.261-.451 0-.059-.016-.126-.079-.333a1.34 1.34 0 0 1-.061-.388c0-.341.25-.765.59-.999.087-.061.152-.141.152-.187 0-.019-.01-.034-.023-.034-.022 0-.059.023-.105.064-.04.036-.107.078-.198.125l-.132.066-.149.058c-.046.017-.069.042-.069.073 0 .018.007.044.019.075s.016.047.016.061-.014.052-.042.096l-.115.19c-.113.186-.195.266-.339.33-.093.272-.226.41-.421.438-.156.314-.294.423-.57.454-.038.121-.105.221-.248.38-.059.064-.095.109-.107.132-.065.116-.07.123-.1.123-.038 0-.124-.057-.189-.123a.5.5 0 0 1-.074-.1l-.075-.132c-.024-.043-.049-.062-.08-.062l-.053.003-.182.025a.58.58 0 0 0-.331.19l-.099.149c-.01.015-.025.024-.039.024-.074 0-.119-.163-.119-.433 0-.237.036-.395.118-.508l.231-.322c.032-.045.05-.089.05-.126a.17.17 0 0 0-.041-.106c-.049-.058-.096-.093-.132-.099l-.174-.024c-.031-.005-.044-.012-.044-.026s.009-.034.027-.057a.79.79 0 0 1 .207-.182c.205-.125.339-.174.477-.174a.81.81 0 0 1 .432.142l.239.157c.034.023.074.034.115.034.111 0 .194-.066.272-.215.076-.144.109-.26.109-.383a.51.51 0 0 0-.53-.534c-.143 0-.29.056-.43.165-.042.033-.07.064-.083.091l-.066.149c-.015.036-.039.057-.059.057-.042 0-.135-.119-.213-.272-.041-.08-.08-.179-.115-.298-.049-.161-.071-.282-.071-.412l.012-.224a.83.83 0 0 1-.149-.511.92.92 0 0 1 .059-.29.61.61 0 0 1-.091-.368l.008-.103.025-.115.017-.099c-.08-.104-.102-.174-.182-.536-.044-.205-.071-.294-.106-.347-.017-.023-.025-.041-.025-.05 0-.048.115-.092.247-.092l.116.009c-.029-.076-.035-.106-.035-.166a1.03 1.03 0 0 1 .06-.329c.044-.126.053-.161.053-.202 0-.098-.015-.197-.036-.252l-.066-.165a.23.23 0 0 1-.021-.085c0-.017.011-.023.041-.023.162 0 .472.175.715.405s.32.413.32.752l-.014.181c.225.197.323.36.323.537 0 .129-.042.215-.166.337-.223.222-.273.337-.273.622 0 .438.131.675.373.675.213 0 .377-.231.404-.57l.025-.314a3.22 3.22 0 0 1 .041-.297c.014-.075.025-.174.025-.215 0-.111-.073-.252-.29-.504-.232-.271-.31-.417-.31-.629 0-.254.176-.44.414-.44.059 0 .089.009.16.045.003-.144.01-.17.091-.372.064-.159.133-.272.165-.272.011 0 .028.012.049.033a.86.86 0 0 0 .108.09l.083.058a.75.75 0 0 1 .074.075c.051-.185.104-.272.256-.421.116-.113.159-.148.181-.148s.038.015.059.041l.099.131.14.158c.054.059.073.111.099.255l.082-.049.132-.074a.97.97 0 0 0 .1-.066c.022-.016.041-.026.052-.026s.029.024.039.059l.049.19.108.215c.032.064.042.114.042.212v.052l.055-.002c.163 0 .303.192.303.414 0 .294-.235.674-.54.876-.028.099-.035.139-.035.219 0 .657.173.794 1.183.945a5.56 5.56 0 0 1 .677.14l.653.215c.223.074.373.107.489.107.123 0 .191-.035.191-.1 0-.113-.165-.214-.672-.412l-.719-.28c-.731-.286-1.076-.731-1.076-1.388 0-.764.574-1.338 1.34-1.338.289 0 .57.06.976.207l.605.198c.101 0 .156-.069.171-.214.013-.124.047-.174.115-.174.101 0 .212.094.29.248.068.134.107.291.107.428a.47.47 0 0 1-.405.504.8.8 0 0 1-.665.342c-.329 0-.492-.065-1.054-.416-.201-.126-.287-.157-.434-.157-.197 0-.343.122-.343.287 0 .151.122.251.397.324.164.043.188.055.273.14.066-.016.096-.02.137-.02.268 0 .599.145.879.384.509.157.962.706.962 1.167 0 .509-.441.965-.935.965a3.37 3.37 0 0 1-.639-.102z" data-v-7c4b1471></path></g><g class="C" data-v-7c4b1471><path d="M22.437 23.219l.033-.091.033-.116c.006-.021.037-.068.091-.139.084-.11.126-.249.126-.411 0-.051-.013-.098-.044-.16-.016-.032-.025-.058-.025-.076 0-.011.006-.024.017-.038s.022-.023.033-.025l.005-.001c.021 0 .057.024.085.058.035.043.066.066.084.066.037 0 .076-.063.098-.157l.017-.083c0-.04-.033-.144-.074-.231-.016-.033-.024-.061-.024-.084 0-.029.019-.053.046-.053s.063.026.11.07c.023.022.045.034.063.034.052 0 .089-.053.128-.182l.033-.129c0-.024-.01-.038-.029-.038l-.029.002-.098.01c-.151 0-.364-.104-.464-.224l-.231-.281-.206-.273-.19-.124c-.031-.02-.049-.045-.049-.067 0-.031.031-.052.079-.052s.179.048.425.152a2.58 2.58 0 0 0 .837.183c.395 0 .721-.319.721-.707 0-.212-.095-.408-.327-.681-.043-.051-.118-.101-.153-.101-.027 0-.046.02-.046.049a.26.26 0 0 0 .025.093c.01.026.016.052.016.076 0 .038-.014.06-.039.06-.052 0-.106-.085-.151-.234-.055-.184-.162-.294-.414-.421a.89.89 0 0 0-.261-.093c-.016 0-.027.014-.027.036 0 .031.012.065.033.098a.41.41 0 0 1 .067.158v.008c0 .032-.016.05-.046.05-.053 0-.065-.014-.186-.199-.062-.095-.228-.22-.372-.281-.045-.019-.093-.031-.14-.033h-.005c-.03 0-.053.014-.053.032 0 .007.013.033.037.076l.012.047c0 .025-.017.045-.039.045-.05 0-.203-.126-.324-.265-.131-.151-.215-.343-.215-.494 0-.316.302-.596.64-.596.085 0 .157.015.212.042l.289.149.339.224c.195.127.462.223.624.223.06 0 .106-.008.26-.051.081-.022.131-.054.131-.084l-.003-.007-.029-.017-.182-.016c-.101-.01-.33-.113-.512-.232l-.38-.247-.338-.141c-.026-.01-.042-.027-.042-.043s.018-.031.039-.031c0 0 .032.006.094.017l.314.091c.079.022.206.081.38.173.344.183.569.264.734.264.191 0 .331-.131.331-.309 0-.06-.013-.157-.032-.236a.33.33 0 0 0-.05-.107c-.014-.021-.03-.032-.041-.032-.024 0-.037.025-.037.067l.004.057.003.048c0 .152-.059.206-.228.206-.137 0-.322-.055-.742-.222-.394-.155-.64-.214-.884-.214-.622 0-1.167.507-1.167 1.085 0 .499.332.882 1.011 1.161l.702.289c.581.239.794.41.794.636 0 .188-.175.309-.448.309-.118 0-.265-.032-.536-.119-.657-.209-1.085-.296-1.461-.296-.613 0-1.049.168-1.398.536-.046.049-.073.063-.114.063-.028 0-.056-.011-.118-.047a.4.4 0 0 0-.181-.059c-.06 0-.154.052-.198.109l-.082.107c-.024.031-.051.05-.073.05-.06 0-.166-.102-.232-.224-.041-.074-.069-.147-.082-.215-.011-.052-.017-.137-.017-.226l.008-.153.025-.165.025-.141c.006-.02.022-.033.04-.033.043 0 .062.039.062.129l-.002.052-.002.037c0 .235.093.45.196.45.081 0 .152-.144.211-.43.012-.057.035-.09.063-.09s.053.042.078.165c.015.08.068.191.124.264.032.042.066.067.094.067.068 0 .111-.101.145-.339.014-.092.041-.15.073-.15.014 0 .026.007.035.018a1.3 1.3 0 0 1 .058.091c.02.034.052.062.091.082s.074.033.096.033a.12.12 0 0 0 .077-.033c.037-.033.058-.058.058-.068s-.017-.038-.049-.073c-.054-.058-.096-.136-.149-.273L20 19.462c-.013-.03-.027-.05-.042-.058l-.026-.007-.049.007-.173.049-.054.009c-.023 0-.039-.015-.039-.039s.03-.062.068-.092c.209-.166.333-.445.38-.851.028-.247.06-.34.165-.487.022-.031.041-.049.052-.049.032 0 .05.025.05.073 0 .032-.009.072-.027.117a.29.29 0 0 0-.018.1l.001.031.017.05.091-.033.099-.115a.61.61 0 0 0 .109-.338c0-.079-.019-.134-.066-.19-.052-.062-.088-.087-.126-.087l-.056.004-.059.004c-.053 0-.08-.016-.08-.049l.014-.063.009-.091-.049-.206-.041-.1-.025-.083-.058.041-.124.074c-.032.019-.066.05-.1.091-.117.142-.125.149-.172.149-.03 0-.047-.011-.047-.031l.005-.035a.85.85 0 0 0 .025-.171c0-.144-.105-.361-.231-.481-.173.165-.265.346-.265.521v.099c0 .038-.012.057-.038.057h-.003c-.021-.003-.041-.019-.058-.041l-.099-.132a1.33 1.33 0 0 0-.133-.108l-.066-.057-.033.066c-.082.164-.11.262-.115.397-.004.11-.013.142-.039.142-.013 0-.03-.006-.052-.019-.125-.07-.137-.075-.191-.075-.149 0-.25.106-.25.263 0 .061.035.154.103.275.076.135.189.258.239.258.014 0 .017-.01.017-.051v-.017l-.008-.14-.017-.108-.001-.014c0-.032.012-.046.037-.046.075 0 .14.219.195.655.048.384.154.648.322.801.043.039.067.071.067.089s-.015.026-.033.026c-.024 0-.062-.015-.108-.041a.18.18 0 0 0-.09-.026c-.083 0-.175.096-.191.2l-.075.47-.058.661c-.011.125.006.293.05.495.038.178.058.309.058.391 0 .495-.308.886-.818 1.037l-.529.156c-.329.098-.491.315-.537.718.085-.067.111-.078.24-.098l.156-.025a.43.43 0 0 0 .141-.041c.015-.009.045-.04.091-.091l.14-.156c.053-.06.101-.094.13-.094.02 0 .035.022.035.048s-.02.078-.058.136-.06.135-.06.213c0 .105.028.172.135.324.013-.05.024-.083.033-.099a1.69 1.69 0 0 1 .124-.173c.114-.148.197-.302.198-.372.002-.128.014-.169.046-.169s.033.006.045.079c.003.018.012.035.025.05.019.021.038.033.05.033s.033-.012.066-.033c.057-.038.101-.077.132-.116s.062-.093.082-.149c.028-.072.042-.125.042-.156l-.001-.107c0-.028.012-.051.027-.051s.062.043.089.091c.018.032.03.042.053.042h.005c.03-.003.056-.015.074-.034.101-.098.166-.219.166-.31l-.009-.152-.033-.125-.002-.011c0-.026.019-.048.04-.048.037 0 .078.053.135.174.026.057.051.084.075.084.039 0 .07-.029.148-.142l.074-.107c.016-.024.025-.052.025-.083s-.009-.057-.025-.081l-.067-.099c-.011-.017-.016-.045-.016-.083a.81.81 0 0 1 .124-.388l.107-.207c.013-.034.027-.05.046-.05s.028.02.028.041l-.008.091v.066.066c.011.03.031.042.071.042.025 0 .054-.006.086-.017l.206-.075a1.42 1.42 0 0 0 .19-.091c.324-.179.424-.216.583-.216.095 0 .218.037.392.117.036.017.065.026.085.026s.05-.015.072-.042c.016-.019.024-.042.024-.063 0-.03-.011-.066-.033-.102-.026-.046-.051-.103-.051-.123s.014-.036.038-.036.047.02.062.044l.19.33c.154.274.354.464.529.504l.363.083c.113.025.141.069.141.209a.72.72 0 0 1-.331.616l-.298.191c-.108.068-.198.259-.198.416 0 .102.025.282.058.425l.058-.057.141-.083.124-.124.066-.182.066-.165c.032-.08.07-.132.096-.132s.045.045.045.126l-.001.023v.031c0 .118.015.17.107.348z" data-v-7c4b1471></path><path d="M17.543 25.575c0-.393.143-.564.712-.853 1.123-.569 1.728-1.091 1.728-1.488 0-.08-.026-.197-.1-.445a1.41 1.41 0 0 1-.071-.358c0-.183.151-.46.344-.632l.157-.14c.086-.077.15-.188.15-.261l-.001-.037-.002-.049c0-.082.032-.112.117-.116l.075-.008.064-.017c.027 0 .053.015.069.041l.041.066.058.066c.019.021.038.034.053.034s.033-.008.063-.024c.04-.022.071-.031.1-.031.043 0 .065.023.065.071 0 .166-.107.406-.298.661-.317.429-.421.66-.421.934 0 .215.062.404.182.56.021.028.033.05.033.061 0 .022-.021.059-.058.104l-.05.067c-.066.095-.11.132-.153.132-.032 0-.063-.027-.078-.067l-.033-.09c-.01-.026-.028-.043-.047-.043-.006 0-.02.014-.036.034-.021.027-.032.059-.032.091v.14c0 .056-.02.114-.058.165a.69.69 0 0 1-.116.123c-.031.027-.054.042-.065.042-.024 0-.047-.026-.059-.066l-.05-.165c-.007-.024-.024-.041-.041-.041-.028 0-.051.046-.058.116-.017.187-.049.301-.098.347l-.116.106c-.037.036-.081.061-.105.061s-.045-.011-.052-.027l-.058-.124c-.007-.015-.021-.025-.037-.025-.033 0-.058.066-.07.191-.03.297-.092.43-.256.545a.45.45 0 0 0-.206.231c-.072-.106-.1-.196-.1-.329a.3.3 0 0 1 .034-.157c.034-.062.053-.109.053-.137 0-.02-.013-.036-.03-.036-.008 0-.022.012-.041.032-.023.027-.059.055-.107.083-.149.085-.171.116-.207.281a.42.42 0 0 1-.38.371c-.28.057-.297.064-.397.158l-.01-.147zm-.534-8.514v.083l.001.122a.51.51 0 0 1-.018.134c-.027.115-.033.154-.033.187 0 .158.022.236.115.399.052.09.085.199.085.277 0 .033-.014.053-.036.053-.014 0-.029-.008-.04-.024l-.066-.091c-.053-.072-.103-.091-.237-.091l-.143.009a1.79 1.79 0 0 1 .099.289c.067.27.076.289.223.421s.161.15.161.198c0 .037-.026.067-.057.067l-.055-.008-.033-.003c-.045 0-.086.083-.086.175a.59.59 0 0 0 .053.208c.02.049.04.082.058.1l.083.082a.12.12 0 0 1 .033.085c0 .04-.023.074-.05.074a.19.19 0 0 1-.074-.027c-.021-.01-.04-.016-.058-.016-.037 0-.055.023-.055.068l.013.123.05.214c.011.048.042.102.091.158a.39.39 0 0 0 .124.099l.166.075c.016.012.024.036.024.066s-.024.058-.055.058a.7.7 0 0 1-.086-.017l-.084-.008c-.084 0-.09.005-.09.069a.52.52 0 0 0 .017.128l.033.132c.035.14.08.215.13.215.011 0 .032-.012.06-.033a1.02 1.02 0 0 1 .511-.173c.155 0 .332.019.399.041.038.013.065.02.079.02.02 0 .027-.02.028-.094l.017-.149.033-.256.024-.19v-.108l-.067.058c-.14.132-.229.173-.374.173-.38 0-.598-.427-.598-1.171 0-.307.045-.742.13-1.24.018-.105.026-.178.026-.229 0-.272-.172-.537-.472-.729z" data-v-7c4b1471></path><path d="M17.57 18.91l.074-.074.074-.067c.062-.054.111-.169.111-.254 0-.039-.008-.075-.028-.118-.029-.062-.084-.117-.117-.117-.028 0-.053.068-.064.175l-.017.124-.024.124-.009.124v.082zm-.726 3.377c.085.031.102.045.19.149.115.133.178.165.33.165a.42.42 0 0 0 .117-.016l.107-.033.083-.025c.04-.011.066-.028.066-.042s-.065-.072-.165-.139a1.59 1.59 0 0 0-.198-.116c-.071-.035-.131-.051-.189-.051-.116 0-.228.036-.339.109zm1.113 2.323c0-.266.262-.485.582-.485.16 0 .253.032.253.086 0 .042-.07.1-.223.181l-.24.132-.364.181-.008-.096zm3.363-2.003c.117-.024.157-.054.215-.057l.149.008a.48.48 0 0 0 .099-.066l.116-.082c.165-.113.174-.12.174-.156 0-.046-.067-.068-.206-.068-.26 0-.419.122-.546.422z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M18.827 17.58l.116.074c.066.042.121.073.165.09.128.053.174.071.174.12a.35.35 0 0 1-.017.094.51.51 0 0 0-.025.116c-.012.112-.08.141-.125.141-.056 0-.114-.086-.206-.248a1.91 1.91 0 0 1-.132-.33l-.005-.032c0-.02.012-.032.036-.032l.018.006z" data-v-7c4b1471></path><path d="M19.942 17.868l-.19.191c-.067.089-.067.167-.083.182s-.032.021-.058.025l-.012.001c-.048 0-.084-.035-.087-.083l-.008-.14-.009-.141v-.008c0-.054.03-.083.073-.092l.176-.049.181-.091c.053-.026.069-.033.078-.033.021 0 .039.022.039.048 0 .012-.006.03-.018.052l-.083.139z" data-v-7c4b1471></path><path d="M19.105 18.431l.124.092a.22.22 0 0 0 .111.028.51.51 0 0 0 .195-.053l.108-.049c.025-.011.047-.018.064-.018.066 0 .118.07.118.157 0 .122-.088.247-.216.429l-.09.191c-.072.103-.12.14-.179.14-.098 0-.146-.056-.293-.348-.106-.208-.149-.336-.149-.438 0-.087.045-.148.111-.148.027 0 .06-.006.096.016z" data-v-7c4b1471></path></g><path d="M19.216 18.884l.008.157c.004.071.063.142.12.142.076 0 .131-.115.16-.29l.019-.115c0-.061-.02-.077-.098-.077l-.044.002h-.058l-.037-.002c-.052 0-.072.04-.072.145l.002.038z" class="C" data-v-7c4b1471></path><path d="M8.359 21.057c.072.182.097.272.107.389l.016.346c.013.347.128.52.347.52.079 0 .145-.021.182-.058l.091-.091.036-.011c.058 0 .09.038.09.107l-.01.134c-.007.06.004.188.033.379a2.25 2.25 0 0 1 .025.333c0 .176-.013.244-.075.402.058.162.091.345.091.499 0 .18-.029.3-.099.402.033.142.041.199.041.28s-.011.187-.033.314c-.029.172-.041.305-.033.395l.001.021c0 .052-.016.078-.048.078s-.069-.024-.102-.066c-.05-.065-.083-.088-.121-.088-.057 0-.075.03-.16.278-.026.077-.093.18-.198.306-.082.098-.133.179-.149.24l-.049.173c-.01.022-.029.039-.05.041h-.01c-.136 0-.292-.292-.452-.842-.045-.156-.103-.225-.187-.225-.06 0-.121.04-.144.093l-.05.116c-.009.02-.026.033-.043.033-.066 0-.167-.121-.229-.273-.051-.126-.081-.263-.081-.38 0-.259.191-.452.766-.776.251-.141.348-.32.348-.643 0-.256-.077-.403-.297-.57-.175-.132-.275-.182-.368-.182-.078 0-.14.052-.14.118 0 .078.045.125.153.162.127.046.182.09.182.147s-.054.165-.141.299a.76.76 0 0 1-.264.273c-.039.227-.146.372-.397.537-.05.142-.078.193-.165.305l-.174.223a.72.72 0 0 0-.124.256c-.018.126-.03.148-.078.148-.066 0-.2-.116-.335-.289a.4.4 0 0 1-.107-.273c-.086.068-.113.077-.281.099-.101.014-.176.033-.223.059-.127.066-.168.084-.191.084-.034 0-.046-.018-.049-.076l-.008-.182c0-.249.098-.54.248-.734-.145-.125-.289-.202-.405-.215-.049-.005-.062-.011-.062-.029 0-.03.032-.095.078-.152.062-.078.111-.133.149-.165a.82.82 0 0 1 .479-.174c.107 0 .266.039.471.116.074.028.14.041.197.041.091 0 .142-.059.142-.163 0-.15-.074-.385-.241-.762-.112-.255-.163-.427-.163-.545 0-.286.135-.583.304-.668-.08-.04-.115-.05-.184-.05-.083 0-.148.021-.254.083-.122.071-.19.134-.19.176 0 .016.025.042.067.072.064.045.075.061.075.122 0 .204-.169.477-.405.654-.052.289-.185.464-.446.586-.034.157-.056.203-.182.38-.216.302-.266.399-.305.57-.02.089-.041.125-.068.125-.035 0-.108-.055-.188-.142l-.132-.158c-.035-.054-.053-.107-.091-.263-.06.017-.076.02-.118.02l-.121-.003-.083-.002c-.181 0-.373.041-.471.101-.036.022-.06.033-.072.033-.053 0-.097-.112-.097-.25 0-.298.083-.545.276-.814-.071-.059-.094-.081-.157-.148-.027-.03-.05-.05-.066-.058l-.091-.05c-.031-.017-.05-.037-.05-.054 0-.044.121-.159.264-.252.159-.102.285-.149.4-.149.135 0 .207.035.376.182.117.102.197.149.254.149.04 0 .084-.028.118-.075.028-.038.043-.08.043-.123 0-.158-.189-.424-.398-.562-.156-.102-.326-.157-.492-.157-.23 0-.428.114-.515.297l-.091.19c-.021.044-.048.067-.079.067-.053 0-.081-.024-.268-.241l-.214-.248a.66.66 0 0 1-.133-.214.49.49 0 0 0-.041-.107c-.007-.011-.036-.033-.082-.067-.166-.117-.287-.349-.356-.685l-.025-.107c-.202-.248-.323-.555-.323-.821a.36.36 0 0 1 .018-.113c-.118-.118-.131-.147-.224-.463-.066-.224-.108-.32-.19-.428-.022-.029-.033-.052-.033-.064 0-.045.069-.081.19-.101l.166-.019.165.011-.01-.226c0-.133.01-.219.044-.377a1.01 1.01 0 0 0 .025-.172 1.68 1.68 0 0 0-.017-.142.73.73 0 0 0-.024-.124c-.012-.031-.018-.055-.018-.07 0-.027.018-.041.049-.041.024 0 .058.012.109.036.144.071.263.143.355.215.285.223.509.566.509.779l-.004.065-.017.121c0 .036.02.084.058.134l.091.125c.081.11.133.238.133.324 0 .077-.016.111-.149.304-.1.144-.141.252-.141.369 0 .149.059.337.166.522.097.171.21.265.317.265.184 0 .265-.163.294-.604l.066-.627.008-.113c0-.052-.016-.082-.082-.161l-.249-.288c-.236-.276-.307-.418-.307-.616 0-.227.149-.384.365-.384l.099.009-.001-.064a.87.87 0 0 1 .042-.224c.052-.177.117-.314.151-.314.011 0 .038.013.08.041a1.22 1.22 0 0 0 .149.057.99.99 0 0 1 .164.082.73.73 0 0 1 .133-.289c.107-.138.228-.247.271-.247.026 0 .074.035.134.099l.157.165c.026.027.057.081.091.157l.033.074.099-.057.132-.075.157-.075c.042-.028.072-.043.085-.043.028 0 .043.05.105.323a1.9 1.9 0 0 1 .058.364c.264.037.388.188.388.471 0 .266-.131.531-.413.832-.077.083-.096.122-.096.199 0 .73.254.96 1.17 1.057a5.83 5.83 0 0 1 1.404.298c.355.106.559.15.691.15.156 0 .243-.057.243-.157 0-.153-.111-.201-.769-.34-1.531-.322-2.132-.845-2.132-1.854 0-.719.538-1.259 1.257-1.259.317 0 .6.097 1.048.355.416.241.502.282.597.282.111 0 .155-.071.155-.25 0-.116.035-.172.107-.172.066 0 .166.068.256.174.137.162.19.245.19.469 0 .299-.159.494-.471.58l-.074.082c-.17.192-.32.256-.59.256-.235 0-.53-.096-.732-.24l-.397-.28c-.171-.121-.296-.173-.416-.173-.169 0-.27.101-.27.27s.11.301.29.357l.372.116c.132.041.194.067.355.148l.185-.018a1.57 1.57 0 0 1 .699.174l.149.067.207.058c.469.131.851.656.851 1.168 0 .584-.379.946-.991.946l-.191-.009z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M8.218 25.961l.049-.1.116-.124a.61.61 0 0 0 .15-.427c0-.085-.015-.177-.043-.267a.67.67 0 0 1-.034-.146c0-.052.039-.102.079-.102.016 0 .039.015.062.042l.124.14c.077.087.103.103.199.124l.008-.115.008-.136a.96.96 0 0 0-.182-.558c-.066-.084-.096-.135-.096-.164s.022-.059.045-.059c.008 0 .02.006.034.017.08.059.121.084.143.084.04 0 .076-.1.076-.208a.77.77 0 0 0-.169-.461l-.099-.123c-.011-.016-.017-.032-.017-.047 0-.035.029-.061.072-.061.031 0 .05.008.102.041.024.016.047.025.063.025.064 0 .088-.09.088-.336l-.003-.102-.008-.149c-.009-.175-.025-.206-.099-.206H8.82c-.403 0-.591-.255-.695-.942-.075-.496-.26-.849-.487-.925-.136-.045-.184-.075-.184-.111 0-.023.038-.045.079-.045.03 0 .099.018.156.041.329.133.607.198.849.198.396 0 .708-.291.708-.66 0-.282-.134-.569-.376-.801a.47.47 0 0 0-.206-.124l-.016-.001-.042.014.017.119.032.083.009.034c0 .018-.017.032-.039.032s-.047-.013-.061-.034l-.149-.223c-.118-.176-.376-.323-.568-.323-.068 0-.111.03-.111.078 0 .023.022.066.05.097.042.048.075.105.075.132l-.009.033c-.009.019-.021.031-.033.033l-.008.001c-.04 0-.06-.018-.24-.223-.147-.168-.448-.332-.609-.332-.023 0-.044.01-.061.026-.01.011-.017.023-.017.032 0 .025.022.058.058.092.051.047.092.103.092.127 0 .016-.022.031-.046.031-.047 0-.153-.068-.351-.224-.308-.242-.446-.47-.446-.735 0-.319.228-.57.52-.57.171 0 .398.082.554.199l.405.305a1.07 1.07 0 0 0 .65.215c.052 0 .1-.012.143-.033.036-.018.058-.04.058-.055s-.031-.024-.075-.036c-.146-.035-.264-.085-.347-.148l-.413-.313-.323-.223c-.218-.114-.26-.141-.26-.167 0-.013.021-.024.047-.024s.047.006.073.017l.215.091a2.47 2.47 0 0 1 .347.206c.442.295.667.39.927.39.247 0 .429-.175.429-.413 0-.047-.012-.098-.034-.143l-.041-.082c-.014-.029-.041-.052-.049-.041l-.025.033-.025.174c-.001.007-.01.027-.025.058a.29.29 0 0 1-.057.082c-.045.044-.083.058-.152.058-.128 0-.271-.068-.658-.315s-.744-.379-1.024-.379A1.07 1.07 0 0 0 5.977 18c0 .827.505 1.2 2.09 1.546.603.131.868.308.868.576 0 .245-.198.399-.512.399a1.19 1.19 0 0 1-.224-.016 30.59 30.59 0 0 1-.851-.232c-.354-.098-.752-.154-1.103-.154-.622 0-1.041.185-1.326.584-.033.047-.066.075-.088.075s-.042-.012-.069-.033l-.115-.091a.56.56 0 0 0-.297-.113.26.26 0 0 0-.175.072c-.041.037-.072.057-.084.057-.046 0-.131-.104-.196-.239-.093-.193-.141-.386-.141-.562 0-.082.026-.14.062-.14.029 0 .05.022.062.066l.066.239c.037.134.112.239.169.239.025 0 .07-.036.12-.099.031-.038.053-.077.067-.116.052-.151.064-.173.105-.173.048 0 .073.03.106.181.007.028.016.07.054.142.033.062.067.113.099.149.025.027.056.042.085.042.073 0 .139-.121.139-.255l-.002-.035-.01-.153c0-.059.009-.077.043-.077.019 0 .034.01.058.04.035.046.083.085.141.115.05.027.091.041.118.041s.053-.015.08-.041.035-.044.035-.065-.03-.062-.117-.15c-.047-.048-.103-.16-.165-.33-.023-.065-.054-.109-.078-.109a.24.24 0 0 0-.054.018l-.107.041-.074.033-.028.006c-.035 0-.063-.025-.063-.057s.075-.121.166-.196c.065-.055.105-.136.107-.223.005-.139.031-.248.058-.248l.025-.289a2.98 2.98 0 0 1 .124-.569c.01-.03.023-.051.033-.051.023 0 .044.062.044.131l-.003.043-.005.084c0 .105.017.159.049.159s.077-.052.113-.143l.074-.19c.022-.056.033-.108.033-.154 0-.161-.096-.277-.23-.277l-.06.01-.058.017c-.043 0-.08-.048-.08-.104l.006-.119-.008-.165-.033-.173-.009-.083-.074.041-.091.075-.132.107-.108.091c-.019.016-.04.025-.061.025-.063 0-.071-.017-.071-.146v-.032c0-.104-.045-.185-.214-.384l-.042-.05-.032.058-.091.149-.091.231-.017.148c-.004.043-.021.076-.037.076s-.046-.022-.078-.059a.64.64 0 0 0-.355-.239l-.016.057-.05.174-.009.156v.066.053c0 .046-.017.073-.045.073l-.005-.001c-.003-.001-.019-.015-.049-.041-.019-.017-.054-.026-.1-.026-.154 0-.252.083-.252.215 0 .099.034.173.186.404.068.103.146.184.176.184.02 0 .037-.054.037-.117l-.015-.099a.41.41 0 0 1-.024-.126c0-.046.013-.071.039-.071.074 0 .151.214.183.512l.033.305a1.02 1.02 0 0 0 .099.33c.075.154.149.259.215.305.079.056.128.111.128.141s-.021.05-.044.05a5.64 5.64 0 0 1-.167-.059l-.069-.01c-.165 0-.22.094-.22.378a1.58 1.58 0 0 0 .488 1.044l.38.306c.193.154.339.411.339.595 0 .242-.265.479-.719.644l-.273.098c-.212.077-.297.214-.38.611a.68.68 0 0 1 .228-.042l.037.001.123.003c.111 0 .191-.067.191-.16v-.075c0-.022.031-.053.074-.075l.115-.058.091-.074c.014-.012.03-.019.046-.019.025 0 .045.024.045.051 0 .02-.006.046-.017.075-.028.073-.044.173-.044.284 0 .202.032.311.135.459.009-.055.018-.09.025-.107.017-.042.064-.117.141-.223.125-.175.217-.388.217-.503l-.003-.035-.025-.189c0-.034.015-.067.041-.091.018-.016.038-.024.057-.024.035 0 .057.021.085.082.019.046.043.074.059.074.029 0 .086-.076.122-.165.028-.066.041-.125.041-.175a.29.29 0 0 0-.025-.106.29.29 0 0 1-.026-.094c0-.021.007-.031.034-.055.014-.01.028-.017.042-.017s.033.007.049.017l.074.05c.025.017.048.025.066.025.033 0 .081-.037.116-.092.072-.11.117-.201.117-.235 0-.022-.014-.038-.059-.07-.164-.116-.273-.292-.273-.442 0-.035.008-.074.025-.119l.082-.223.066-.174.024-.009c.028 0 .045.023.045.064a.28.28 0 0 1-.019.094.54.54 0 0 0-.035.157c0 .059.03.091.086.091a.35.35 0 0 0 .138-.033c.25-.108.448-.166.567-.166.112 0 .182.025.3.108.031.022.058.034.076.034s.032-.013.048-.034c.01-.014.017-.029.017-.041 0-.03-.018-.069-.05-.107-.042-.052-.059-.085-.059-.117 0-.026.017-.043.044-.043.13 0 .333.41.527 1.059.152.513.195.565.685.843.415.234.573.466.573.843 0 .371-.103.846-.317 1.469-.067.197-.097.33-.097.44 0 .087.017.172.081.394l.057.19a.98.98 0 0 0 .065.124zm-1.916-1.156a1.38 1.38 0 0 1 .157-.281c.134-.201.182-.308.182-.404v-.133c0-.124.022-.182.072-.182a.05.05 0 0 1 .044.025l.042.066c.013.021.035.033.058.033s.069-.032.093-.069a.58.58 0 0 0 .096-.295l-.009-.091-.008-.093c0-.05.02-.072.068-.072.032 0 .046.013.073.075.017.04.042.066.062.066s.049-.018.069-.05l.066-.099.05-.075.012-.04c0-.023-.018-.044-.062-.076-.153-.11-.22-.28-.231-.587a2.62 2.62 0 0 0-.041-.454c-.005-.016-.054-.134-.149-.355a2.15 2.15 0 0 1-.099-.297c-.019-.076-.043-.1-.103-.1-.225 0-.394.192-.394.445 0 .149.062.384.175.67.159.401.201.544.201.686 0 .303-.121.435-.54.586-.473.169-.695.413-.777.85l.083-.033.115-.041.132-.033c.114-.028.205-.092.223-.157l.033-.115c.008-.028.024-.047.05-.058l.074-.033.074-.033.016.033-.033.107a.74.74 0 0 0-.017.192c0 .181.036.287.141.418zm-4.437-7.639l.017.093c0 .026-.011.097-.033.213l-.015.168a1.35 1.35 0 0 0 .072.435c.042.119.053.156.053.188l-.003.035c-.008.034-.02.059-.032.059s-.046-.018-.084-.052c-.095-.082-.198-.125-.305-.125a.81.81 0 0 0-.141.017l.041.091.124.306.174.264.181.125a.2.2 0 0 1 .05.049c.017.023.025.042.025.055s-.014.027-.033.028l-.166.008c-.025.001-.045.017-.045.035 0 .028.024.111.071.246l.083.24.14.132.148.108c.044.031.07.067.07.093s-.014.046-.029.047l-.082.008c-.027.003-.042.026-.042.066 0 .062.058.174.166.322.128.174.218.232.364.232h.124c.059 0 .095.029.095.076 0 .039-.025.061-.079.073-.114.024-.149.047-.149.1 0 .021.015.053.042.09l.067.091c.055.075.117.125.157.125.022 0 .074-.023.121-.076.074-.085.159-.154.268-.181.287-.073.347-.103.347-.169 0-.023-.011-.056-.033-.095-.091-.164-.124-.27-.124-.404 0-.103-.027-.175-.064-.175-.015 0-.041.025-.068.067-.068.104-.184.174-.289.174-.286 0-.631-.448-.801-1.041a1.65 1.65 0 0 1-.061-.445 2.32 2.32 0 0 1 .061-.504l.091-.405.011-.112c0-.205-.189-.468-.481-.673z" data-v-7c4b1471></path><path d="M2.46 18.315c-.061.115-.117.374-.117.538 0 .096.031.173.072.173.013 0 .03-.016.045-.041l.091-.166.059-.101c.033-.049.057-.097.057-.123 0-.098-.091-.219-.207-.28zm.907 3.723l.041.041.066.066c.161.167.185.183.263.183.072 0 .222-.041.307-.084.036-.018.058-.04.058-.057 0-.011-.03-.044-.082-.091l-.091-.083c-.069-.062-.171-.12-.22-.12-.095 0-.192.042-.342.145zm1.918 1.495a.82.82 0 0 1 .165.083l.132.082c.025.011.07.017.131.017.038 0 .072-.006.1-.017l.174-.066.165-.058c.044-.016.075-.041.075-.064 0-.012-.037-.03-.091-.043-.037-.01-.079-.023-.124-.041-.203-.082-.233-.094-.305-.094a.55.55 0 0 0-.422.201zm2.092 1.66c.055-.133.093-.155.281-.166.212-.012.414-.391.414-.648 0-.025-.009-.047-.019-.047s-.03.01-.056.027l-.123.074-.24.107c-.16.071-.301.285-.301.453a.57.57 0 0 0 .045.2z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M3.807 18.018l.082.116a.78.78 0 0 0 .179.129c.025 0 .038-.018.053-.071l.033-.125.009-.138c0-.109-.048-.15-.273-.208-.056-.015-.079-.013-.115-.042s-.079-.05-.099-.05-.034.01-.034.023l.009.037.041.123.116.207zm.883-.001l-.108.191-.041.045-.008.001c-.044 0-.088-.1-.091-.204l-.016-.164c.013-.068.053-.098.156-.116.084-.015.14-.031.166-.049l.132-.091.014-.003c.022 0 .038.015.038.04l-.003.011-.058.141-.182.197z" data-v-7c4b1471></path><path d="M4.474 18.579l.132-.066c.024-.012.045-.018.063-.018.064 0 .112.048.112.134 0 .094-.074.221-.175.403-.109.197-.191.331-.276.331-.119 0-.287-.189-.451-.505a.54.54 0 0 1-.063-.23c0-.104.046-.159.133-.159.036 0 .07.008.095.026l.074.05a.39.39 0 0 0 .21.066c.052 0 .101-.011.145-.033z" data-v-7c4b1471></path></g><path d="M4.384 18.761s.076.016.076.067l-.002.024-.025.123c-.027.165-.065.267-.119.267a.08.08 0 0 1-.063-.035c-.085-.12-.15-.274-.15-.358 0-.059.026-.089.079-.089l.014.001.091.016.099-.016z" class="C" data-v-7c4b1471></path><path d="M23.617 5.468l.033.124.067.264c.104.472.191.595.421.595.131 0 .24-.073.273-.182l.049-.166c.011-.038.04-.06.079-.06.09 0 .138.099.145.299.002.076.019.211.049.404.023.139.033.267.033.382 0 .084-.01.133-.049.237l.016.099c.028.163.035.227.035.303 0 .354-.017.459-.101.597.034.135.044.199.044.296a2.47 2.47 0 0 1-.027.291l-.033.248.008.148c.022.055.033.089.033.099 0 .023-.015.041-.033.041l-.165-.066-.182-.099c-.059-.032-.1-.064-.19-.148-.16.141-.183.154-.373.206-.274.076-.375.11-.429.149s-.093.057-.114.057c-.064 0-.109-.124-.109-.306 0-.264.015-.401.058-.504-.161-.054-.242-.074-.3-.074a.69.69 0 0 0-.121.016l-.037.004c-.024 0-.038-.029-.038-.079 0-.081.057-.242.133-.378.103-.184.274-.338.405-.364l.297-.057c.176-.034.281-.128.281-.251s-.077-.197-.257-.261l-.347-.124c-.117-.042-.227-.094-.331-.157l-.181-.108c-.03-.016-.062-.024-.09-.024-.073 0-.118.054-.118.141 0 .14.108.255.24.255.03 0 .058-.006.084-.017a.27.27 0 0 1 .076-.025c.038 0 .065.036.065.089a1.04 1.04 0 0 1-.19.522l-.099.124-.075.099c-.027.213-.236.517-.446.652-.051.032-.082.054-.091.066s-.029.047-.066.116c-.017.032-.073.098-.166.198a1.55 1.55 0 0 1-.206.19c-.092.07-.148.121-.165.149l-.083.141c-.017.029-.038.043-.065.043-.074 0-.142-.077-.183-.207-.03-.097-.057-.125-.12-.125a.64.64 0 0 0-.145.025l-.115.016h-.29c-.156 0-.365.038-.463.084-.037.018-.066.026-.085.026-.043 0-.064-.043-.064-.126 0-.262.202-.659.464-.916-.039-.152-.119-.254-.223-.28l-.149-.041c-.015-.008-.025-.023-.025-.039 0-.053.059-.105.19-.167.235-.111.303-.132.434-.132.15 0 .276.051.433.174.224.174.285.207.393.207.176 0 .345-.161.345-.331 0-.114-.063-.241-.218-.438-.22-.281-.298-.464-.298-.702 0-.16.032-.247.174-.478.022-.037.035-.068.035-.089 0-.045-.075-.086-.159-.086-.088 0-.198.032-.347.1-.249.114-.397.22-.397.285 0 .026.015.052.041.07l.083.058c.01.007.017.02.017.034s-.016.043-.034.064-.072.09-.165.215a1.89 1.89 0 0 1-.346.363c-.045.034-.072.062-.084.083l-.066.124c-.055.103-.218.274-.356.371l-.115.091a.75.75 0 0 0-.075.115c-.084.151-.217.338-.305.429l-.223.231c-.064.066-.154.257-.24.511-.068.2-.111.29-.142.29-.014 0-.037-.018-.064-.05-.16-.183-.305-.298-.375-.298-.018 0-.051.015-.096.042-.063.038-.138.067-.223.083l-.297.057c-.249.049-.348.102-.463.248-.029.036-.053.057-.066.057-.029 0-.051-.043-.051-.1l.008-.072c.002-.008.016-.082.042-.223.043-.235.103-.43.165-.537l.149-.256c.026-.046.041-.093.041-.133 0-.19-.178-.345-.447-.387-.055-.008-.083-.028-.083-.06 0-.038.065-.111.166-.188.175-.132.368-.215.505-.215.092 0 .215.032.379.099l.182.074a.49.49 0 0 0 .184.043c.192 0 .379-.175.609-.57.104-.18.149-.308.149-.425 0-.209-.097-.335-.26-.335-.128 0-.25.054-.351.156-.09.091-.137.182-.137.264l.005.084.003.044c0 .067-.023.097-.072.097-.062 0-.135-.061-.278-.232l-.232-.272c-.251-.294-.303-.411-.314-.711-.28-.357-.35-.555-.35-.762l.011-.17c-.201-.241-.24-.334-.24-.572 0-.107.006-.135.05-.221-.099-.145-.13-.21-.157-.322l-.041-.174a.9.9 0 0 0-.132-.305c-.074-.106-.083-.122-.083-.153 0-.086.083-.142.29-.193l.066-.017-.017-.183c0-.048.006-.094.025-.197l.058-.306c.022-.119.034-.21.034-.271 0-.07-.011-.127-.05-.257-.025-.084-.054-.148-.082-.19s-.05-.084-.05-.109.028-.05.066-.05c.02 0 .055.01.083.027.279.152.584.393.801.636.187.208.247.381.289.843.253.188.348.358.348.626 0 .159-.052.313-.166.488l-.165.256c-.049.083-.075.179-.075.279 0 .149.045.289.157.488.123.219.234.314.365.314.217 0 .413-.374.413-.786l-.002-.081v-.223l.025-.198.016-.24.016-.199.006-.11c0-.081-.017-.105-.271-.376-.28-.3-.413-.554-.413-.789 0-.222.191-.408.42-.408a.6.6 0 0 1 .175.032v-.099l.009-.174.049-.214c.04-.171.058-.203.117-.203l.041.013.149.132.181.116.05.049c.012-.103.021-.124.107-.232l.115-.157.083-.124.05-.074c.033-.05.047-.063.069-.063s.038.029.064.121c.011.037.036.075.074.107l.108.091c.087.074.145.183.181.339a.49.49 0 0 1 .165-.149l.133-.074c.049-.028.079-.042.088-.042s.023.02.037.05c.019.043.041.085.066.124.106.171.116.196.116.311v.069.099c.268.074.372.218.372.514 0 .222-.102.451-.315.707-.141.17-.165.222-.165.35 0 .582.439.935 1.181.946l.545.008a2.67 2.67 0 0 1 .901.148c.382.125.507.158.623.158.084 0 .154-.047.154-.103 0-.119-.2-.212-.868-.401-1.361-.384-1.942-.935-1.942-1.836 0-.698.583-1.237 1.336-1.237a2.88 2.88 0 0 1 .82.141l.338.124a.52.52 0 0 0 .178.035c.117 0 .161-.073.178-.291.008-.097.039-.14.101-.14.087 0 .256.136.378.306s.215.404.215.554c0 .176-.092.31-.29.421-.185.282-.413.412-.72.412-.235 0-.635-.159-1.139-.454-.198-.116-.285-.148-.395-.148-.149 0-.244.083-.244.214 0 .119.088.194.334.28.391.139.475.185.587.323.421.073.604.146.868.346.754.242 1.174.707 1.174 1.303 0 .543-.368.935-.88.935a1.59 1.59 0 0 1-.402-.049z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M24.436 9.084a.58.58 0 0 1-.026-.152l.009-.112.026-.321c0-.061-.018-.109-.06-.166l-.099-.132c-.064-.087-.09-.151-.09-.225 0-.048.011-.064.044-.064l.038.008.074.041c.034.019.062.028.083.028.05 0 .075-.077.075-.232 0-.215-.04-.318-.165-.415l-.115-.091c-.02-.016-.034-.043-.034-.067 0-.04.023-.065.061-.065.023 0 .056.007.096.017l.091.025.035.008c.028 0 .041-.022.041-.07l-.002-.029-.025-.174c-.026-.18-.056-.249-.109-.249l-.015.002-.215.041-.036.003c-.258 0-.562-.245-.641-.515l-.157-.537c-.047-.161-.187-.338-.397-.504-.069-.053-.129-.085-.173-.09l-.148-.017c-.027-.003-.045-.019-.045-.04 0-.034.029-.05.09-.05a2.35 2.35 0 0 1 .732.14c.271.089.419.121.552.121.414 0 .663-.216.663-.573 0-.179-.065-.404-.157-.548-.081-.124-.216-.251-.389-.363-.025-.017-.048-.026-.065-.026-.023 0-.042.018-.042.042l.008.107v.01c0 .04-.023.073-.053.073-.046 0-.093-.054-.146-.165-.152-.323-.398-.517-.65-.517-.055 0-.087.019-.087.053 0 .025.015.063.043.109a1.02 1.02 0 0 1 .074.148c.013.032.02.056.02.071s-.013.024-.038.024c-.037 0-.098-.034-.139-.077l-.124-.132-.107-.166a.79.79 0 0 0-.33-.289l-.207-.107c-.041-.022-.078-.033-.105-.033s-.039.013-.039.038.013.059.037.095a.42.42 0 0 1 .076.165c0 .028-.018.05-.041.05-.058 0-.162-.104-.307-.305a.78.78 0 0 1-.158-.464c0-.348.268-.619.613-.619.177 0 .399.074.618.207l.397.24c.142.086.334.149.453.149.066 0 .085-.004.233-.042a.4.4 0 0 0 .107-.041.06.06 0 0 0 .03-.049c0-.032-.02-.047-.072-.05-.184-.014-.566-.164-.817-.323l-.273-.156c-.068-.024-.114-.059-.114-.088 0-.013.014-.02.036-.02l.048.008.36.132.347.124a2.18 2.18 0 0 0 .555.116c.157 0 .265-.108.265-.265 0-.079-.02-.168-.052-.23-.05-.099-.116-.182-.143-.182s-.05.033-.05.088l.004.044.01.106c0 .111-.061.184-.156.184a.46.46 0 0 1-.135-.026l-.537-.173c-.467-.151-.648-.19-.878-.19-.649 0-1.139.438-1.139 1.022 0 .658.452 1.071 1.612 1.472 1.125.39 1.272.479 1.272.775 0 .17-.16.307-.359.307a1 1 0 0 1-.194-.017l-.562-.132c-.345-.084-.637-.116-1.027-.116-.819 0-1.214.125-1.642.521-.099.092-.139.109-.249.109l-.056-.002-.13-.008c-.099 0-.174.047-.234.149l-.132.223c-.018.031-.054.049-.095.049-.086 0-.131-.057-.212-.256a.64.64 0 0 0-.165-.249l-.174-.149c-.026-.023-.041-.044-.041-.06s.015-.044.041-.08a.84.84 0 0 0 .075-.115c.026-.047.039-.081.041-.1l.008-.074c.006-.024.03-.041.059-.041.055 0 .064.036.066.24a.59.59 0 0 0 .066.223l.091.198c.012.026.041.042.075.042.086 0 .194-.201.223-.413.024-.176.043-.232.079-.232s.049.021.119.166c.022.044.056.087.099.124s.075.054.099.058h.009c.068 0 .091-.043.107-.199l.008-.082-.016-.116-.011-.08c0-.053.018-.085.049-.085.018 0 .04.012.062.033l.082.083c.055.055.111.083.166.083.071 0 .115-.031.115-.081 0-.041-.009-.061-.082-.183a2.51 2.51 0 0 1-.058-.107c-.073-.139-.108-.174-.173-.174l-.084.008-.107.041c-.032.012-.054.019-.066.019s-.025-.014-.025-.033c0-.074.049-.192.141-.336.057-.09.104-.311.14-.649l.042-.388c.025-.238.051-.323.099-.323.017 0 .031.007.033.017l.024.124v.099c0 .12.013.218.028.218.035 0 .087-.041.137-.111.036-.05.063-.106.083-.165.027-.089.042-.161.042-.217 0-.191-.081-.273-.339-.345-.026-.007-.043-.016-.049-.025l-.009-.074v-.14l-.017-.1-.032-.083-.042-.091-.066.041c-.169.106-.21.138-.265.206-.075.095-.101.119-.132.119-.023 0-.041-.016-.041-.036l.033-.141.003-.024c0-.181-.102-.356-.284-.487l-.058.066-.058.083-.099.116a.39.39 0 0 0-.092.245l.001.027.001.028c0 .065-.01.088-.041.088l-.018-.008-.066-.082-.082-.099-.149-.091-.074-.058c-.08.173-.083.191-.083.414v.069c0 .074-.024.119-.064.119-.019 0-.054-.014-.101-.041-.037-.021-.082-.034-.123-.034-.12 0-.209.102-.209.241 0 .151.032.255.133.42.065.107.15.194.19.194.005 0 .015-.076.025-.177l-.025-.14-.005-.063c0-.053.019-.086.05-.086.046 0 .075.051.104.19l.066.306.066.388a1.54 1.54 0 0 0 .099.33 1.66 1.66 0 0 0 .14.289c.092.118.102.134.102.16 0 .02-.017.038-.036.038s-.043-.009-.075-.025a.32.32 0 0 0-.099-.033l-.06-.009c-.137 0-.165.11-.165.658 0 .342.071.798.159 1.019l.174.438a.56.56 0 0 1 .042.205c0 .604-.817 1.569-1.678 1.983-.335.161-.434.328-.463.784a1.57 1.57 0 0 1 .289-.116c.244-.068.296-.103.33-.215l.033-.107c.015-.046.045-.068.215-.157.064-.033.109-.07.132-.107.043-.071.076-.106.098-.106s.036.016.036.043l-.002.013-.091.236-.01.058c0 .173.097.317.275.408l.024-.099c.029-.126.058-.209.083-.248l.158-.231c.057-.084.067-.139.067-.352l-.009-.26-.005-.066c0-.043.02-.066.058-.066.029 0 .053.011.055.025l.024.157c.006.033.04.058.081.058.08 0 .182-.102.291-.289.042-.073.059-.13.059-.197a1.9 1.9 0 0 0-.026-.257l-.003-.049c0-.041.024-.067.063-.067.021 0 .037.007.04.016l.058.191c.019.063.063.107.108.107.127 0 .258-.259.272-.537.008-.174.03-.235.083-.235.021 0 .036.008.041.02l.033.107c.012.039.036.065.059.065s.063-.037.115-.099a.83.83 0 0 0 .074-.107c.016-.029.024-.059.024-.087 0-.042-.009-.053-.074-.103s-.091-.104-.091-.173c0-.12.051-.248.198-.504.135-.232.153-.259.186-.259s.055.031.055.07c0 .024-.006.049-.017.072-.022.048-.034.081-.034.099 0 .032.029.059.066.059a.34.34 0 0 0 .15-.05 1.66 1.66 0 0 1 .223-.091l.281-.099c.052-.016.111-.025.17-.025.048 0 .096.006.144.017.094.022.158.034.19.034a.13.13 0 0 0 .133-.131.35.35 0 0 0-.051-.143l-.074-.115c-.036-.038-.051-.059-.051-.074l.009-.033c.01-.02.028-.033.047-.033.065 0 .193.133.308.323.648 1.057.962 1.391 1.438 1.535.496.15.613.255.613.551 0 .157-.083.346-.258.588-.33.458-.447.726-.447 1.031l.001.068.091-.042c.068-.034.112-.054.133-.058l.206-.041c.065-.013.117-.09.117-.173l-.001-.016-.016-.182-.002-.029c0-.102.05-.187.112-.187.023 0 .049.021.063.05l.132.28c.053.115.116.17.288.248zM17.663 3.42l-.008.132-.002.081c0 .134.021.192.071.192s.141-.102.179-.215a.84.84 0 0 0 .042-.224.41.41 0 0 0-.042-.156c-.037-.09-.075-.132-.121-.132-.071 0-.104.091-.118.322z" data-v-7c4b1471></path><path d="M17.089 1.893l.116.115c.169.169.285.513.285.846l-.004.137-.025.388-.008.325c0 1.079.314 1.815.776 1.815.116 0 .258-.107.356-.265.021-.034.049-.06.066-.06.026 0 .039.022.041.068l.017.182.016.156.025.174.025.162c0 .083-.017.092-.215.119a.55.55 0 0 0-.231.091c-.128.082-.203.139-.215.165l-.091.19c-.012.024-.032.041-.053.041-.048 0-.113-.073-.204-.231-.084-.146-.121-.23-.121-.269 0-.026.021-.037.072-.037h.017l.099.008.074-.008.067-.008c.028-.007.049-.031.049-.061 0-.02-.013-.038-.033-.046l-.372-.158c-.089-.038-.185-.134-.272-.272-.095-.152-.151-.285-.151-.36 0-.059.025-.097.065-.097.016 0 .037.006.061.019a.31.31 0 0 0 .097.033c.014 0 .03-.006.044-.017s.025-.027.025-.04c0-.024-.025-.055-.066-.085-.309-.219-.497-.485-.497-.704 0-.064.015-.094.047-.094l.061.006c.045.009.099.017.116.017.045 0 .076-.025.076-.064 0-.024-.03-.054-.076-.076-.206-.103-.257-.162-.388-.455-.101-.224-.153-.318-.24-.429.108-.052.166-.067.257-.067.113 0 .184.074.264.273.05.126.105.211.137.211.015 0 .021-.026.021-.095a.96.96 0 0 0-.034-.248c-.02-.075-.029-.137-.024-.182l-.017-.116-.091-.05c-.084-.045-.141-.154-.141-.271a.61.61 0 0 1 .017-.15l.05-.191a1.6 1.6 0 0 0 .059-.349v-.071l.074.082zm.992 5.953a1.22 1.22 0 0 1-.396-.123.75.75 0 0 0-.31-.084c-.131 0-.231.041-.351.141l.074.057.166.116.158.124c.04.032.089.05.139.05.162 0 .299-.074.522-.281zm3.182 1.502a.47.47 0 0 1 .165-.215c.231-.198.339-.345.339-.462v-.083-.099c0-.106.017-.141.073-.141.025 0 .045.01.051.025l.041.098c.01.023.036.037.07.037.059 0 .101-.042.203-.21.043-.071.061-.117.061-.164 0-.029-.006-.061-.02-.093l-.058-.131a.4.4 0 0 1-.034-.129c0-.054.027-.086.074-.086.025 0 .047.01.06.026l.065.082c.015.018.019.029.044.029s.085-.029.106-.07l.066-.132c.017-.033.025-.059.025-.075 0-.028-.026-.056-.067-.073l-.116-.05c-.135-.058-.248-.291-.248-.512 0-.025.009-.061.025-.107l.05-.141a.41.41 0 0 0 .025-.137c0-.081-.025-.159-.082-.251l-.099-.173c-.048-.123-.076-.164-.117-.164-.029 0-.057.011-.106.039l-.083.05-.116.133-.116.057c-.069.035-.135.184-.135.309 0 .178.039.275.309.757.114.204.168.355.168.47 0 .336-.343.613-.903.727-.669.137-.815.248-.892.678.116-.06.162-.068.366-.068l.121.002h.019c.111 0 .125-.008.196-.107a.62.62 0 0 1 .273-.23l.132-.058.107-.058.033-.009c.03 0 .046.012.046.035 0 .028-.027.076-.079.139-.08.097-.126.196-.126.272 0 .11.018.166.085.265z" data-v-7c4b1471></path><path d="M20.223 8.069c.095.059.191.145.248.223.079.108.09.116.169.116a.52.52 0 0 0 .12-.017l.074-.016c.141-.027.183-.047.183-.085 0-.023-.013-.046-.035-.064l-.099-.082-.091-.083a.44.44 0 0 0-.261-.092c-.103 0-.186.027-.309.1zm2.675.446l.049-.009.083-.008.066.008.045.003c.084 0 .127-.009.162-.036.051-.039.104-.101.157-.19.021-.035.032-.068.032-.094 0-.056-.051-.088-.141-.088-.227 0-.366.126-.454.413z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M19.816 2.81c.009.077.049.14.09.14.035 0 .09-.118.176-.263.117-.199.214-.365.214-.471 0-.035-.018-.062-.041-.062l-.009.003-.091.084a.94.94 0 0 1-.273.148c-.106.038-.136.07-.136.14l.012.067.069.143-.011.072zm-.724-.611c-.029-.024-.07-.042-.092-.042-.035 0-.05.018-.05.06l.002.024c.006.045.018.092.032.14a.72.72 0 0 0 .109.231l.125.209a.43.43 0 0 0 .18.114l.006.001c.022 0 .041-.014.043-.034l.026-.165.041-.149.005-.035c0-.04-.023-.076-.062-.096l-.141-.075-.222-.181z" data-v-7c4b1471></path><path d="M19.637 3.248h-.044l-.096-.009c-.003-.001-.037-.021-.1-.059-.037-.022-.077-.033-.116-.033-.076 0-.121.047-.121.126 0 .062.037.182.105.344l.108.264c.091.231.185.33.31.33.107 0 .193-.088.26-.263l.091-.372c.044-.116.153-.164.153-.245 0-.095-.106-.186-.196-.186l-.048.009-.124.084-.182.008z" data-v-7c4b1471></path></g><path d="M19.627 3.477h-.099c-.015 0-.03.009-.041.025-.016.023-.025.038-.025.041l.041.198.033.173c.019.095.034.133.105.133.081 0 .15-.055.159-.157l.017-.182.016-.188c0-.018-.009-.036-.024-.051s-.027-.018-.043-.018l-.007.001-.132.025z" class="C" data-v-7c4b1471></path><path d="M8.2 5.467l.033.124.066.264c.104.472.192.595.421.595.131 0 .24-.073.273-.182l.05-.166c.011-.038.04-.06.079-.06.089 0 .131.093.135.3.002.059.021.194.059.404.022.124.032.245.032.36a.59.59 0 0 1-.049.259c.028.162.05.38.05.506 0 .204-.038.394-.099.493a1.04 1.04 0 0 1 .042.311 1.55 1.55 0 0 1-.026.275l-.044.285a.35.35 0 0 0 .019.111c.016.047.025.081.025.099 0 .023-.011.041-.025.041l-.165-.066-.182-.099c-.059-.032-.099-.064-.19-.148-.161.144-.208.166-.628.281-.084.022-.146.048-.182.074-.052.037-.09.057-.11.057-.071 0-.105-.114-.105-.35 0-.175.018-.337.05-.46a.94.94 0 0 0-.295-.074 1.68 1.68 0 0 0-.126.016l-.031.003c-.024 0-.044-.036-.044-.079 0-.032.022-.112.051-.179l.082-.198c.03-.072.098-.154.198-.239a.53.53 0 0 1 .215-.125l.297-.057c.177-.034.282-.128.282-.251s-.08-.199-.257-.261l-.356-.124a1.91 1.91 0 0 1-.323-.157c-.197-.117-.23-.132-.277-.132-.071 0-.112.054-.112.147 0 .147.096.25.232.25.028 0 .056-.006.083-.017.04-.016.069-.025.085-.025.037 0 .058.022.058.063l-.002.036-.017.14c-.014.12-.092.265-.264.496l-.074.099a1.14 1.14 0 0 1-.446.652l-.091.066c-.007.008-.029.047-.066.116-.017.032-.072.098-.165.198-.07.076-.139.139-.207.19-.092.07-.148.121-.165.149l-.083.141c-.015.025-.04.041-.067.041-.073 0-.14-.076-.182-.206-.03-.097-.057-.125-.12-.125-.032 0-.081.009-.144.025l-.116.016h-.298c-.148 0-.358.039-.454.084-.038.018-.067.026-.085.026s-.033-.011-.055-.051c-.015-.028-.018-.041-.018-.066 0-.151.108-.435.249-.661.09-.143.119-.176.223-.264-.033-.14-.123-.249-.231-.28l-.141-.041c-.019-.006-.033-.022-.033-.04 0-.043.067-.102.191-.167.161-.085.324-.132.454-.132.137 0 .27.054.422.174.224.175.284.207.393.207.177 0 .345-.162.345-.331 0-.113-.06-.234-.218-.438-.23-.298-.299-.452-.299-.671 0-.136.042-.309.093-.386l.083-.123c.017-.028.028-.055.028-.08 0-.055-.064-.095-.149-.095-.13 0-.381.093-.581.215-.099.061-.166.132-.166.176 0 .026.013.05.034.064l.091.058c.011.007.018.02.018.034s-.014.038-.034.064l-.165.215a2.76 2.76 0 0 1-.347.363.57.57 0 0 0-.083.083c-.001.001-.023.042-.066.124a1.3 1.3 0 0 1-.364.371c-.061.042-.099.072-.115.091s-.034.053-.066.115c-.021.04-.057.098-.108.173-.081.12-.148.205-.198.256L3.4 8.67c-.071.071-.133.209-.231.511-.058.183-.111.29-.142.29-.014 0-.037-.018-.065-.051-.164-.187-.305-.298-.379-.298-.017 0-.048.014-.092.042-.061.038-.136.067-.223.083l-.305.057c-.231.044-.322.094-.455.248-.032.036-.058.057-.072.057-.025 0-.044-.041-.044-.096l.008-.077.042-.223c.044-.239.098-.416.165-.537l.14-.256c.028-.05.042-.095.042-.134 0-.19-.197-.365-.437-.387-.061-.005-.084-.02-.084-.054 0-.042.067-.125.159-.193.165-.126.382-.215.52-.215.084 0 .213.035.371.099l.182.074a.49.49 0 0 0 .184.043c.192 0 .379-.175.609-.57.104-.179.149-.308.149-.424 0-.208-.099-.336-.262-.336a.53.53 0 0 0-.357.156c-.083.084-.127.172-.127.256l.003.092.001.02c0 .077-.026.12-.072.12-.078 0-.191-.111-.516-.504-.242-.293-.296-.418-.306-.711-.28-.357-.35-.508-.35-.762l.011-.17c-.194-.233-.249-.36-.249-.574 0-.078.013-.124.059-.218-.099-.145-.13-.21-.157-.322l-.041-.174c-.028-.118-.073-.22-.132-.305-.075-.107-.083-.121-.083-.158 0-.096.066-.134.356-.205-.012-.08-.017-.142-.017-.183 0-.048.006-.094.025-.197l.058-.306c.023-.123.035-.211.035-.264s-.009-.098-.06-.263a.9.9 0 0 0-.074-.19c-.027-.042-.05-.094-.05-.113 0-.029.02-.045.057-.045.082 0 .298.131.522.316.474.392.572.549.637 1.023l.025.166c.227.167.347.369.347.585a1.03 1.03 0 0 1-.165.529l-.166.256a.5.5 0 0 0-.074.273c0 .155.045.298.157.494.127.223.235.314.368.314.218 0 .409-.379.409-.81v-.058l-.001-.149c0-.084.005-.167.018-.272l.024-.24.009-.199.003-.13c0-.046-.024-.093-.086-.158l-.181-.198c-.263-.287-.407-.556-.407-.757a.42.42 0 0 1 .422-.439.6.6 0 0 1 .175.032v-.056c0-.364.075-.633.175-.633l.04.013.149.132.181.116.05.049c0-.102.005-.113.108-.232.025-.028.063-.081.115-.157l.082-.124.042-.074c.022-.042.044-.062.068-.062s.042.03.072.12c.019.056.032.071.174.198.049.044.083.081.098.108s.036.078.067.164l.025.067a.5.5 0 0 1 .165-.149l.124-.074c.046-.028.078-.043.092-.043s.026.02.041.05.035.07.066.124c.088.153.117.227.117.299l-.002.081V1.9c.271.085.373.225.373.517 0 .21-.11.451-.323.705-.142.169-.158.202-.158.32 0 .611.426.965 1.175.976l.553.008c.356.006.569.041.9.148.393.128.507.158.621.158.038 0 .076-.012.107-.034s.049-.049.049-.073c0-.116-.203-.209-.867-.397-1.363-.386-1.941-.934-1.941-1.836 0-.697.58-1.237 1.33-1.237a2.89 2.89 0 0 1 .826.141l.338.124c.062.023.122.035.178.035.115 0 .174-.096.177-.291.003-.104.026-.14.094-.14.095 0 .242.119.377.306s.224.401.224.542c0 .188-.088.319-.29.432-.185.282-.414.412-.72.412-.239 0-.578-.135-1.139-.454-.196-.111-.295-.148-.398-.148-.146 0-.241.084-.241.214 0 .119.092.196.334.28.357.125.445.174.586.323.421.073.604.146.868.346.725.23 1.175.724 1.175 1.291 0 .551-.373.947-.892.947a1.6 1.6 0 0 1-.391-.05z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M9.021 9.083a.91.91 0 0 1-.03-.199l.006-.065.025-.132.009-.107c0-.115-.019-.196-.058-.248L8.872 8.2c-.032-.044-.061-.097-.082-.158-.011-.03-.017-.054-.017-.069 0-.037.023-.061.06-.061.022 0 .031.004.106.048.032.019.06.028.081.028.049 0 .076-.08.076-.22 0-.159-.015-.248-.05-.288l-.124-.14-.107-.091c-.026-.022-.044-.05-.044-.072 0-.035.027-.06.067-.06s.117.018.183.042l.038.008c.034 0 .047-.02.047-.068l-.002-.03-.025-.174-.049-.181c-.016-.059-.025-.068-.063-.068l-.021.003-.206.041-.033.003c-.271 0-.557-.228-.644-.515l-.165-.537c-.051-.166-.175-.323-.397-.504-.067-.055-.124-.086-.165-.09l-.149-.017c-.027-.003-.045-.019-.045-.041 0-.034.032-.049.103-.049l.272.024a2.14 2.14 0 0 1 .446.116c.247.085.414.121.554.121.413 0 .661-.223.661-.597 0-.256-.134-.545-.338-.731-.117-.104-.233-.182-.273-.182-.023 0-.042.019-.042.044l.008.105v.01c0 .041-.024.073-.057.073-.042 0-.089-.054-.143-.165-.156-.325-.404-.518-.661-.518-.043 0-.074.018-.074.044 0 .017.013.057.041.119l.066.148a.28.28 0 0 1 .027.076c0 .009-.016.018-.035.018-.038 0-.098-.034-.14-.078l-.123-.132-.108-.166c-.086-.132-.155-.192-.339-.289l-.198-.107c-.038-.022-.073-.033-.103-.033s-.044.014-.044.043.011.058.032.09c.061.094.085.141.085.166s-.019.049-.043.049c-.064 0-.197-.13-.315-.305-.097-.145-.149-.307-.149-.464 0-.355.259-.619.61-.619.181 0 .41.077.621.207l.388.24a1.02 1.02 0 0 0 .452.149 1.11 1.11 0 0 0 .242-.042c.036-.01.07-.023.099-.041.023-.014.039-.032.039-.046s-.009-.032-.023-.036l-.049-.016-.182-.034c-.047-.008-.122-.036-.223-.083l-.414-.206a1.61 1.61 0 0 0-.272-.156c-.067-.023-.109-.053-.109-.079s.013-.036.037-.036.054.011.088.032c.122.076.228.116.306.116l.355.124c.176.062.433.116.549.116.15 0 .271-.122.271-.273a.72.72 0 0 0-.143-.362c-.019-.027-.039-.042-.056-.042-.033 0-.051.024-.051.066l.008.067.013.098c0 .107-.072.191-.164.191a.44.44 0 0 1-.13-.026l-.537-.173c-.45-.145-.658-.19-.884-.19-.642.001-1.132.442-1.132 1.022 0 .657.462 1.081 1.603 1.472 1.123.385 1.281.479 1.281.768 0 .166-.168.314-.356.314a1.03 1.03 0 0 1-.198-.017l-.562-.132a4.18 4.18 0 0 0-1.028-.116c-.81 0-1.26.142-1.649.521-.097.095-.129.109-.241.109l-.056-.002-.116-.008h-.015c-.097 0-.169.045-.233.149l-.141.223c-.023.031-.058.049-.094.049-.074 0-.119-.056-.203-.256a.7.7 0 0 0-.173-.249l-.165-.149c-.027-.024-.042-.046-.042-.063s.012-.045.033-.077c.079-.115.107-.166.116-.215.022-.11.025-.115.071-.115s.066.051.07.24a.6.6 0 0 0 .067.223l.091.198c.011.026.041.042.074.042.085 0 .194-.201.223-.413.024-.177.042-.232.079-.232.015 0 .029.006.036.017s.034.061.074.148a.33.33 0 0 0 .108.124.34.34 0 0 0 .099.058l.01.001c.067 0 .09-.044.106-.2l.008-.082-.017-.116-.012-.08c0-.053.019-.085.049-.085.018 0 .04.012.062.033l.083.083c.055.054.11.082.163.082.069 0 .111-.028.111-.073 0-.04-.015-.079-.076-.191l-.058-.107c-.066-.125-.119-.174-.181-.174l-.083.008-.099.041c-.03.012-.052.019-.065.019s-.026-.014-.026-.033c0-.074.048-.191.14-.336.065-.105.11-.328.132-.649.028-.431.086-.711.146-.711.041 0 .057.035.057.127l-.004.113-.003.079c0 .077.017.139.037.139.071 0 .155-.105.222-.276.028-.072.043-.128.043-.168l-.009-.038-.165-.306-.165-.049c-.031-.009-.051-.023-.058-.037V1.9v-.09l-.016-.1-.033-.083-.041-.091c-.181.098-.267.163-.331.247s-.101.12-.133.12c-.021 0-.04-.02-.04-.044s.008-.065.025-.134l.005-.046c0-.178-.108-.352-.286-.465l-.05.066-.058.083-.099.116a.48.48 0 0 0-.083.165.32.32 0 0 0-.017.094v.013l.001.03c0 .069-.007.086-.038.086l-.02-.009-.058-.082-.09-.099-.14-.091-.074-.058-.033.074c-.026.058-.04.102-.041.133l-.009.132-.008.14.001.024c0 .072-.017.1-.061.1-.015 0-.057-.018-.097-.041a.24.24 0 0 0-.12-.034c-.129 0-.211.089-.211.228 0 .165.049.323.133.433l.107.14c.03.037.084.07.087.049l.02-.173-.025-.14-.005-.063c0-.053.018-.086.049-.086.047 0 .075.051.105.19l.066.306.067.388a1.54 1.54 0 0 0 .099.33 1.71 1.71 0 0 0 .141.289c.092.118.102.134.102.16 0 .02-.017.038-.035.038s-.043-.009-.074-.025a.34.34 0 0 0-.099-.033l-.065-.009c-.133 0-.16.114-.16.665a3.35 3.35 0 0 0 .159 1.012l.173.438a.54.54 0 0 1 .041.205c0 .602-.799 1.547-1.677 1.983-.327.161-.431.317-.454.676l-.008.108a1.15 1.15 0 0 1 .28-.116c.246-.064.294-.095.331-.215l.032-.107c.013-.04.033-.055.223-.157.065-.034.111-.072.133-.107.041-.07.076-.107.098-.107s.032.01.032.027l-.007.029-.082.236-.01.058c0 .175.096.317.274.408.029-.191.049-.256.108-.346l.148-.231c.055-.086.078-.19.078-.353a1.53 1.53 0 0 0-.02-.258l-.006-.065c0-.042.022-.066.058-.066.029 0 .052.01.055.024l.033.157c.006.034.041.058.08.058.08 0 .183-.103.291-.289a.44.44 0 0 0 .061-.219l-.012-.112-.025-.123-.002-.024c0-.054.03-.092.073-.092.019 0 .034.007.037.016l.049.191c.017.063.061.107.107.107.056 0 .151-.096.207-.207a.73.73 0 0 0 .075-.338c0-.176.018-.227.083-.227.021 0 .037.007.041.02l.033.107c.013.039.036.065.059.065.034 0 .127-.1.189-.206.017-.027.025-.055.025-.077 0-.035-.024-.071-.075-.113-.086-.071-.091-.082-.091-.19s.017-.158.091-.289l.107-.198c.104-.204.142-.257.18-.257.032 0 .06.034.06.073 0 .022-.006.045-.017.068a.28.28 0 0 0-.033.098c0 .032.027.059.061.059s.088-.017.146-.05c.022-.013.099-.042.231-.091l.273-.099a.49.49 0 0 1 .165-.025.66.66 0 0 1 .148.017l.191.034c.075 0 .141-.064.141-.138 0-.055-.057-.163-.133-.251-.028-.033-.042-.058-.042-.075l.01-.033c.011-.02.028-.033.044-.033.056 0 .186.139.303.323.711 1.119.966 1.39 1.446 1.535.486.148.613.261.613.55 0 .158-.081.341-.258.589-.36.503-.447.706-.447 1.038v.061l.091-.042c.067-.034.112-.054.132-.058l.206-.041c.062-.013.117-.092.117-.169l-.001-.021-.025-.182-.002-.024c0-.064.032-.138.076-.174.014-.01.028-.018.043-.018.023 0 .051.022.065.051l.132.28c.054.115.117.169.288.248zm-6.66-5.985c-.06 0-.091.052-.097.165l-.016.156-.02.2c0 .118.035.205.083.205s.135-.101.177-.216c.025-.07.037-.13.037-.193 0-.055-.013-.121-.037-.187-.03-.084-.076-.132-.126-.132zm-.685-1.205l.116.115c.189.188.282.501.282.945 0 .098-.005.163-.026.425l-.017.34c0 1.024.333 1.798.774 1.798.139 0 .229-.066.359-.264.021-.032.054-.06.075-.06s.039.021.041.068l.017.182.017.156.016.174.025.157.002.02c0 .046-.02.072-.059.079l-.149.025c-.074.012-.152.043-.231.091-.125.075-.199.132-.215.165l-.091.19c-.013.024-.032.041-.052.041-.048 0-.125-.084-.213-.231-.075-.126-.121-.228-.121-.27 0-.026.02-.036.073-.036h.016l.107.008.066-.008.075-.008c.028-.004.05-.028.05-.058 0-.022-.014-.041-.033-.049l-.371-.158a.58.58 0 0 1-.281-.272c-.102-.196-.15-.313-.15-.371 0-.051.027-.085.067-.085.014 0 .034.006.058.018a.33.33 0 0 0 .105.033c.032 0 .061-.025.061-.052s-.03-.062-.066-.089c-.31-.225-.488-.484-.488-.707 0-.062.015-.092.048-.092l.06.007.107.016h.005c.04 0 .071-.026.071-.064 0-.023-.028-.054-.068-.076l-.132-.074c-.046-.029-.085-.068-.115-.116a7.22 7.22 0 0 1-.14-.264l-.116-.223-.075-.14a.75.75 0 0 0-.05-.066.52.52 0 0 1 .257-.067c.113 0 .184.074.264.273.05.126.105.211.136.211.016 0 .021-.027.021-.097a.94.94 0 0 0-.034-.246c-.02-.075-.029-.137-.025-.182l-.016-.116-.099-.05c-.022-.01-.048-.04-.075-.082-.046-.072-.066-.133-.066-.188 0-.035.009-.092.025-.15.084-.315.108-.438.108-.54l-.001-.071.075.082z" data-v-7c4b1471></path><path d="M2.666 7.846c-.115-.017-.169-.03-.24-.058l-.157-.065c-.133-.057-.233-.084-.316-.084-.125 0-.227.042-.344.141l.074.057.165.116.157.124a.22.22 0 0 0 .136.05c.162 0 .303-.076.525-.281zm3.179 1.503c.045-.103.067-.131.165-.215.231-.198.338-.345.338-.462v-.083-.099c0-.106.018-.141.073-.141.025 0 .044.01.051.025l.041.098c.01.023.037.038.07.038s.079-.038.121-.096l.083-.115c.041-.058.06-.107.06-.161a.27.27 0 0 0-.018-.095l-.058-.132a.39.39 0 0 1-.034-.129c0-.054.027-.086.074-.086.024 0 .046.01.059.026l.066.082c.014.018.038.029.063.029.03 0 .065-.029.086-.07l.066-.132c.017-.033.026-.059.026-.074 0-.026-.027-.052-.075-.075l-.107-.05c-.142-.065-.249-.292-.249-.529a.24.24 0 0 1 .017-.09l.058-.141a.33.33 0 0 0 .025-.132c0-.086-.028-.172-.083-.256l-.108-.173c-.049-.13-.074-.167-.114-.167l-.042.01-.058.033-.091.05-.107.133-.115.057c-.069.035-.135.182-.135.303 0 .215.024.274.307.762.11.188.168.352.168.472 0 .338-.303.579-.911.725l-.38.091c-.28.067-.445.259-.504.587.116-.06.161-.068.366-.068l.122.002h.019c.11 0 .125-.008.195-.107.082-.114.162-.182.273-.23l.132-.058.107-.058.033-.009c.03 0 .046.012.046.034 0 .032-.03.086-.078.14-.095.106-.132.194-.132.32 0 .08.019.126.091.217z" data-v-7c4b1471></path><path d="M4.805 8.068a.95.95 0 0 1 .248.223c.079.108.09.116.168.116.033 0 .074-.006.121-.017l.074-.016c.136-.026.183-.046.183-.08 0-.02-.015-.045-.042-.069l-.091-.082-.091-.083a.45.45 0 0 0-.273-.093c-.099 0-.157.02-.298.101zm2.679.447l.049-.009.075-.008.074.008.036.003c.183 0 .36-.155.36-.319 0-.057-.052-.088-.146-.088-.223 0-.362.128-.449.414z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M4.402 2.81c.009.076.049.14.089.14.035 0 .075-.091.161-.237.117-.198.229-.392.229-.497 0-.035-.017-.062-.041-.062l-.008.003-.091.083c-.066.06-.118.088-.281.148-.093.036-.127.073-.127.141l.012.066.041.116.017.099z" data-v-7c4b1471></path><path d="M3.896 2.379l.14.075c.039.02.06.047.06.08l-.01.052c-.02.07-.004.147-.005.176-.007.148-.119.146-.165.146-.027 0-.013-.017-.078-.091l-.182-.206c-.056-.063-.105-.208-.124-.371l-.001-.021c0-.045.014-.062.05-.062.022 0 .063.018.091.042l.223.181zm.323.867h-.045l-.095-.009a3.81 3.81 0 0 1-.099-.059c-.037-.022-.078-.033-.118-.033-.075 0-.123.041-.123.104s.033.181.109.367l.108.264c.094.232.186.331.307.331.108 0 .123-.106.196-.289.165-.415.257-.476.257-.623 0-.1-.022-.127-.113-.127l-.077-.018-.132.084-.174.008z" data-v-7c4b1471></path></g><path d="M4.212 3.477h-.107c-.016 0-.032.009-.041.025l-.016.041.041.198.033.173c.018.095.053.133.124.133.081 0 .131-.055.14-.157l.016-.182.016-.188c0-.018-.009-.036-.025-.051s-.027-.018-.041-.018l-.008.001-.132.025z" class="C" data-v-7c4b1471></path><path d="M14.987 10.884c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3h-.705c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3h-.705c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3H9.98v4.28h.81v1.641h.95v-1.64h2.399v1.64h.949v-1.64h.809v-4.28h-.911z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M10.965 15.539h.492v.632h-.492v-.632zm0 .774h.492v.2h-.492v-.2zm3.351-.774h.492v.632h-.492v-.632zm0 .774h.492v.2h-.492v-.2z" data-v-7c4b1471></path></g><g fill="#e73337" data-v-7c4b1471><path d="M10.309 11.234h5.256v.195h-5.256v-.195zm.002 3.599h5.254v-3.22h-5.256l.002 3.22z" data-v-7c4b1471></path></g><g class="C" data-v-7c4b1471><use xlink:href="#B" data-v-7c4b1471></use><path d="M12.715 11.1c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1zm1.598 0c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1zm-2.633 2.158c.339.424.766.633 1.294.633.489 0 .826-.175 1.222-.633-.395-.405-.763-.575-1.246-.575-.508 0-.966.207-1.27.575z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M12.938 12.873c-.364 0-.659.113-1.008.383.289.286.629.433 1.002.433.42 0 .694-.116 1.006-.425-.358-.284-.633-.391-1-.391zm-.014.483c-.195 0-.36-.052-.569-.183.174-.094.37-.142.578-.142a1.38 1.38 0 0 1 .597.142 1.03 1.03 0 0 1-.605.183zm2.079 1.191c-.157 0-.284-.127-.284-.283s.127-.284.284-.284.284.127.284.284-.127.283-.284.283z" data-v-7c4b1471></path></g><path d="M15.002 14.049c-.119 0-.216.097-.216.216s.097.215.216.215.216-.097.216-.215-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path><path d="M10.887 14.547c-.161 0-.293-.127-.293-.283s.127-.284.284-.284.284.127.284.284a.28.28 0 0 1-.275.283z" class="B" data-v-7c4b1471></path><path d="M10.876 14.049c-.119 0-.216.097-.216.216s.101.215.225.215c.114 0 .208-.097.208-.215s-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path><path d="M10.877 12.593c-.156 0-.284-.127-.284-.284s.127-.283.284-.283.284.127.284.283-.128.284-.284.284z" class="B" data-v-7c4b1471></path><use xlink:href="#C" class="C" data-v-7c4b1471></use><path d="M15.003 12.593c-.157 0-.284-.127-.284-.284s.127-.283.284-.283.284.127.284.283-.127.284-.284.284z" class="B" data-v-7c4b1471></path><path d="M15.002 12.092c-.119 0-.216.097-.216.216s.097.216.216.216.216-.097.216-.216-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path></g><defs data-v-7c4b1471><clipPath id="A" data-v-7c4b1471><path fill="#fff" d="M0 0h148.235v30H0z" data-v-7c4b1471></path></clipPath><path id="B" d="M11.113 11.1c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1z" data-v-7c4b1471></path><path id="C" d="M10.876 12.092c-.119 0-.216.097-.216.216s.097.216.216.216.216-.097.216-.216-.097-.216-.216-.216z" data-v-7c4b1471></path></defs></svg></a></div><div class="gh-navAndProfile"><nav class="gh-nav"><div class="gh-accordion gh-mobile-nav"><div class="gh-menu-layer"><div class="gh-logo-mobile-container"><a href="https://www.cambridge.org/academic"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="149" height="30" viewbox="0 0 149 30" fill="none" aria-label="Homepage Cambridge University Press" class="gh-logo" data-v-7c4b1471><g clip-path="url(#A)" data-v-7c4b1471><g class="B" data-v-7c4b1471><path d="M147.065 1.786l.036 2.785-.445.114c-.1-1.217-.376-2.211-2.324-2.211h-2.618v5.147h2.247c1.103 0 1.402-.616 1.527-1.679h.447v4.304l-.446.007c-.069-1.029-.311-1.829-1.518-1.829h-2.257v4.52c0 1.314 1.133 1.314 2.332 1.314 2.151 0 3.149-.135 3.75-2.31l.441.098-.602 2.901h-9.774v-.482h.186c.75 0 1.443-.14 1.443-1.162V3.43c0-1.022-.693-1.163-1.443-1.163h-.186v-.481h9.204zm-53.267 0h-5.895v.481h.26c.486 0 1.388 0 1.388 1.525v9.569c0 .815-.748 1.104-1.388 1.104h-.26v.482h5.724v-.482h-.261c-.932 0-1.591-.115-1.591-1.6V8.766h1.454l3.918 6.18h3.371v-.492c-.889-.048-1.311-.356-1.828-1.153l-3.23-4.915.085-.035c1.152-.466 2.615-1.414 2.615-3.286 0-2.207-1.447-3.28-4.363-3.28zm-.749.536c1.685 0 2.804 1.123 2.804 2.858 0 2.138-.903 3.051-2.953 3.051h-1.125l.001-5.826a8.87 8.87 0 0 1 1.273-.083zm13.772-.539h-5.501v.481h.186c.751 0 1.443.142 1.443 1.163V13.3c0 1.021-.692 1.162-1.443 1.162h-.186v.481h5.501v-.481h-.186c-.75 0-1.444-.14-1.444-1.162V3.427c0-1.021.694-1.163 1.444-1.163h.186v-.481zM31.488 8.355c0 4.435 2.709 6.939 6.956 6.939 2.089 0 3.924-.818 5.112-2.354l-.345-.644c-1.055 1.355-2.452 2.089-4.412 2.089-3.487 0-4.927-3.074-4.927-6.324 0-3.652 1.781-5.919 4.647-5.919 2.79 0 3.616 1.617 4.112 3.383l.433-.056-.139-3.046c-1.373-.641-2.776-.945-4.46-.945-1.88 0-3.545.638-4.814 1.775-1.367 1.224-2.162 3.146-2.162 5.102zM59.883 4.33l4.761 10.706h.397l4.553-10.921.401 8.416.019.944c0 .893-.643.931-1.533.984l-.036.002v.475h5.464v-.475c-1.008-.038-1.544-.13-1.622-1.151l-.509-8.729c-.112-1.86.151-2.32 1.632-2.32h.07l.001-.485h-3.654l-4.231 9.975-4.472-9.974h-3.727v.479h.152c1.046 0 1.669.644 1.602 1.779l-.516 8.32c-.069 1.215-.161 2.046-1.657 2.106h-.169v.478h4.439v-.478h-.113c-1.21-.045-1.768-.203-1.689-2.003l.437-8.128zm55.196 10.61c1.826 0 3.63-.436 4.919-1.712 1.28-1.27 2.015-3.056 2.015-4.9 0-1.974-.668-3.636-1.93-4.81-1.28-1.191-3.113-1.741-5.301-1.741l-5.887.005v.482h.187c.749 0 1.443.141 1.443 1.162v9.871c0 1.023-.694 1.163-1.443 1.163h-.187v.483l6.184-.004zM114.1 2.349c3.704 0 5.505 1.986 5.505 6.347 0 2.806-1.514 5.669-4.841 5.669-1.413 0-2.003-.225-2.003-1.621l.001-10.334a17.99 17.99 0 0 1 1.338-.062zm19.119 11.918c-.493.224-1.326.347-2.132.33-3.496 0-5.062-3.303-5.062-6.636 0-3.628 1.855-5.882 4.914-5.882 2.267 0 3.277.887 3.823 3.15l.441-.028-.209-3.141c-1.018-.34-2.623-.651-4.184-.651-3.487 0-7.192 2.112-7.192 6.746 0 4.357 2.81 7.173 7.156 7.173 1.801 0 3.422-.307 4.63-.695l.002-3.592c0-.706.374-1.079 1.082-1.079h.153v-.47h-5.062v.47h.154c.973 0 1.487.374 1.487 1.079l-.001 3.226zM80.916 8.129c2.063 0 3.294 1.147 3.294 3.168 0 1.797-1.21 3.117-2.814 3.117-1.336 0-1.746-.368-1.746-1.654V8.129h1.265zm-.073-5.86c1.733 0 2.612.865 2.612 2.723 0 1.682-.999 2.562-2.813 2.562h-.991V2.269h1.192zm1.507 5.51c1.616-.261 3.451-1.032 3.451-3.117 0-1.883-1.511-2.878-4.256-2.878l-5.846.003v.481h.261c.484 0 1.387 0 1.387 1.524v9.548c0 .817-.748 1.121-1.387 1.121h-.261v.477l6.451.001c2.463 0 4.358-1.655 4.358-3.72 0-.866-.303-1.609-.877-2.195-.742-.759-1.624-1.124-3.281-1.245zM50.387 1.784l4.289 11.418c.318.903.783 1.258 1.463 1.258h.04l-.001.486H51.08v-.486h.116c.991 0 1.38-.415 1.082-1.327L51.24 10.24h-4.149l-.956 2.654c-.42 1.245.237 1.553 1.201 1.553h.174l.001.494h-4.008l.002-.494h.045c.811 0 1.358-.498 1.668-1.305 0-.001 4.18-11.36 4.18-11.36l.992.002zm-3.065 7.81h3.684l-1.84-5.127-1.845 5.127zM32.257 20.335c.66 0 .829.18.829.672v4.573c0 1.56 1.153 2.449 3.111 2.449 1.982 0 3.063-1.02 3.063-2.677v-4.141c0-.744.18-.876.793-.876v-.348l-1.093.024a30.18 30.18 0 0 1-1.237-.024v.348c.625.012.925.012.925 1.116v3.997c0 1.272-.757 2.077-2.102 2.077-1.333 0-2.054-.84-2.054-2.089v-4.43c0-.612.264-.672.781-.672h.084v-.348l-1.573.024c-.409 0-.841-.012-1.526-.024v.348zm9.028 0c.829.024 1.237.312 1.237.732v5.33c0 .78-.252 1.056-1.177 1.068v.348l1.429-.024a43.1 43.1 0 0 1 1.453.024v-.348h-.048c-.877 0-1.165-.336-1.165-1.128v-4.982l5.909 6.686h.312v-6.578c0-.6.216-1.128.853-1.128h.264v-.348l-1.429.024c-.288 0-.757-.012-1.393-.024v.348c.853 0 1.213.192 1.213 1.14v4.189l-5.045-5.678-1.081.024c-.432 0-.877-.012-1.333-.024v.348h0zm13.441 7.13c-.685-.012-.853-.12-.853-.66v-5.678c0-.672.168-.792.877-.792v-.348l-1.501.024c-.564 0-1.177-.012-1.633-.024v.348c.673 0 .865.18.865.696v5.462c0 .78-.168.972-.901.972v.348c.36-.012.709-.024 1.622-.024a56.45 56.45 0 0 1 1.525.024v-.348h0zm1.293-7.13c.601.012.829.072.973.432l2.931 7.238h.372l2.739-6.878c.132-.336.216-.492.288-.588.096-.12.18-.204.444-.204h.072v-.348l-1.057.024c-.325 0-.565-.012-1.057-.024v.348c.517.012.853.012.853.3 0 .144-.06.336-.156.6L60.631 26l-1.886-4.681c-.144-.348-.18-.528-.18-.66 0-.312.324-.312.829-.324v-.348l-1.742.024c-.541 0-.973-.012-1.633-.024v.348h0zm8.959 0c.877 0 1.009.084 1.009.744v5.642c0 .54-.132.732-1.009.744v.348l2.774-.024 3.375.012.42-1.789h-.312c-.204.552-.336.78-.468.924a1.14 1.14 0 0 1-.589.312c-.252.06-.709.096-1.381.096-.541 0-.865-.048-1.069-.156-.216-.108-.325-.312-.325-.732V23.96h1.55c.492 0 .829.156.829.816v.192l.3-.06c-.012-.324-.036-.648-.036-1.428l.012-.996h-.3c0 .708-.192.948-.877.948h-1.478v-2.977l1.502-.036c.829 0 1.021.084 1.201.24.168.156.288.348.372 1.02l.312-.12c-.036-.456-.048-.9-.048-1.116 0-.156.012-.348.012-.456l-3.231.024c-.853 0-1.694-.012-2.546-.024v.348h0zm7.686 0h.06c.745 0 .937.108.937.792v5.522c0 .612-.144.816-.949.816h-.06v.348a76.17 76.17 0 0 1 1.778-.024l1.682.024v-.348h-.084c-.757 0-.961-.192-.961-.624V24.14h.757l.588 1.068a22.31 22.31 0 0 0 .697 1.092l.672 1.032c.264.408.409.48.613.48.252 0 .793-.024 1.189-.024.276 0 .516.012.817.024v-.348a1.26 1.26 0 0 1-.913-.336c-.228-.204-.516-.552-.841-1.044L77.18 23.9c1.057-.396 1.621-1.044 1.621-1.957 0-1.26-.961-1.957-2.895-1.957l-1.982.024c-.288 0-.757-.012-1.261-.024v.348h0zm2.402.084c.192-.06.421-.084.613-.084 1.057 0 1.681.6 1.681 1.753 0 .828-.36 1.224-.673 1.416-.24.144-.516.228-.961.228h-.661v-3.313zm9.104-.624c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.814 2.341.865.432 1.609.804 1.609 1.693 0 .66-.492 1.404-1.526 1.404-1.057 0-1.501-.684-1.826-1.765l-.3.048c.036.156.096.648.168 1.62.697.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.48-1.704-1.802-2.389-1.153-.6-1.694-.936-1.694-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.312-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zm6.391 7.671c-.685-.012-.853-.12-.853-.66v-5.678c0-.672.168-.792.877-.792v-.348l-1.502.024c-.564 0-1.177-.012-1.633-.024v.348c.673 0 .865.18.865.696v5.462c0 .78-.168.972-.901.972v.348c.36-.012.709-.024 1.621-.024a56.9 56.9 0 0 1 1.525.024v-.348zm1.517-5.966c.144-.792.409-1.044 1.333-1.044h1.633v6.002c0 .936-.192 1.008-.901 1.008h-.156v.348l1.79-.024 1.766.024v-.348c-.865-.012-1.105-.156-1.105-.864v-6.146h1.67c.432 0 .721.072.865.216.132.132.204.264.24.9l.276-.108c0-.408 0-.852.144-1.789l-.204-.096c-.18.3-.288.396-.673.396h-5.933c-.348 0-.529-.132-.613-.396h-.252a10.45 10.45 0 0 1-.216 1.921h.336zm8.746-1.164c.433 0 .745.048.913.276l2.558 3.469v2.653c0 .672-.216.72-1.045.732v.348l1.826-.024 1.694.024v-.348c-.853 0-1.057-.144-1.057-.624v-3.085l2.126-2.977c.252-.348.504-.444.828-.444v-.348l-1.045.024c-.252 0-.672-.012-1.213-.024v.348c.493.012.793.024.793.24 0 .12-.144.348-.24.492l-1.477 2.221-1.73-2.329c-.156-.216-.18-.324-.18-.384 0-.204.288-.228.624-.24v-.348l-1.693.024c-.565 0-.889-.012-1.682-.024v.348zm14.689.036c.12-.012.36-.036.636-.036 1.405 0 1.453 1.729 1.453 1.981 0 1.116-.492 1.717-1.465 1.717h-.06v.312h.18c1.393 0 1.874-.408 2.198-.744.264-.276.565-.768.565-1.488 0-1.236-.805-2.125-2.607-2.125l-1.717.024c-.385 0-.985-.012-1.442-.024v.348c.565 0 .853.036.853.708v5.63c0 .672-.24.792-.865.792v.348l1.586-.024 1.838.024v-.348c-.649 0-1.154 0-1.154-.756l.001-6.338zm4.466-.036h.06c.744 0 .937.108.937.792v5.522c0 .612-.145.816-.949.816h-.06v.348c.42-.012 1.225-.024 1.777-.024l1.682.024v-.348h-.084c-.757 0-.961-.192-.961-.624V24.14h.757l.588 1.068c.216.372.529.84.697 1.092l.672 1.032c.265.408.409.48.613.48.252 0 .793-.024 1.189-.024.276 0 .517.012.817.024v-.348a1.26 1.26 0 0 1-.913-.336c-.228-.204-.516-.552-.841-1.044l-1.465-2.185c1.057-.396 1.621-1.044 1.621-1.957 0-1.26-.96-1.957-2.894-1.957l-1.982.024c-.288 0-.757-.012-1.261-.024v.348h0zm2.402.084a2.15 2.15 0 0 1 .613-.084c1.057 0 1.681.6 1.681 1.753 0 .828-.36 1.224-.673 1.416-.24.144-.516.228-.96.228h-.661V20.42zm6.516-.084c.877 0 1.009.084 1.009.744v5.642c0 .54-.132.732-1.009.744v.348l2.775-.024 3.375.012.42-1.789h-.312c-.204.552-.336.78-.469.924-.144.144-.3.24-.588.312-.252.06-.709.096-1.381.096-.541 0-.865-.048-1.069-.156-.217-.108-.325-.312-.325-.732V23.96h1.55c.492 0 .828.156.828.816v.192l.301-.06c-.012-.324-.036-.648-.036-1.428l.012-.996h-.301c0 .708-.192.948-.876.948h-1.478v-2.977l1.502-.036c.828 0 1.021.084 1.201.24.168.156.288.348.372 1.02l.312-.12c-.036-.456-.048-.9-.048-1.116 0-.156.012-.348.012-.456l-3.231.024c-.852 0-1.693-.012-2.546-.024v.348zm10.51-.54c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.813 2.341.865.432 1.61.804 1.61 1.693 0 .66-.492 1.404-1.526 1.404-1.056 0-1.501-.684-1.825-1.765l-.3.048c.036.156.096.648.168 1.62.696.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.481-1.704-1.802-2.389-1.153-.6-1.693-.936-1.693-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.313-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zm6.233 0c-1.501 0-2.534.888-2.534 2.317 0 1.188.757 1.813 1.813 2.341.865.432 1.61.804 1.61 1.693 0 .66-.493 1.404-1.526 1.404-1.056 0-1.501-.684-1.825-1.765l-.3.048c.036.156.096.648.168 1.62.696.324 1.297.504 1.994.504 1.417 0 2.654-.876 2.654-2.317 0-.936-.48-1.704-1.802-2.389-1.153-.6-1.693-.936-1.693-1.68 0-.6.444-1.369 1.465-1.369.757 0 1.177.42 1.369 1.332l.313-.036c-.06-.504-.12-1.092-.12-1.549l-.757-.084c-.432-.048-.697-.072-.829-.072zM0 0v15.599c0 7.67 4.03 9.859 5.967 10.911.356.193 6.333 3.162 6.715 3.352l.258.138.257-.137 6.716-3.352C21.85 25.459 25.88 23.27 25.88 15.6V0H0z" data-v-7c4b1471></path></g><path d="M12.939 29.365l2.633-1.309V15.762h9.746l.001-.163V10.5h-9.747V.559h-5.266V10.5H.559v5.099l.001.163h9.746v12.294l2.633 1.309z" fill="#fff" data-v-7c4b1471></path><g fill="#e73337" data-v-7c4b1471><path d="M.559.559h9.747V10.5H.559V.559zm15.015 0h9.747V10.5h-9.747V.559zm-5.269 15.205H.559c.056 7.126 3.712 9.191 5.674 10.256.199.108 2.281 1.146 4.073 2.038V15.764zm5.269 0v12.294l4.073-2.038c1.961-1.065 5.618-3.13 5.673-10.256h-9.746z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M19.073 13.234l.201-.239-.254-.302-.255.302.202.239c.003.12.01 1.175-.611 1.552 0 0 .207.04.486-.014a3.1 3.1 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.59-1.791l.201-.239-.254-.302-.255.302.202.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.09 3.09 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.053.486.014.486.014-.622-.377-.614-1.431-.611-1.551zm1.586 1.791l.201-.239-.254-.302-.254.302.201.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.485-.014a3.11 3.11 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.053.485.014.485.014-.622-.377-.614-1.431-.611-1.551zm-6.348 0l.201-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.551zM3.733 13.234l.201-.239-.254-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.59-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.612 1.551 0 0 .207.04.486-.014 0 0 .048.275.178.588.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586 1.791l.201-.239-.254-.302-.255.302.202.239c.003.12.01 1.175-.611 1.552 0 0 .207.04.485-.014a3.07 3.07 0 0 0 .179.588c.13-.313.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.586-1.791l.201-.239-.254-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.485-.014 0 0 .049.274.179.588.13-.314.179-.588.179-.588.278.053.486.014.486.014-.622-.377-.614-1.431-.611-1.551zm-6.348 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.551 0 0 .207.04.486-.014a3.12 3.12 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.551zm10.844 5.803l.201-.239-.254-.302-.254.302.202.239c.003.121.01 1.175-.612 1.551 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.551zm0 5.387l.201-.239-.254-.302-.254.302.202.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.48-2.694l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .178.589c.131-.314.179-.589.179-.589.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.09 3.09 0 0 0 .179.589c.131-.314.179-.589.179-.589.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm2.957 5.387l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.11 3.11 0 0 0 .178.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.121.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm1.477-20.59l.201-.239-.254-.302-.254.302.202.239c.003.12.01 1.175-.612 1.552 0 0 .207.04.486-.014a3.12 3.12 0 0 0 .179.588c.13-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.615-1.431-.611-1.552zm1.48-2.693l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014 0 0 .048.275.178.588.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.13 3.13 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm2.957 5.387l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.1 3.1 0 0 0 .178.588c.131-.314.179-.588.179-.588.278.054.485.014.485.014-.622-.377-.614-1.431-.611-1.552zm-2.957 0l.202-.239-.255-.302-.254.302.201.239c.003.12.011 1.175-.611 1.552 0 0 .207.04.486-.014a3.07 3.07 0 0 0 .179.588c.131-.314.179-.588.179-.588.278.054.486.014.486.014-.622-.377-.614-1.431-.611-1.552zm11.221 13.502c.144.19.346.314.512.314a1.06 1.06 0 0 0 .223-.034.43.43 0 0 1 .11-.016c.04 0 .057.015.057.052a.56.56 0 0 1-.043.155c-.026.062-.043.112-.049.148l-.033.173-.083.148-.115.107-.033.034v.074.037c0 .202-.06.316-.223.425-.013.008-.033.022-.058.041l-.017.067-.082.288a1.73 1.73 0 0 1-.157.231c-.1.131-.165.237-.206.338-.034.083-.06.107-.114.107-.081 0-.162-.102-.176-.222a.42.42 0 0 1-.041.05l-.098.066-.158.107-.131.099-.091.091c-.016.016-.033.025-.047.025-.042 0-.085-.071-.126-.207l-.082-.271c-.022-.073-.034-.163-.034-.267 0-.137.009-.185.058-.295-.165.017-.212.027-.297.058a.29.29 0 0 1-.072.016c-.027 0-.044-.019-.044-.051a.51.51 0 0 1 .033-.122l.108-.28a.7.7 0 0 1 .566-.416.72.72 0 0 1 .211.044.45.45 0 0 0 .125.026c.096 0 .172-.07.172-.158 0-.044-.029-.091-.066-.108-.014-.007-.113-.037-.297-.091-.139-.041-.208-.092-.364-.273-.113-.131-.158-.168-.199-.168-.032 0-.049.021-.049.058l.008.11.003.051c0 .229-.068.448-.234.75l-.232.42c-.11.2-.157.352-.157.504 0 .166.072.313.165.339l.124.032a.09.09 0 0 1 .041.026c.011.013.026.034.026.047s-.015.046-.042.076c-.015.016-.064.08-.149.189-.142.186-.232.253-.405.305-.1.242-.263.383-.554.479l-.033.057c-.098.174-.317.331-.456.331h-.032c-.018.151-.126.308-.289.421l-.19.139a.34.34 0 0 0-.041.075c-.037.082-.072.116-.118.116-.088 0-.161-.082-.212-.24l-.075.049-.157.107c-.038.018-.135.047-.289.083-.203.047-.378.133-.422.207l-.074.123-.034.014c-.04 0-.087-.057-.107-.13a2.03 2.03 0 0 1-.075-.464c0-.293.097-.484.397-.783l-.001-.071c0-.456.348-.765.86-.765l.083.002.281.017.026.001c.152 0 .292-.058.444-.183.202-.165.261-.266.261-.451 0-.059-.016-.126-.079-.333a1.34 1.34 0 0 1-.061-.388c0-.341.25-.765.59-.999.087-.061.152-.141.152-.187 0-.019-.01-.034-.023-.034-.022 0-.059.023-.105.064-.04.036-.107.078-.198.125l-.132.066-.149.058c-.046.017-.069.042-.069.073 0 .018.007.044.019.075s.016.047.016.061-.014.052-.042.096l-.115.19c-.113.186-.195.266-.339.33-.093.272-.226.41-.421.438-.156.314-.294.423-.57.454-.038.121-.105.221-.248.38-.059.064-.095.109-.107.132-.065.116-.07.123-.1.123-.038 0-.124-.057-.189-.123a.5.5 0 0 1-.074-.1l-.075-.132c-.024-.043-.049-.062-.08-.062l-.053.003-.182.025a.58.58 0 0 0-.331.19l-.099.149c-.01.015-.025.024-.039.024-.074 0-.119-.163-.119-.433 0-.237.036-.395.118-.508l.231-.322c.032-.045.05-.089.05-.126a.17.17 0 0 0-.041-.106c-.049-.058-.096-.093-.132-.099l-.174-.024c-.031-.005-.044-.012-.044-.026s.009-.034.027-.057a.79.79 0 0 1 .207-.182c.205-.125.339-.174.477-.174a.81.81 0 0 1 .432.142l.239.157c.034.023.074.034.115.034.111 0 .194-.066.272-.215.076-.144.109-.26.109-.383a.51.51 0 0 0-.53-.534c-.143 0-.29.056-.43.165-.042.033-.07.064-.083.091l-.066.149c-.015.036-.039.057-.059.057-.042 0-.135-.119-.213-.272-.041-.08-.08-.179-.115-.298-.049-.161-.071-.282-.071-.412l.012-.224a.83.83 0 0 1-.149-.511.92.92 0 0 1 .059-.29.61.61 0 0 1-.091-.368l.008-.103.025-.115.017-.099c-.08-.104-.102-.174-.182-.536-.044-.205-.071-.294-.106-.347-.017-.023-.025-.041-.025-.05 0-.048.115-.092.247-.092l.116.009c-.029-.076-.035-.106-.035-.166a1.03 1.03 0 0 1 .06-.329c.044-.126.053-.161.053-.202 0-.098-.015-.197-.036-.252l-.066-.165a.23.23 0 0 1-.021-.085c0-.017.011-.023.041-.023.162 0 .472.175.715.405s.32.413.32.752l-.014.181c.225.197.323.36.323.537 0 .129-.042.215-.166.337-.223.222-.273.337-.273.622 0 .438.131.675.373.675.213 0 .377-.231.404-.57l.025-.314a3.22 3.22 0 0 1 .041-.297c.014-.075.025-.174.025-.215 0-.111-.073-.252-.29-.504-.232-.271-.31-.417-.31-.629 0-.254.176-.44.414-.44.059 0 .089.009.16.045.003-.144.01-.17.091-.372.064-.159.133-.272.165-.272.011 0 .028.012.049.033a.86.86 0 0 0 .108.09l.083.058a.75.75 0 0 1 .074.075c.051-.185.104-.272.256-.421.116-.113.159-.148.181-.148s.038.015.059.041l.099.131.14.158c.054.059.073.111.099.255l.082-.049.132-.074a.97.97 0 0 0 .1-.066c.022-.016.041-.026.052-.026s.029.024.039.059l.049.19.108.215c.032.064.042.114.042.212v.052l.055-.002c.163 0 .303.192.303.414 0 .294-.235.674-.54.876-.028.099-.035.139-.035.219 0 .657.173.794 1.183.945a5.56 5.56 0 0 1 .677.14l.653.215c.223.074.373.107.489.107.123 0 .191-.035.191-.1 0-.113-.165-.214-.672-.412l-.719-.28c-.731-.286-1.076-.731-1.076-1.388 0-.764.574-1.338 1.34-1.338.289 0 .57.06.976.207l.605.198c.101 0 .156-.069.171-.214.013-.124.047-.174.115-.174.101 0 .212.094.29.248.068.134.107.291.107.428a.47.47 0 0 1-.405.504.8.8 0 0 1-.665.342c-.329 0-.492-.065-1.054-.416-.201-.126-.287-.157-.434-.157-.197 0-.343.122-.343.287 0 .151.122.251.397.324.164.043.188.055.273.14.066-.016.096-.02.137-.02.268 0 .599.145.879.384.509.157.962.706.962 1.167 0 .509-.441.965-.935.965a3.37 3.37 0 0 1-.639-.102z" data-v-7c4b1471></path></g><g class="C" data-v-7c4b1471><path d="M22.437 23.219l.033-.091.033-.116c.006-.021.037-.068.091-.139.084-.11.126-.249.126-.411 0-.051-.013-.098-.044-.16-.016-.032-.025-.058-.025-.076 0-.011.006-.024.017-.038s.022-.023.033-.025l.005-.001c.021 0 .057.024.085.058.035.043.066.066.084.066.037 0 .076-.063.098-.157l.017-.083c0-.04-.033-.144-.074-.231-.016-.033-.024-.061-.024-.084 0-.029.019-.053.046-.053s.063.026.11.07c.023.022.045.034.063.034.052 0 .089-.053.128-.182l.033-.129c0-.024-.01-.038-.029-.038l-.029.002-.098.01c-.151 0-.364-.104-.464-.224l-.231-.281-.206-.273-.19-.124c-.031-.02-.049-.045-.049-.067 0-.031.031-.052.079-.052s.179.048.425.152a2.58 2.58 0 0 0 .837.183c.395 0 .721-.319.721-.707 0-.212-.095-.408-.327-.681-.043-.051-.118-.101-.153-.101-.027 0-.046.02-.046.049a.26.26 0 0 0 .025.093c.01.026.016.052.016.076 0 .038-.014.06-.039.06-.052 0-.106-.085-.151-.234-.055-.184-.162-.294-.414-.421a.89.89 0 0 0-.261-.093c-.016 0-.027.014-.027.036 0 .031.012.065.033.098a.41.41 0 0 1 .067.158v.008c0 .032-.016.05-.046.05-.053 0-.065-.014-.186-.199-.062-.095-.228-.22-.372-.281-.045-.019-.093-.031-.14-.033h-.005c-.03 0-.053.014-.053.032 0 .007.013.033.037.076l.012.047c0 .025-.017.045-.039.045-.05 0-.203-.126-.324-.265-.131-.151-.215-.343-.215-.494 0-.316.302-.596.64-.596.085 0 .157.015.212.042l.289.149.339.224c.195.127.462.223.624.223.06 0 .106-.008.26-.051.081-.022.131-.054.131-.084l-.003-.007-.029-.017-.182-.016c-.101-.01-.33-.113-.512-.232l-.38-.247-.338-.141c-.026-.01-.042-.027-.042-.043s.018-.031.039-.031c0 0 .032.006.094.017l.314.091c.079.022.206.081.38.173.344.183.569.264.734.264.191 0 .331-.131.331-.309 0-.06-.013-.157-.032-.236a.33.33 0 0 0-.05-.107c-.014-.021-.03-.032-.041-.032-.024 0-.037.025-.037.067l.004.057.003.048c0 .152-.059.206-.228.206-.137 0-.322-.055-.742-.222-.394-.155-.64-.214-.884-.214-.622 0-1.167.507-1.167 1.085 0 .499.332.882 1.011 1.161l.702.289c.581.239.794.41.794.636 0 .188-.175.309-.448.309-.118 0-.265-.032-.536-.119-.657-.209-1.085-.296-1.461-.296-.613 0-1.049.168-1.398.536-.046.049-.073.063-.114.063-.028 0-.056-.011-.118-.047a.4.4 0 0 0-.181-.059c-.06 0-.154.052-.198.109l-.082.107c-.024.031-.051.05-.073.05-.06 0-.166-.102-.232-.224-.041-.074-.069-.147-.082-.215-.011-.052-.017-.137-.017-.226l.008-.153.025-.165.025-.141c.006-.02.022-.033.04-.033.043 0 .062.039.062.129l-.002.052-.002.037c0 .235.093.45.196.45.081 0 .152-.144.211-.43.012-.057.035-.09.063-.09s.053.042.078.165c.015.08.068.191.124.264.032.042.066.067.094.067.068 0 .111-.101.145-.339.014-.092.041-.15.073-.15.014 0 .026.007.035.018a1.3 1.3 0 0 1 .058.091c.02.034.052.062.091.082s.074.033.096.033a.12.12 0 0 0 .077-.033c.037-.033.058-.058.058-.068s-.017-.038-.049-.073c-.054-.058-.096-.136-.149-.273L20 19.462c-.013-.03-.027-.05-.042-.058l-.026-.007-.049.007-.173.049-.054.009c-.023 0-.039-.015-.039-.039s.03-.062.068-.092c.209-.166.333-.445.38-.851.028-.247.06-.34.165-.487.022-.031.041-.049.052-.049.032 0 .05.025.05.073 0 .032-.009.072-.027.117a.29.29 0 0 0-.018.1l.001.031.017.05.091-.033.099-.115a.61.61 0 0 0 .109-.338c0-.079-.019-.134-.066-.19-.052-.062-.088-.087-.126-.087l-.056.004-.059.004c-.053 0-.08-.016-.08-.049l.014-.063.009-.091-.049-.206-.041-.1-.025-.083-.058.041-.124.074c-.032.019-.066.05-.1.091-.117.142-.125.149-.172.149-.03 0-.047-.011-.047-.031l.005-.035a.85.85 0 0 0 .025-.171c0-.144-.105-.361-.231-.481-.173.165-.265.346-.265.521v.099c0 .038-.012.057-.038.057h-.003c-.021-.003-.041-.019-.058-.041l-.099-.132a1.33 1.33 0 0 0-.133-.108l-.066-.057-.033.066c-.082.164-.11.262-.115.397-.004.11-.013.142-.039.142-.013 0-.03-.006-.052-.019-.125-.07-.137-.075-.191-.075-.149 0-.25.106-.25.263 0 .061.035.154.103.275.076.135.189.258.239.258.014 0 .017-.01.017-.051v-.017l-.008-.14-.017-.108-.001-.014c0-.032.012-.046.037-.046.075 0 .14.219.195.655.048.384.154.648.322.801.043.039.067.071.067.089s-.015.026-.033.026c-.024 0-.062-.015-.108-.041a.18.18 0 0 0-.09-.026c-.083 0-.175.096-.191.2l-.075.47-.058.661c-.011.125.006.293.05.495.038.178.058.309.058.391 0 .495-.308.886-.818 1.037l-.529.156c-.329.098-.491.315-.537.718.085-.067.111-.078.24-.098l.156-.025a.43.43 0 0 0 .141-.041c.015-.009.045-.04.091-.091l.14-.156c.053-.06.101-.094.13-.094.02 0 .035.022.035.048s-.02.078-.058.136-.06.135-.06.213c0 .105.028.172.135.324.013-.05.024-.083.033-.099a1.69 1.69 0 0 1 .124-.173c.114-.148.197-.302.198-.372.002-.128.014-.169.046-.169s.033.006.045.079c.003.018.012.035.025.05.019.021.038.033.05.033s.033-.012.066-.033c.057-.038.101-.077.132-.116s.062-.093.082-.149c.028-.072.042-.125.042-.156l-.001-.107c0-.028.012-.051.027-.051s.062.043.089.091c.018.032.03.042.053.042h.005c.03-.003.056-.015.074-.034.101-.098.166-.219.166-.31l-.009-.152-.033-.125-.002-.011c0-.026.019-.048.04-.048.037 0 .078.053.135.174.026.057.051.084.075.084.039 0 .07-.029.148-.142l.074-.107c.016-.024.025-.052.025-.083s-.009-.057-.025-.081l-.067-.099c-.011-.017-.016-.045-.016-.083a.81.81 0 0 1 .124-.388l.107-.207c.013-.034.027-.05.046-.05s.028.02.028.041l-.008.091v.066.066c.011.03.031.042.071.042.025 0 .054-.006.086-.017l.206-.075a1.42 1.42 0 0 0 .19-.091c.324-.179.424-.216.583-.216.095 0 .218.037.392.117.036.017.065.026.085.026s.05-.015.072-.042c.016-.019.024-.042.024-.063 0-.03-.011-.066-.033-.102-.026-.046-.051-.103-.051-.123s.014-.036.038-.036.047.02.062.044l.19.33c.154.274.354.464.529.504l.363.083c.113.025.141.069.141.209a.72.72 0 0 1-.331.616l-.298.191c-.108.068-.198.259-.198.416 0 .102.025.282.058.425l.058-.057.141-.083.124-.124.066-.182.066-.165c.032-.08.07-.132.096-.132s.045.045.045.126l-.001.023v.031c0 .118.015.17.107.348z" data-v-7c4b1471></path><path d="M17.543 25.575c0-.393.143-.564.712-.853 1.123-.569 1.728-1.091 1.728-1.488 0-.08-.026-.197-.1-.445a1.41 1.41 0 0 1-.071-.358c0-.183.151-.46.344-.632l.157-.14c.086-.077.15-.188.15-.261l-.001-.037-.002-.049c0-.082.032-.112.117-.116l.075-.008.064-.017c.027 0 .053.015.069.041l.041.066.058.066c.019.021.038.034.053.034s.033-.008.063-.024c.04-.022.071-.031.1-.031.043 0 .065.023.065.071 0 .166-.107.406-.298.661-.317.429-.421.66-.421.934 0 .215.062.404.182.56.021.028.033.05.033.061 0 .022-.021.059-.058.104l-.05.067c-.066.095-.11.132-.153.132-.032 0-.063-.027-.078-.067l-.033-.09c-.01-.026-.028-.043-.047-.043-.006 0-.02.014-.036.034-.021.027-.032.059-.032.091v.14c0 .056-.02.114-.058.165a.69.69 0 0 1-.116.123c-.031.027-.054.042-.065.042-.024 0-.047-.026-.059-.066l-.05-.165c-.007-.024-.024-.041-.041-.041-.028 0-.051.046-.058.116-.017.187-.049.301-.098.347l-.116.106c-.037.036-.081.061-.105.061s-.045-.011-.052-.027l-.058-.124c-.007-.015-.021-.025-.037-.025-.033 0-.058.066-.07.191-.03.297-.092.43-.256.545a.45.45 0 0 0-.206.231c-.072-.106-.1-.196-.1-.329a.3.3 0 0 1 .034-.157c.034-.062.053-.109.053-.137 0-.02-.013-.036-.03-.036-.008 0-.022.012-.041.032-.023.027-.059.055-.107.083-.149.085-.171.116-.207.281a.42.42 0 0 1-.38.371c-.28.057-.297.064-.397.158l-.01-.147zm-.534-8.514v.083l.001.122a.51.51 0 0 1-.018.134c-.027.115-.033.154-.033.187 0 .158.022.236.115.399.052.09.085.199.085.277 0 .033-.014.053-.036.053-.014 0-.029-.008-.04-.024l-.066-.091c-.053-.072-.103-.091-.237-.091l-.143.009a1.79 1.79 0 0 1 .099.289c.067.27.076.289.223.421s.161.15.161.198c0 .037-.026.067-.057.067l-.055-.008-.033-.003c-.045 0-.086.083-.086.175a.59.59 0 0 0 .053.208c.02.049.04.082.058.1l.083.082a.12.12 0 0 1 .033.085c0 .04-.023.074-.05.074a.19.19 0 0 1-.074-.027c-.021-.01-.04-.016-.058-.016-.037 0-.055.023-.055.068l.013.123.05.214c.011.048.042.102.091.158a.39.39 0 0 0 .124.099l.166.075c.016.012.024.036.024.066s-.024.058-.055.058a.7.7 0 0 1-.086-.017l-.084-.008c-.084 0-.09.005-.09.069a.52.52 0 0 0 .017.128l.033.132c.035.14.08.215.13.215.011 0 .032-.012.06-.033a1.02 1.02 0 0 1 .511-.173c.155 0 .332.019.399.041.038.013.065.02.079.02.02 0 .027-.02.028-.094l.017-.149.033-.256.024-.19v-.108l-.067.058c-.14.132-.229.173-.374.173-.38 0-.598-.427-.598-1.171 0-.307.045-.742.13-1.24.018-.105.026-.178.026-.229 0-.272-.172-.537-.472-.729z" data-v-7c4b1471></path><path d="M17.57 18.91l.074-.074.074-.067c.062-.054.111-.169.111-.254 0-.039-.008-.075-.028-.118-.029-.062-.084-.117-.117-.117-.028 0-.053.068-.064.175l-.017.124-.024.124-.009.124v.082zm-.726 3.377c.085.031.102.045.19.149.115.133.178.165.33.165a.42.42 0 0 0 .117-.016l.107-.033.083-.025c.04-.011.066-.028.066-.042s-.065-.072-.165-.139a1.59 1.59 0 0 0-.198-.116c-.071-.035-.131-.051-.189-.051-.116 0-.228.036-.339.109zm1.113 2.323c0-.266.262-.485.582-.485.16 0 .253.032.253.086 0 .042-.07.1-.223.181l-.24.132-.364.181-.008-.096zm3.363-2.003c.117-.024.157-.054.215-.057l.149.008a.48.48 0 0 0 .099-.066l.116-.082c.165-.113.174-.12.174-.156 0-.046-.067-.068-.206-.068-.26 0-.419.122-.546.422z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M18.827 17.58l.116.074c.066.042.121.073.165.09.128.053.174.071.174.12a.35.35 0 0 1-.017.094.51.51 0 0 0-.025.116c-.012.112-.08.141-.125.141-.056 0-.114-.086-.206-.248a1.91 1.91 0 0 1-.132-.33l-.005-.032c0-.02.012-.032.036-.032l.018.006z" data-v-7c4b1471></path><path d="M19.942 17.868l-.19.191c-.067.089-.067.167-.083.182s-.032.021-.058.025l-.012.001c-.048 0-.084-.035-.087-.083l-.008-.14-.009-.141v-.008c0-.054.03-.083.073-.092l.176-.049.181-.091c.053-.026.069-.033.078-.033.021 0 .039.022.039.048 0 .012-.006.03-.018.052l-.083.139z" data-v-7c4b1471></path><path d="M19.105 18.431l.124.092a.22.22 0 0 0 .111.028.51.51 0 0 0 .195-.053l.108-.049c.025-.011.047-.018.064-.018.066 0 .118.07.118.157 0 .122-.088.247-.216.429l-.09.191c-.072.103-.12.14-.179.14-.098 0-.146-.056-.293-.348-.106-.208-.149-.336-.149-.438 0-.087.045-.148.111-.148.027 0 .06-.006.096.016z" data-v-7c4b1471></path></g><path d="M19.216 18.884l.008.157c.004.071.063.142.12.142.076 0 .131-.115.16-.29l.019-.115c0-.061-.02-.077-.098-.077l-.044.002h-.058l-.037-.002c-.052 0-.072.04-.072.145l.002.038z" class="C" data-v-7c4b1471></path><path d="M8.359 21.057c.072.182.097.272.107.389l.016.346c.013.347.128.52.347.52.079 0 .145-.021.182-.058l.091-.091.036-.011c.058 0 .09.038.09.107l-.01.134c-.007.06.004.188.033.379a2.25 2.25 0 0 1 .025.333c0 .176-.013.244-.075.402.058.162.091.345.091.499 0 .18-.029.3-.099.402.033.142.041.199.041.28s-.011.187-.033.314c-.029.172-.041.305-.033.395l.001.021c0 .052-.016.078-.048.078s-.069-.024-.102-.066c-.05-.065-.083-.088-.121-.088-.057 0-.075.03-.16.278-.026.077-.093.18-.198.306-.082.098-.133.179-.149.24l-.049.173c-.01.022-.029.039-.05.041h-.01c-.136 0-.292-.292-.452-.842-.045-.156-.103-.225-.187-.225-.06 0-.121.04-.144.093l-.05.116c-.009.02-.026.033-.043.033-.066 0-.167-.121-.229-.273-.051-.126-.081-.263-.081-.38 0-.259.191-.452.766-.776.251-.141.348-.32.348-.643 0-.256-.077-.403-.297-.57-.175-.132-.275-.182-.368-.182-.078 0-.14.052-.14.118 0 .078.045.125.153.162.127.046.182.09.182.147s-.054.165-.141.299a.76.76 0 0 1-.264.273c-.039.227-.146.372-.397.537-.05.142-.078.193-.165.305l-.174.223a.72.72 0 0 0-.124.256c-.018.126-.03.148-.078.148-.066 0-.2-.116-.335-.289a.4.4 0 0 1-.107-.273c-.086.068-.113.077-.281.099-.101.014-.176.033-.223.059-.127.066-.168.084-.191.084-.034 0-.046-.018-.049-.076l-.008-.182c0-.249.098-.54.248-.734-.145-.125-.289-.202-.405-.215-.049-.005-.062-.011-.062-.029 0-.03.032-.095.078-.152.062-.078.111-.133.149-.165a.82.82 0 0 1 .479-.174c.107 0 .266.039.471.116.074.028.14.041.197.041.091 0 .142-.059.142-.163 0-.15-.074-.385-.241-.762-.112-.255-.163-.427-.163-.545 0-.286.135-.583.304-.668-.08-.04-.115-.05-.184-.05-.083 0-.148.021-.254.083-.122.071-.19.134-.19.176 0 .016.025.042.067.072.064.045.075.061.075.122 0 .204-.169.477-.405.654-.052.289-.185.464-.446.586-.034.157-.056.203-.182.38-.216.302-.266.399-.305.57-.02.089-.041.125-.068.125-.035 0-.108-.055-.188-.142l-.132-.158c-.035-.054-.053-.107-.091-.263-.06.017-.076.02-.118.02l-.121-.003-.083-.002c-.181 0-.373.041-.471.101-.036.022-.06.033-.072.033-.053 0-.097-.112-.097-.25 0-.298.083-.545.276-.814-.071-.059-.094-.081-.157-.148-.027-.03-.05-.05-.066-.058l-.091-.05c-.031-.017-.05-.037-.05-.054 0-.044.121-.159.264-.252.159-.102.285-.149.4-.149.135 0 .207.035.376.182.117.102.197.149.254.149.04 0 .084-.028.118-.075.028-.038.043-.08.043-.123 0-.158-.189-.424-.398-.562-.156-.102-.326-.157-.492-.157-.23 0-.428.114-.515.297l-.091.19c-.021.044-.048.067-.079.067-.053 0-.081-.024-.268-.241l-.214-.248a.66.66 0 0 1-.133-.214.49.49 0 0 0-.041-.107c-.007-.011-.036-.033-.082-.067-.166-.117-.287-.349-.356-.685l-.025-.107c-.202-.248-.323-.555-.323-.821a.36.36 0 0 1 .018-.113c-.118-.118-.131-.147-.224-.463-.066-.224-.108-.32-.19-.428-.022-.029-.033-.052-.033-.064 0-.045.069-.081.19-.101l.166-.019.165.011-.01-.226c0-.133.01-.219.044-.377a1.01 1.01 0 0 0 .025-.172 1.68 1.68 0 0 0-.017-.142.73.73 0 0 0-.024-.124c-.012-.031-.018-.055-.018-.07 0-.027.018-.041.049-.041.024 0 .058.012.109.036.144.071.263.143.355.215.285.223.509.566.509.779l-.004.065-.017.121c0 .036.02.084.058.134l.091.125c.081.11.133.238.133.324 0 .077-.016.111-.149.304-.1.144-.141.252-.141.369 0 .149.059.337.166.522.097.171.21.265.317.265.184 0 .265-.163.294-.604l.066-.627.008-.113c0-.052-.016-.082-.082-.161l-.249-.288c-.236-.276-.307-.418-.307-.616 0-.227.149-.384.365-.384l.099.009-.001-.064a.87.87 0 0 1 .042-.224c.052-.177.117-.314.151-.314.011 0 .038.013.08.041a1.22 1.22 0 0 0 .149.057.99.99 0 0 1 .164.082.73.73 0 0 1 .133-.289c.107-.138.228-.247.271-.247.026 0 .074.035.134.099l.157.165c.026.027.057.081.091.157l.033.074.099-.057.132-.075.157-.075c.042-.028.072-.043.085-.043.028 0 .043.05.105.323a1.9 1.9 0 0 1 .058.364c.264.037.388.188.388.471 0 .266-.131.531-.413.832-.077.083-.096.122-.096.199 0 .73.254.96 1.17 1.057a5.83 5.83 0 0 1 1.404.298c.355.106.559.15.691.15.156 0 .243-.057.243-.157 0-.153-.111-.201-.769-.34-1.531-.322-2.132-.845-2.132-1.854 0-.719.538-1.259 1.257-1.259.317 0 .6.097 1.048.355.416.241.502.282.597.282.111 0 .155-.071.155-.25 0-.116.035-.172.107-.172.066 0 .166.068.256.174.137.162.19.245.19.469 0 .299-.159.494-.471.58l-.074.082c-.17.192-.32.256-.59.256-.235 0-.53-.096-.732-.24l-.397-.28c-.171-.121-.296-.173-.416-.173-.169 0-.27.101-.27.27s.11.301.29.357l.372.116c.132.041.194.067.355.148l.185-.018a1.57 1.57 0 0 1 .699.174l.149.067.207.058c.469.131.851.656.851 1.168 0 .584-.379.946-.991.946l-.191-.009z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M8.218 25.961l.049-.1.116-.124a.61.61 0 0 0 .15-.427c0-.085-.015-.177-.043-.267a.67.67 0 0 1-.034-.146c0-.052.039-.102.079-.102.016 0 .039.015.062.042l.124.14c.077.087.103.103.199.124l.008-.115.008-.136a.96.96 0 0 0-.182-.558c-.066-.084-.096-.135-.096-.164s.022-.059.045-.059c.008 0 .02.006.034.017.08.059.121.084.143.084.04 0 .076-.1.076-.208a.77.77 0 0 0-.169-.461l-.099-.123c-.011-.016-.017-.032-.017-.047 0-.035.029-.061.072-.061.031 0 .05.008.102.041.024.016.047.025.063.025.064 0 .088-.09.088-.336l-.003-.102-.008-.149c-.009-.175-.025-.206-.099-.206H8.82c-.403 0-.591-.255-.695-.942-.075-.496-.26-.849-.487-.925-.136-.045-.184-.075-.184-.111 0-.023.038-.045.079-.045.03 0 .099.018.156.041.329.133.607.198.849.198.396 0 .708-.291.708-.66 0-.282-.134-.569-.376-.801a.47.47 0 0 0-.206-.124l-.016-.001-.042.014.017.119.032.083.009.034c0 .018-.017.032-.039.032s-.047-.013-.061-.034l-.149-.223c-.118-.176-.376-.323-.568-.323-.068 0-.111.03-.111.078 0 .023.022.066.05.097.042.048.075.105.075.132l-.009.033c-.009.019-.021.031-.033.033l-.008.001c-.04 0-.06-.018-.24-.223-.147-.168-.448-.332-.609-.332-.023 0-.044.01-.061.026-.01.011-.017.023-.017.032 0 .025.022.058.058.092.051.047.092.103.092.127 0 .016-.022.031-.046.031-.047 0-.153-.068-.351-.224-.308-.242-.446-.47-.446-.735 0-.319.228-.57.52-.57.171 0 .398.082.554.199l.405.305a1.07 1.07 0 0 0 .65.215c.052 0 .1-.012.143-.033.036-.018.058-.04.058-.055s-.031-.024-.075-.036c-.146-.035-.264-.085-.347-.148l-.413-.313-.323-.223c-.218-.114-.26-.141-.26-.167 0-.013.021-.024.047-.024s.047.006.073.017l.215.091a2.47 2.47 0 0 1 .347.206c.442.295.667.39.927.39.247 0 .429-.175.429-.413 0-.047-.012-.098-.034-.143l-.041-.082c-.014-.029-.041-.052-.049-.041l-.025.033-.025.174c-.001.007-.01.027-.025.058a.29.29 0 0 1-.057.082c-.045.044-.083.058-.152.058-.128 0-.271-.068-.658-.315s-.744-.379-1.024-.379A1.07 1.07 0 0 0 5.977 18c0 .827.505 1.2 2.09 1.546.603.131.868.308.868.576 0 .245-.198.399-.512.399a1.19 1.19 0 0 1-.224-.016 30.59 30.59 0 0 1-.851-.232c-.354-.098-.752-.154-1.103-.154-.622 0-1.041.185-1.326.584-.033.047-.066.075-.088.075s-.042-.012-.069-.033l-.115-.091a.56.56 0 0 0-.297-.113.26.26 0 0 0-.175.072c-.041.037-.072.057-.084.057-.046 0-.131-.104-.196-.239-.093-.193-.141-.386-.141-.562 0-.082.026-.14.062-.14.029 0 .05.022.062.066l.066.239c.037.134.112.239.169.239.025 0 .07-.036.12-.099.031-.038.053-.077.067-.116.052-.151.064-.173.105-.173.048 0 .073.03.106.181.007.028.016.07.054.142.033.062.067.113.099.149.025.027.056.042.085.042.073 0 .139-.121.139-.255l-.002-.035-.01-.153c0-.059.009-.077.043-.077.019 0 .034.01.058.04.035.046.083.085.141.115.05.027.091.041.118.041s.053-.015.08-.041.035-.044.035-.065-.03-.062-.117-.15c-.047-.048-.103-.16-.165-.33-.023-.065-.054-.109-.078-.109a.24.24 0 0 0-.054.018l-.107.041-.074.033-.028.006c-.035 0-.063-.025-.063-.057s.075-.121.166-.196c.065-.055.105-.136.107-.223.005-.139.031-.248.058-.248l.025-.289a2.98 2.98 0 0 1 .124-.569c.01-.03.023-.051.033-.051.023 0 .044.062.044.131l-.003.043-.005.084c0 .105.017.159.049.159s.077-.052.113-.143l.074-.19c.022-.056.033-.108.033-.154 0-.161-.096-.277-.23-.277l-.06.01-.058.017c-.043 0-.08-.048-.08-.104l.006-.119-.008-.165-.033-.173-.009-.083-.074.041-.091.075-.132.107-.108.091c-.019.016-.04.025-.061.025-.063 0-.071-.017-.071-.146v-.032c0-.104-.045-.185-.214-.384l-.042-.05-.032.058-.091.149-.091.231-.017.148c-.004.043-.021.076-.037.076s-.046-.022-.078-.059a.64.64 0 0 0-.355-.239l-.016.057-.05.174-.009.156v.066.053c0 .046-.017.073-.045.073l-.005-.001c-.003-.001-.019-.015-.049-.041-.019-.017-.054-.026-.1-.026-.154 0-.252.083-.252.215 0 .099.034.173.186.404.068.103.146.184.176.184.02 0 .037-.054.037-.117l-.015-.099a.41.41 0 0 1-.024-.126c0-.046.013-.071.039-.071.074 0 .151.214.183.512l.033.305a1.02 1.02 0 0 0 .099.33c.075.154.149.259.215.305.079.056.128.111.128.141s-.021.05-.044.05a5.64 5.64 0 0 1-.167-.059l-.069-.01c-.165 0-.22.094-.22.378a1.58 1.58 0 0 0 .488 1.044l.38.306c.193.154.339.411.339.595 0 .242-.265.479-.719.644l-.273.098c-.212.077-.297.214-.38.611a.68.68 0 0 1 .228-.042l.037.001.123.003c.111 0 .191-.067.191-.16v-.075c0-.022.031-.053.074-.075l.115-.058.091-.074c.014-.012.03-.019.046-.019.025 0 .045.024.045.051 0 .02-.006.046-.017.075-.028.073-.044.173-.044.284 0 .202.032.311.135.459.009-.055.018-.09.025-.107.017-.042.064-.117.141-.223.125-.175.217-.388.217-.503l-.003-.035-.025-.189c0-.034.015-.067.041-.091.018-.016.038-.024.057-.024.035 0 .057.021.085.082.019.046.043.074.059.074.029 0 .086-.076.122-.165.028-.066.041-.125.041-.175a.29.29 0 0 0-.025-.106.29.29 0 0 1-.026-.094c0-.021.007-.031.034-.055.014-.01.028-.017.042-.017s.033.007.049.017l.074.05c.025.017.048.025.066.025.033 0 .081-.037.116-.092.072-.11.117-.201.117-.235 0-.022-.014-.038-.059-.07-.164-.116-.273-.292-.273-.442 0-.035.008-.074.025-.119l.082-.223.066-.174.024-.009c.028 0 .045.023.045.064a.28.28 0 0 1-.019.094.54.54 0 0 0-.035.157c0 .059.03.091.086.091a.35.35 0 0 0 .138-.033c.25-.108.448-.166.567-.166.112 0 .182.025.3.108.031.022.058.034.076.034s.032-.013.048-.034c.01-.014.017-.029.017-.041 0-.03-.018-.069-.05-.107-.042-.052-.059-.085-.059-.117 0-.026.017-.043.044-.043.13 0 .333.41.527 1.059.152.513.195.565.685.843.415.234.573.466.573.843 0 .371-.103.846-.317 1.469-.067.197-.097.33-.097.44 0 .087.017.172.081.394l.057.19a.98.98 0 0 0 .065.124zm-1.916-1.156a1.38 1.38 0 0 1 .157-.281c.134-.201.182-.308.182-.404v-.133c0-.124.022-.182.072-.182a.05.05 0 0 1 .044.025l.042.066c.013.021.035.033.058.033s.069-.032.093-.069a.58.58 0 0 0 .096-.295l-.009-.091-.008-.093c0-.05.02-.072.068-.072.032 0 .046.013.073.075.017.04.042.066.062.066s.049-.018.069-.05l.066-.099.05-.075.012-.04c0-.023-.018-.044-.062-.076-.153-.11-.22-.28-.231-.587a2.62 2.62 0 0 0-.041-.454c-.005-.016-.054-.134-.149-.355a2.15 2.15 0 0 1-.099-.297c-.019-.076-.043-.1-.103-.1-.225 0-.394.192-.394.445 0 .149.062.384.175.67.159.401.201.544.201.686 0 .303-.121.435-.54.586-.473.169-.695.413-.777.85l.083-.033.115-.041.132-.033c.114-.028.205-.092.223-.157l.033-.115c.008-.028.024-.047.05-.058l.074-.033.074-.033.016.033-.033.107a.74.74 0 0 0-.017.192c0 .181.036.287.141.418zm-4.437-7.639l.017.093c0 .026-.011.097-.033.213l-.015.168a1.35 1.35 0 0 0 .072.435c.042.119.053.156.053.188l-.003.035c-.008.034-.02.059-.032.059s-.046-.018-.084-.052c-.095-.082-.198-.125-.305-.125a.81.81 0 0 0-.141.017l.041.091.124.306.174.264.181.125a.2.2 0 0 1 .05.049c.017.023.025.042.025.055s-.014.027-.033.028l-.166.008c-.025.001-.045.017-.045.035 0 .028.024.111.071.246l.083.24.14.132.148.108c.044.031.07.067.07.093s-.014.046-.029.047l-.082.008c-.027.003-.042.026-.042.066 0 .062.058.174.166.322.128.174.218.232.364.232h.124c.059 0 .095.029.095.076 0 .039-.025.061-.079.073-.114.024-.149.047-.149.1 0 .021.015.053.042.09l.067.091c.055.075.117.125.157.125.022 0 .074-.023.121-.076.074-.085.159-.154.268-.181.287-.073.347-.103.347-.169 0-.023-.011-.056-.033-.095-.091-.164-.124-.27-.124-.404 0-.103-.027-.175-.064-.175-.015 0-.041.025-.068.067-.068.104-.184.174-.289.174-.286 0-.631-.448-.801-1.041a1.65 1.65 0 0 1-.061-.445 2.32 2.32 0 0 1 .061-.504l.091-.405.011-.112c0-.205-.189-.468-.481-.673z" data-v-7c4b1471></path><path d="M2.46 18.315c-.061.115-.117.374-.117.538 0 .096.031.173.072.173.013 0 .03-.016.045-.041l.091-.166.059-.101c.033-.049.057-.097.057-.123 0-.098-.091-.219-.207-.28zm.907 3.723l.041.041.066.066c.161.167.185.183.263.183.072 0 .222-.041.307-.084.036-.018.058-.04.058-.057 0-.011-.03-.044-.082-.091l-.091-.083c-.069-.062-.171-.12-.22-.12-.095 0-.192.042-.342.145zm1.918 1.495a.82.82 0 0 1 .165.083l.132.082c.025.011.07.017.131.017.038 0 .072-.006.1-.017l.174-.066.165-.058c.044-.016.075-.041.075-.064 0-.012-.037-.03-.091-.043-.037-.01-.079-.023-.124-.041-.203-.082-.233-.094-.305-.094a.55.55 0 0 0-.422.201zm2.092 1.66c.055-.133.093-.155.281-.166.212-.012.414-.391.414-.648 0-.025-.009-.047-.019-.047s-.03.01-.056.027l-.123.074-.24.107c-.16.071-.301.285-.301.453a.57.57 0 0 0 .045.2z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M3.807 18.018l.082.116a.78.78 0 0 0 .179.129c.025 0 .038-.018.053-.071l.033-.125.009-.138c0-.109-.048-.15-.273-.208-.056-.015-.079-.013-.115-.042s-.079-.05-.099-.05-.034.01-.034.023l.009.037.041.123.116.207zm.883-.001l-.108.191-.041.045-.008.001c-.044 0-.088-.1-.091-.204l-.016-.164c.013-.068.053-.098.156-.116.084-.015.14-.031.166-.049l.132-.091.014-.003c.022 0 .038.015.038.04l-.003.011-.058.141-.182.197z" data-v-7c4b1471></path><path d="M4.474 18.579l.132-.066c.024-.012.045-.018.063-.018.064 0 .112.048.112.134 0 .094-.074.221-.175.403-.109.197-.191.331-.276.331-.119 0-.287-.189-.451-.505a.54.54 0 0 1-.063-.23c0-.104.046-.159.133-.159.036 0 .07.008.095.026l.074.05a.39.39 0 0 0 .21.066c.052 0 .101-.011.145-.033z" data-v-7c4b1471></path></g><path d="M4.384 18.761s.076.016.076.067l-.002.024-.025.123c-.027.165-.065.267-.119.267a.08.08 0 0 1-.063-.035c-.085-.12-.15-.274-.15-.358 0-.059.026-.089.079-.089l.014.001.091.016.099-.016z" class="C" data-v-7c4b1471></path><path d="M23.617 5.468l.033.124.067.264c.104.472.191.595.421.595.131 0 .24-.073.273-.182l.049-.166c.011-.038.04-.06.079-.06.09 0 .138.099.145.299.002.076.019.211.049.404.023.139.033.267.033.382 0 .084-.01.133-.049.237l.016.099c.028.163.035.227.035.303 0 .354-.017.459-.101.597.034.135.044.199.044.296a2.47 2.47 0 0 1-.027.291l-.033.248.008.148c.022.055.033.089.033.099 0 .023-.015.041-.033.041l-.165-.066-.182-.099c-.059-.032-.1-.064-.19-.148-.16.141-.183.154-.373.206-.274.076-.375.11-.429.149s-.093.057-.114.057c-.064 0-.109-.124-.109-.306 0-.264.015-.401.058-.504-.161-.054-.242-.074-.3-.074a.69.69 0 0 0-.121.016l-.037.004c-.024 0-.038-.029-.038-.079 0-.081.057-.242.133-.378.103-.184.274-.338.405-.364l.297-.057c.176-.034.281-.128.281-.251s-.077-.197-.257-.261l-.347-.124c-.117-.042-.227-.094-.331-.157l-.181-.108c-.03-.016-.062-.024-.09-.024-.073 0-.118.054-.118.141 0 .14.108.255.24.255.03 0 .058-.006.084-.017a.27.27 0 0 1 .076-.025c.038 0 .065.036.065.089a1.04 1.04 0 0 1-.19.522l-.099.124-.075.099c-.027.213-.236.517-.446.652-.051.032-.082.054-.091.066s-.029.047-.066.116c-.017.032-.073.098-.166.198a1.55 1.55 0 0 1-.206.19c-.092.07-.148.121-.165.149l-.083.141c-.017.029-.038.043-.065.043-.074 0-.142-.077-.183-.207-.03-.097-.057-.125-.12-.125a.64.64 0 0 0-.145.025l-.115.016h-.29c-.156 0-.365.038-.463.084-.037.018-.066.026-.085.026-.043 0-.064-.043-.064-.126 0-.262.202-.659.464-.916-.039-.152-.119-.254-.223-.28l-.149-.041c-.015-.008-.025-.023-.025-.039 0-.053.059-.105.19-.167.235-.111.303-.132.434-.132.15 0 .276.051.433.174.224.174.285.207.393.207.176 0 .345-.161.345-.331 0-.114-.063-.241-.218-.438-.22-.281-.298-.464-.298-.702 0-.16.032-.247.174-.478.022-.037.035-.068.035-.089 0-.045-.075-.086-.159-.086-.088 0-.198.032-.347.1-.249.114-.397.22-.397.285 0 .026.015.052.041.07l.083.058c.01.007.017.02.017.034s-.016.043-.034.064-.072.09-.165.215a1.89 1.89 0 0 1-.346.363c-.045.034-.072.062-.084.083l-.066.124c-.055.103-.218.274-.356.371l-.115.091a.75.75 0 0 0-.075.115c-.084.151-.217.338-.305.429l-.223.231c-.064.066-.154.257-.24.511-.068.2-.111.29-.142.29-.014 0-.037-.018-.064-.05-.16-.183-.305-.298-.375-.298-.018 0-.051.015-.096.042-.063.038-.138.067-.223.083l-.297.057c-.249.049-.348.102-.463.248-.029.036-.053.057-.066.057-.029 0-.051-.043-.051-.1l.008-.072c.002-.008.016-.082.042-.223.043-.235.103-.43.165-.537l.149-.256c.026-.046.041-.093.041-.133 0-.19-.178-.345-.447-.387-.055-.008-.083-.028-.083-.06 0-.038.065-.111.166-.188.175-.132.368-.215.505-.215.092 0 .215.032.379.099l.182.074a.49.49 0 0 0 .184.043c.192 0 .379-.175.609-.57.104-.18.149-.308.149-.425 0-.209-.097-.335-.26-.335-.128 0-.25.054-.351.156-.09.091-.137.182-.137.264l.005.084.003.044c0 .067-.023.097-.072.097-.062 0-.135-.061-.278-.232l-.232-.272c-.251-.294-.303-.411-.314-.711-.28-.357-.35-.555-.35-.762l.011-.17c-.201-.241-.24-.334-.24-.572 0-.107.006-.135.05-.221-.099-.145-.13-.21-.157-.322l-.041-.174a.9.9 0 0 0-.132-.305c-.074-.106-.083-.122-.083-.153 0-.086.083-.142.29-.193l.066-.017-.017-.183c0-.048.006-.094.025-.197l.058-.306c.022-.119.034-.21.034-.271 0-.07-.011-.127-.05-.257-.025-.084-.054-.148-.082-.19s-.05-.084-.05-.109.028-.05.066-.05c.02 0 .055.01.083.027.279.152.584.393.801.636.187.208.247.381.289.843.253.188.348.358.348.626 0 .159-.052.313-.166.488l-.165.256c-.049.083-.075.179-.075.279 0 .149.045.289.157.488.123.219.234.314.365.314.217 0 .413-.374.413-.786l-.002-.081v-.223l.025-.198.016-.24.016-.199.006-.11c0-.081-.017-.105-.271-.376-.28-.3-.413-.554-.413-.789 0-.222.191-.408.42-.408a.6.6 0 0 1 .175.032v-.099l.009-.174.049-.214c.04-.171.058-.203.117-.203l.041.013.149.132.181.116.05.049c.012-.103.021-.124.107-.232l.115-.157.083-.124.05-.074c.033-.05.047-.063.069-.063s.038.029.064.121c.011.037.036.075.074.107l.108.091c.087.074.145.183.181.339a.49.49 0 0 1 .165-.149l.133-.074c.049-.028.079-.042.088-.042s.023.02.037.05c.019.043.041.085.066.124.106.171.116.196.116.311v.069.099c.268.074.372.218.372.514 0 .222-.102.451-.315.707-.141.17-.165.222-.165.35 0 .582.439.935 1.181.946l.545.008a2.67 2.67 0 0 1 .901.148c.382.125.507.158.623.158.084 0 .154-.047.154-.103 0-.119-.2-.212-.868-.401-1.361-.384-1.942-.935-1.942-1.836 0-.698.583-1.237 1.336-1.237a2.88 2.88 0 0 1 .82.141l.338.124a.52.52 0 0 0 .178.035c.117 0 .161-.073.178-.291.008-.097.039-.14.101-.14.087 0 .256.136.378.306s.215.404.215.554c0 .176-.092.31-.29.421-.185.282-.413.412-.72.412-.235 0-.635-.159-1.139-.454-.198-.116-.285-.148-.395-.148-.149 0-.244.083-.244.214 0 .119.088.194.334.28.391.139.475.185.587.323.421.073.604.146.868.346.754.242 1.174.707 1.174 1.303 0 .543-.368.935-.88.935a1.59 1.59 0 0 1-.402-.049z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M24.436 9.084a.58.58 0 0 1-.026-.152l.009-.112.026-.321c0-.061-.018-.109-.06-.166l-.099-.132c-.064-.087-.09-.151-.09-.225 0-.048.011-.064.044-.064l.038.008.074.041c.034.019.062.028.083.028.05 0 .075-.077.075-.232 0-.215-.04-.318-.165-.415l-.115-.091c-.02-.016-.034-.043-.034-.067 0-.04.023-.065.061-.065.023 0 .056.007.096.017l.091.025.035.008c.028 0 .041-.022.041-.07l-.002-.029-.025-.174c-.026-.18-.056-.249-.109-.249l-.015.002-.215.041-.036.003c-.258 0-.562-.245-.641-.515l-.157-.537c-.047-.161-.187-.338-.397-.504-.069-.053-.129-.085-.173-.09l-.148-.017c-.027-.003-.045-.019-.045-.04 0-.034.029-.05.09-.05a2.35 2.35 0 0 1 .732.14c.271.089.419.121.552.121.414 0 .663-.216.663-.573 0-.179-.065-.404-.157-.548-.081-.124-.216-.251-.389-.363-.025-.017-.048-.026-.065-.026-.023 0-.042.018-.042.042l.008.107v.01c0 .04-.023.073-.053.073-.046 0-.093-.054-.146-.165-.152-.323-.398-.517-.65-.517-.055 0-.087.019-.087.053 0 .025.015.063.043.109a1.02 1.02 0 0 1 .074.148c.013.032.02.056.02.071s-.013.024-.038.024c-.037 0-.098-.034-.139-.077l-.124-.132-.107-.166a.79.79 0 0 0-.33-.289l-.207-.107c-.041-.022-.078-.033-.105-.033s-.039.013-.039.038.013.059.037.095a.42.42 0 0 1 .076.165c0 .028-.018.05-.041.05-.058 0-.162-.104-.307-.305a.78.78 0 0 1-.158-.464c0-.348.268-.619.613-.619.177 0 .399.074.618.207l.397.24c.142.086.334.149.453.149.066 0 .085-.004.233-.042a.4.4 0 0 0 .107-.041.06.06 0 0 0 .03-.049c0-.032-.02-.047-.072-.05-.184-.014-.566-.164-.817-.323l-.273-.156c-.068-.024-.114-.059-.114-.088 0-.013.014-.02.036-.02l.048.008.36.132.347.124a2.18 2.18 0 0 0 .555.116c.157 0 .265-.108.265-.265 0-.079-.02-.168-.052-.23-.05-.099-.116-.182-.143-.182s-.05.033-.05.088l.004.044.01.106c0 .111-.061.184-.156.184a.46.46 0 0 1-.135-.026l-.537-.173c-.467-.151-.648-.19-.878-.19-.649 0-1.139.438-1.139 1.022 0 .658.452 1.071 1.612 1.472 1.125.39 1.272.479 1.272.775 0 .17-.16.307-.359.307a1 1 0 0 1-.194-.017l-.562-.132c-.345-.084-.637-.116-1.027-.116-.819 0-1.214.125-1.642.521-.099.092-.139.109-.249.109l-.056-.002-.13-.008c-.099 0-.174.047-.234.149l-.132.223c-.018.031-.054.049-.095.049-.086 0-.131-.057-.212-.256a.64.64 0 0 0-.165-.249l-.174-.149c-.026-.023-.041-.044-.041-.06s.015-.044.041-.08a.84.84 0 0 0 .075-.115c.026-.047.039-.081.041-.1l.008-.074c.006-.024.03-.041.059-.041.055 0 .064.036.066.24a.59.59 0 0 0 .066.223l.091.198c.012.026.041.042.075.042.086 0 .194-.201.223-.413.024-.176.043-.232.079-.232s.049.021.119.166c.022.044.056.087.099.124s.075.054.099.058h.009c.068 0 .091-.043.107-.199l.008-.082-.016-.116-.011-.08c0-.053.018-.085.049-.085.018 0 .04.012.062.033l.082.083c.055.055.111.083.166.083.071 0 .115-.031.115-.081 0-.041-.009-.061-.082-.183a2.51 2.51 0 0 1-.058-.107c-.073-.139-.108-.174-.173-.174l-.084.008-.107.041c-.032.012-.054.019-.066.019s-.025-.014-.025-.033c0-.074.049-.192.141-.336.057-.09.104-.311.14-.649l.042-.388c.025-.238.051-.323.099-.323.017 0 .031.007.033.017l.024.124v.099c0 .12.013.218.028.218.035 0 .087-.041.137-.111.036-.05.063-.106.083-.165.027-.089.042-.161.042-.217 0-.191-.081-.273-.339-.345-.026-.007-.043-.016-.049-.025l-.009-.074v-.14l-.017-.1-.032-.083-.042-.091-.066.041c-.169.106-.21.138-.265.206-.075.095-.101.119-.132.119-.023 0-.041-.016-.041-.036l.033-.141.003-.024c0-.181-.102-.356-.284-.487l-.058.066-.058.083-.099.116a.39.39 0 0 0-.092.245l.001.027.001.028c0 .065-.01.088-.041.088l-.018-.008-.066-.082-.082-.099-.149-.091-.074-.058c-.08.173-.083.191-.083.414v.069c0 .074-.024.119-.064.119-.019 0-.054-.014-.101-.041-.037-.021-.082-.034-.123-.034-.12 0-.209.102-.209.241 0 .151.032.255.133.42.065.107.15.194.19.194.005 0 .015-.076.025-.177l-.025-.14-.005-.063c0-.053.019-.086.05-.086.046 0 .075.051.104.19l.066.306.066.388a1.54 1.54 0 0 0 .099.33 1.66 1.66 0 0 0 .14.289c.092.118.102.134.102.16 0 .02-.017.038-.036.038s-.043-.009-.075-.025a.32.32 0 0 0-.099-.033l-.06-.009c-.137 0-.165.11-.165.658 0 .342.071.798.159 1.019l.174.438a.56.56 0 0 1 .042.205c0 .604-.817 1.569-1.678 1.983-.335.161-.434.328-.463.784a1.57 1.57 0 0 1 .289-.116c.244-.068.296-.103.33-.215l.033-.107c.015-.046.045-.068.215-.157.064-.033.109-.07.132-.107.043-.071.076-.106.098-.106s.036.016.036.043l-.002.013-.091.236-.01.058c0 .173.097.317.275.408l.024-.099c.029-.126.058-.209.083-.248l.158-.231c.057-.084.067-.139.067-.352l-.009-.26-.005-.066c0-.043.02-.066.058-.066.029 0 .053.011.055.025l.024.157c.006.033.04.058.081.058.08 0 .182-.102.291-.289.042-.073.059-.13.059-.197a1.9 1.9 0 0 0-.026-.257l-.003-.049c0-.041.024-.067.063-.067.021 0 .037.007.04.016l.058.191c.019.063.063.107.108.107.127 0 .258-.259.272-.537.008-.174.03-.235.083-.235.021 0 .036.008.041.02l.033.107c.012.039.036.065.059.065s.063-.037.115-.099a.83.83 0 0 0 .074-.107c.016-.029.024-.059.024-.087 0-.042-.009-.053-.074-.103s-.091-.104-.091-.173c0-.12.051-.248.198-.504.135-.232.153-.259.186-.259s.055.031.055.07c0 .024-.006.049-.017.072-.022.048-.034.081-.034.099 0 .032.029.059.066.059a.34.34 0 0 0 .15-.05 1.66 1.66 0 0 1 .223-.091l.281-.099c.052-.016.111-.025.17-.025.048 0 .096.006.144.017.094.022.158.034.19.034a.13.13 0 0 0 .133-.131.35.35 0 0 0-.051-.143l-.074-.115c-.036-.038-.051-.059-.051-.074l.009-.033c.01-.02.028-.033.047-.033.065 0 .193.133.308.323.648 1.057.962 1.391 1.438 1.535.496.15.613.255.613.551 0 .157-.083.346-.258.588-.33.458-.447.726-.447 1.031l.001.068.091-.042c.068-.034.112-.054.133-.058l.206-.041c.065-.013.117-.09.117-.173l-.001-.016-.016-.182-.002-.029c0-.102.05-.187.112-.187.023 0 .049.021.063.05l.132.28c.053.115.116.17.288.248zM17.663 3.42l-.008.132-.002.081c0 .134.021.192.071.192s.141-.102.179-.215a.84.84 0 0 0 .042-.224.41.41 0 0 0-.042-.156c-.037-.09-.075-.132-.121-.132-.071 0-.104.091-.118.322z" data-v-7c4b1471></path><path d="M17.089 1.893l.116.115c.169.169.285.513.285.846l-.004.137-.025.388-.008.325c0 1.079.314 1.815.776 1.815.116 0 .258-.107.356-.265.021-.034.049-.06.066-.06.026 0 .039.022.041.068l.017.182.016.156.025.174.025.162c0 .083-.017.092-.215.119a.55.55 0 0 0-.231.091c-.128.082-.203.139-.215.165l-.091.19c-.012.024-.032.041-.053.041-.048 0-.113-.073-.204-.231-.084-.146-.121-.23-.121-.269 0-.026.021-.037.072-.037h.017l.099.008.074-.008.067-.008c.028-.007.049-.031.049-.061 0-.02-.013-.038-.033-.046l-.372-.158c-.089-.038-.185-.134-.272-.272-.095-.152-.151-.285-.151-.36 0-.059.025-.097.065-.097.016 0 .037.006.061.019a.31.31 0 0 0 .097.033c.014 0 .03-.006.044-.017s.025-.027.025-.04c0-.024-.025-.055-.066-.085-.309-.219-.497-.485-.497-.704 0-.064.015-.094.047-.094l.061.006c.045.009.099.017.116.017.045 0 .076-.025.076-.064 0-.024-.03-.054-.076-.076-.206-.103-.257-.162-.388-.455-.101-.224-.153-.318-.24-.429.108-.052.166-.067.257-.067.113 0 .184.074.264.273.05.126.105.211.137.211.015 0 .021-.026.021-.095a.96.96 0 0 0-.034-.248c-.02-.075-.029-.137-.024-.182l-.017-.116-.091-.05c-.084-.045-.141-.154-.141-.271a.61.61 0 0 1 .017-.15l.05-.191a1.6 1.6 0 0 0 .059-.349v-.071l.074.082zm.992 5.953a1.22 1.22 0 0 1-.396-.123.75.75 0 0 0-.31-.084c-.131 0-.231.041-.351.141l.074.057.166.116.158.124c.04.032.089.05.139.05.162 0 .299-.074.522-.281zm3.182 1.502a.47.47 0 0 1 .165-.215c.231-.198.339-.345.339-.462v-.083-.099c0-.106.017-.141.073-.141.025 0 .045.01.051.025l.041.098c.01.023.036.037.07.037.059 0 .101-.042.203-.21.043-.071.061-.117.061-.164 0-.029-.006-.061-.02-.093l-.058-.131a.4.4 0 0 1-.034-.129c0-.054.027-.086.074-.086.025 0 .047.01.06.026l.065.082c.015.018.019.029.044.029s.085-.029.106-.07l.066-.132c.017-.033.025-.059.025-.075 0-.028-.026-.056-.067-.073l-.116-.05c-.135-.058-.248-.291-.248-.512 0-.025.009-.061.025-.107l.05-.141a.41.41 0 0 0 .025-.137c0-.081-.025-.159-.082-.251l-.099-.173c-.048-.123-.076-.164-.117-.164-.029 0-.057.011-.106.039l-.083.05-.116.133-.116.057c-.069.035-.135.184-.135.309 0 .178.039.275.309.757.114.204.168.355.168.47 0 .336-.343.613-.903.727-.669.137-.815.248-.892.678.116-.06.162-.068.366-.068l.121.002h.019c.111 0 .125-.008.196-.107a.62.62 0 0 1 .273-.23l.132-.058.107-.058.033-.009c.03 0 .046.012.046.035 0 .028-.027.076-.079.139-.08.097-.126.196-.126.272 0 .11.018.166.085.265z" data-v-7c4b1471></path><path d="M20.223 8.069c.095.059.191.145.248.223.079.108.09.116.169.116a.52.52 0 0 0 .12-.017l.074-.016c.141-.027.183-.047.183-.085 0-.023-.013-.046-.035-.064l-.099-.082-.091-.083a.44.44 0 0 0-.261-.092c-.103 0-.186.027-.309.1zm2.675.446l.049-.009.083-.008.066.008.045.003c.084 0 .127-.009.162-.036.051-.039.104-.101.157-.19.021-.035.032-.068.032-.094 0-.056-.051-.088-.141-.088-.227 0-.366.126-.454.413z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M19.816 2.81c.009.077.049.14.09.14.035 0 .09-.118.176-.263.117-.199.214-.365.214-.471 0-.035-.018-.062-.041-.062l-.009.003-.091.084a.94.94 0 0 1-.273.148c-.106.038-.136.07-.136.14l.012.067.069.143-.011.072zm-.724-.611c-.029-.024-.07-.042-.092-.042-.035 0-.05.018-.05.06l.002.024c.006.045.018.092.032.14a.72.72 0 0 0 .109.231l.125.209a.43.43 0 0 0 .18.114l.006.001c.022 0 .041-.014.043-.034l.026-.165.041-.149.005-.035c0-.04-.023-.076-.062-.096l-.141-.075-.222-.181z" data-v-7c4b1471></path><path d="M19.637 3.248h-.044l-.096-.009c-.003-.001-.037-.021-.1-.059-.037-.022-.077-.033-.116-.033-.076 0-.121.047-.121.126 0 .062.037.182.105.344l.108.264c.091.231.185.33.31.33.107 0 .193-.088.26-.263l.091-.372c.044-.116.153-.164.153-.245 0-.095-.106-.186-.196-.186l-.048.009-.124.084-.182.008z" data-v-7c4b1471></path></g><path d="M19.627 3.477h-.099c-.015 0-.03.009-.041.025-.016.023-.025.038-.025.041l.041.198.033.173c.019.095.034.133.105.133.081 0 .15-.055.159-.157l.017-.182.016-.188c0-.018-.009-.036-.024-.051s-.027-.018-.043-.018l-.007.001-.132.025z" class="C" data-v-7c4b1471></path><path d="M8.2 5.467l.033.124.066.264c.104.472.192.595.421.595.131 0 .24-.073.273-.182l.05-.166c.011-.038.04-.06.079-.06.089 0 .131.093.135.3.002.059.021.194.059.404.022.124.032.245.032.36a.59.59 0 0 1-.049.259c.028.162.05.38.05.506 0 .204-.038.394-.099.493a1.04 1.04 0 0 1 .042.311 1.55 1.55 0 0 1-.026.275l-.044.285a.35.35 0 0 0 .019.111c.016.047.025.081.025.099 0 .023-.011.041-.025.041l-.165-.066-.182-.099c-.059-.032-.099-.064-.19-.148-.161.144-.208.166-.628.281-.084.022-.146.048-.182.074-.052.037-.09.057-.11.057-.071 0-.105-.114-.105-.35 0-.175.018-.337.05-.46a.94.94 0 0 0-.295-.074 1.68 1.68 0 0 0-.126.016l-.031.003c-.024 0-.044-.036-.044-.079 0-.032.022-.112.051-.179l.082-.198c.03-.072.098-.154.198-.239a.53.53 0 0 1 .215-.125l.297-.057c.177-.034.282-.128.282-.251s-.08-.199-.257-.261l-.356-.124a1.91 1.91 0 0 1-.323-.157c-.197-.117-.23-.132-.277-.132-.071 0-.112.054-.112.147 0 .147.096.25.232.25.028 0 .056-.006.083-.017.04-.016.069-.025.085-.025.037 0 .058.022.058.063l-.002.036-.017.14c-.014.12-.092.265-.264.496l-.074.099a1.14 1.14 0 0 1-.446.652l-.091.066c-.007.008-.029.047-.066.116-.017.032-.072.098-.165.198-.07.076-.139.139-.207.19-.092.07-.148.121-.165.149l-.083.141c-.015.025-.04.041-.067.041-.073 0-.14-.076-.182-.206-.03-.097-.057-.125-.12-.125-.032 0-.081.009-.144.025l-.116.016h-.298c-.148 0-.358.039-.454.084-.038.018-.067.026-.085.026s-.033-.011-.055-.051c-.015-.028-.018-.041-.018-.066 0-.151.108-.435.249-.661.09-.143.119-.176.223-.264-.033-.14-.123-.249-.231-.28l-.141-.041c-.019-.006-.033-.022-.033-.04 0-.043.067-.102.191-.167.161-.085.324-.132.454-.132.137 0 .27.054.422.174.224.175.284.207.393.207.177 0 .345-.162.345-.331 0-.113-.06-.234-.218-.438-.23-.298-.299-.452-.299-.671 0-.136.042-.309.093-.386l.083-.123c.017-.028.028-.055.028-.08 0-.055-.064-.095-.149-.095-.13 0-.381.093-.581.215-.099.061-.166.132-.166.176 0 .026.013.05.034.064l.091.058c.011.007.018.02.018.034s-.014.038-.034.064l-.165.215a2.76 2.76 0 0 1-.347.363.57.57 0 0 0-.083.083c-.001.001-.023.042-.066.124a1.3 1.3 0 0 1-.364.371c-.061.042-.099.072-.115.091s-.034.053-.066.115c-.021.04-.057.098-.108.173-.081.12-.148.205-.198.256L3.4 8.67c-.071.071-.133.209-.231.511-.058.183-.111.29-.142.29-.014 0-.037-.018-.065-.051-.164-.187-.305-.298-.379-.298-.017 0-.048.014-.092.042-.061.038-.136.067-.223.083l-.305.057c-.231.044-.322.094-.455.248-.032.036-.058.057-.072.057-.025 0-.044-.041-.044-.096l.008-.077.042-.223c.044-.239.098-.416.165-.537l.14-.256c.028-.05.042-.095.042-.134 0-.19-.197-.365-.437-.387-.061-.005-.084-.02-.084-.054 0-.042.067-.125.159-.193.165-.126.382-.215.52-.215.084 0 .213.035.371.099l.182.074a.49.49 0 0 0 .184.043c.192 0 .379-.175.609-.57.104-.179.149-.308.149-.424 0-.208-.099-.336-.262-.336a.53.53 0 0 0-.357.156c-.083.084-.127.172-.127.256l.003.092.001.02c0 .077-.026.12-.072.12-.078 0-.191-.111-.516-.504-.242-.293-.296-.418-.306-.711-.28-.357-.35-.508-.35-.762l.011-.17c-.194-.233-.249-.36-.249-.574 0-.078.013-.124.059-.218-.099-.145-.13-.21-.157-.322l-.041-.174c-.028-.118-.073-.22-.132-.305-.075-.107-.083-.121-.083-.158 0-.096.066-.134.356-.205-.012-.08-.017-.142-.017-.183 0-.048.006-.094.025-.197l.058-.306c.023-.123.035-.211.035-.264s-.009-.098-.06-.263a.9.9 0 0 0-.074-.19c-.027-.042-.05-.094-.05-.113 0-.029.02-.045.057-.045.082 0 .298.131.522.316.474.392.572.549.637 1.023l.025.166c.227.167.347.369.347.585a1.03 1.03 0 0 1-.165.529l-.166.256a.5.5 0 0 0-.074.273c0 .155.045.298.157.494.127.223.235.314.368.314.218 0 .409-.379.409-.81v-.058l-.001-.149c0-.084.005-.167.018-.272l.024-.24.009-.199.003-.13c0-.046-.024-.093-.086-.158l-.181-.198c-.263-.287-.407-.556-.407-.757a.42.42 0 0 1 .422-.439.6.6 0 0 1 .175.032v-.056c0-.364.075-.633.175-.633l.04.013.149.132.181.116.05.049c0-.102.005-.113.108-.232.025-.028.063-.081.115-.157l.082-.124.042-.074c.022-.042.044-.062.068-.062s.042.03.072.12c.019.056.032.071.174.198.049.044.083.081.098.108s.036.078.067.164l.025.067a.5.5 0 0 1 .165-.149l.124-.074c.046-.028.078-.043.092-.043s.026.02.041.05.035.07.066.124c.088.153.117.227.117.299l-.002.081V1.9c.271.085.373.225.373.517 0 .21-.11.451-.323.705-.142.169-.158.202-.158.32 0 .611.426.965 1.175.976l.553.008c.356.006.569.041.9.148.393.128.507.158.621.158.038 0 .076-.012.107-.034s.049-.049.049-.073c0-.116-.203-.209-.867-.397-1.363-.386-1.941-.934-1.941-1.836 0-.697.58-1.237 1.33-1.237a2.89 2.89 0 0 1 .826.141l.338.124c.062.023.122.035.178.035.115 0 .174-.096.177-.291.003-.104.026-.14.094-.14.095 0 .242.119.377.306s.224.401.224.542c0 .188-.088.319-.29.432-.185.282-.414.412-.72.412-.239 0-.578-.135-1.139-.454-.196-.111-.295-.148-.398-.148-.146 0-.241.084-.241.214 0 .119.092.196.334.28.357.125.445.174.586.323.421.073.604.146.868.346.725.23 1.175.724 1.175 1.291 0 .551-.373.947-.892.947a1.6 1.6 0 0 1-.391-.05z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M9.021 9.083a.91.91 0 0 1-.03-.199l.006-.065.025-.132.009-.107c0-.115-.019-.196-.058-.248L8.872 8.2c-.032-.044-.061-.097-.082-.158-.011-.03-.017-.054-.017-.069 0-.037.023-.061.06-.061.022 0 .031.004.106.048.032.019.06.028.081.028.049 0 .076-.08.076-.22 0-.159-.015-.248-.05-.288l-.124-.14-.107-.091c-.026-.022-.044-.05-.044-.072 0-.035.027-.06.067-.06s.117.018.183.042l.038.008c.034 0 .047-.02.047-.068l-.002-.03-.025-.174-.049-.181c-.016-.059-.025-.068-.063-.068l-.021.003-.206.041-.033.003c-.271 0-.557-.228-.644-.515l-.165-.537c-.051-.166-.175-.323-.397-.504-.067-.055-.124-.086-.165-.09l-.149-.017c-.027-.003-.045-.019-.045-.041 0-.034.032-.049.103-.049l.272.024a2.14 2.14 0 0 1 .446.116c.247.085.414.121.554.121.413 0 .661-.223.661-.597 0-.256-.134-.545-.338-.731-.117-.104-.233-.182-.273-.182-.023 0-.042.019-.042.044l.008.105v.01c0 .041-.024.073-.057.073-.042 0-.089-.054-.143-.165-.156-.325-.404-.518-.661-.518-.043 0-.074.018-.074.044 0 .017.013.057.041.119l.066.148a.28.28 0 0 1 .027.076c0 .009-.016.018-.035.018-.038 0-.098-.034-.14-.078l-.123-.132-.108-.166c-.086-.132-.155-.192-.339-.289l-.198-.107c-.038-.022-.073-.033-.103-.033s-.044.014-.044.043.011.058.032.09c.061.094.085.141.085.166s-.019.049-.043.049c-.064 0-.197-.13-.315-.305-.097-.145-.149-.307-.149-.464 0-.355.259-.619.61-.619.181 0 .41.077.621.207l.388.24a1.02 1.02 0 0 0 .452.149 1.11 1.11 0 0 0 .242-.042c.036-.01.07-.023.099-.041.023-.014.039-.032.039-.046s-.009-.032-.023-.036l-.049-.016-.182-.034c-.047-.008-.122-.036-.223-.083l-.414-.206a1.61 1.61 0 0 0-.272-.156c-.067-.023-.109-.053-.109-.079s.013-.036.037-.036.054.011.088.032c.122.076.228.116.306.116l.355.124c.176.062.433.116.549.116.15 0 .271-.122.271-.273a.72.72 0 0 0-.143-.362c-.019-.027-.039-.042-.056-.042-.033 0-.051.024-.051.066l.008.067.013.098c0 .107-.072.191-.164.191a.44.44 0 0 1-.13-.026l-.537-.173c-.45-.145-.658-.19-.884-.19-.642.001-1.132.442-1.132 1.022 0 .657.462 1.081 1.603 1.472 1.123.385 1.281.479 1.281.768 0 .166-.168.314-.356.314a1.03 1.03 0 0 1-.198-.017l-.562-.132a4.18 4.18 0 0 0-1.028-.116c-.81 0-1.26.142-1.649.521-.097.095-.129.109-.241.109l-.056-.002-.116-.008h-.015c-.097 0-.169.045-.233.149l-.141.223c-.023.031-.058.049-.094.049-.074 0-.119-.056-.203-.256a.7.7 0 0 0-.173-.249l-.165-.149c-.027-.024-.042-.046-.042-.063s.012-.045.033-.077c.079-.115.107-.166.116-.215.022-.11.025-.115.071-.115s.066.051.07.24a.6.6 0 0 0 .067.223l.091.198c.011.026.041.042.074.042.085 0 .194-.201.223-.413.024-.177.042-.232.079-.232.015 0 .029.006.036.017s.034.061.074.148a.33.33 0 0 0 .108.124.34.34 0 0 0 .099.058l.01.001c.067 0 .09-.044.106-.2l.008-.082-.017-.116-.012-.08c0-.053.019-.085.049-.085.018 0 .04.012.062.033l.083.083c.055.054.11.082.163.082.069 0 .111-.028.111-.073 0-.04-.015-.079-.076-.191l-.058-.107c-.066-.125-.119-.174-.181-.174l-.083.008-.099.041c-.03.012-.052.019-.065.019s-.026-.014-.026-.033c0-.074.048-.191.14-.336.065-.105.11-.328.132-.649.028-.431.086-.711.146-.711.041 0 .057.035.057.127l-.004.113-.003.079c0 .077.017.139.037.139.071 0 .155-.105.222-.276.028-.072.043-.128.043-.168l-.009-.038-.165-.306-.165-.049c-.031-.009-.051-.023-.058-.037V1.9v-.09l-.016-.1-.033-.083-.041-.091c-.181.098-.267.163-.331.247s-.101.12-.133.12c-.021 0-.04-.02-.04-.044s.008-.065.025-.134l.005-.046c0-.178-.108-.352-.286-.465l-.05.066-.058.083-.099.116a.48.48 0 0 0-.083.165.32.32 0 0 0-.017.094v.013l.001.03c0 .069-.007.086-.038.086l-.02-.009-.058-.082-.09-.099-.14-.091-.074-.058-.033.074c-.026.058-.04.102-.041.133l-.009.132-.008.14.001.024c0 .072-.017.1-.061.1-.015 0-.057-.018-.097-.041a.24.24 0 0 0-.12-.034c-.129 0-.211.089-.211.228 0 .165.049.323.133.433l.107.14c.03.037.084.07.087.049l.02-.173-.025-.14-.005-.063c0-.053.018-.086.049-.086.047 0 .075.051.105.19l.066.306.067.388a1.54 1.54 0 0 0 .099.33 1.71 1.71 0 0 0 .141.289c.092.118.102.134.102.16 0 .02-.017.038-.035.038s-.043-.009-.074-.025a.34.34 0 0 0-.099-.033l-.065-.009c-.133 0-.16.114-.16.665a3.35 3.35 0 0 0 .159 1.012l.173.438a.54.54 0 0 1 .041.205c0 .602-.799 1.547-1.677 1.983-.327.161-.431.317-.454.676l-.008.108a1.15 1.15 0 0 1 .28-.116c.246-.064.294-.095.331-.215l.032-.107c.013-.04.033-.055.223-.157.065-.034.111-.072.133-.107.041-.07.076-.107.098-.107s.032.01.032.027l-.007.029-.082.236-.01.058c0 .175.096.317.274.408.029-.191.049-.256.108-.346l.148-.231c.055-.086.078-.19.078-.353a1.53 1.53 0 0 0-.02-.258l-.006-.065c0-.042.022-.066.058-.066.029 0 .052.01.055.024l.033.157c.006.034.041.058.08.058.08 0 .183-.103.291-.289a.44.44 0 0 0 .061-.219l-.012-.112-.025-.123-.002-.024c0-.054.03-.092.073-.092.019 0 .034.007.037.016l.049.191c.017.063.061.107.107.107.056 0 .151-.096.207-.207a.73.73 0 0 0 .075-.338c0-.176.018-.227.083-.227.021 0 .037.007.041.02l.033.107c.013.039.036.065.059.065.034 0 .127-.1.189-.206.017-.027.025-.055.025-.077 0-.035-.024-.071-.075-.113-.086-.071-.091-.082-.091-.19s.017-.158.091-.289l.107-.198c.104-.204.142-.257.18-.257.032 0 .06.034.06.073 0 .022-.006.045-.017.068a.28.28 0 0 0-.033.098c0 .032.027.059.061.059s.088-.017.146-.05c.022-.013.099-.042.231-.091l.273-.099a.49.49 0 0 1 .165-.025.66.66 0 0 1 .148.017l.191.034c.075 0 .141-.064.141-.138 0-.055-.057-.163-.133-.251-.028-.033-.042-.058-.042-.075l.01-.033c.011-.02.028-.033.044-.033.056 0 .186.139.303.323.711 1.119.966 1.39 1.446 1.535.486.148.613.261.613.55 0 .158-.081.341-.258.589-.36.503-.447.706-.447 1.038v.061l.091-.042c.067-.034.112-.054.132-.058l.206-.041c.062-.013.117-.092.117-.169l-.001-.021-.025-.182-.002-.024c0-.064.032-.138.076-.174.014-.01.028-.018.043-.018.023 0 .051.022.065.051l.132.28c.054.115.117.169.288.248zm-6.66-5.985c-.06 0-.091.052-.097.165l-.016.156-.02.2c0 .118.035.205.083.205s.135-.101.177-.216c.025-.07.037-.13.037-.193 0-.055-.013-.121-.037-.187-.03-.084-.076-.132-.126-.132zm-.685-1.205l.116.115c.189.188.282.501.282.945 0 .098-.005.163-.026.425l-.017.34c0 1.024.333 1.798.774 1.798.139 0 .229-.066.359-.264.021-.032.054-.06.075-.06s.039.021.041.068l.017.182.017.156.016.174.025.157.002.02c0 .046-.02.072-.059.079l-.149.025c-.074.012-.152.043-.231.091-.125.075-.199.132-.215.165l-.091.19c-.013.024-.032.041-.052.041-.048 0-.125-.084-.213-.231-.075-.126-.121-.228-.121-.27 0-.026.02-.036.073-.036h.016l.107.008.066-.008.075-.008c.028-.004.05-.028.05-.058 0-.022-.014-.041-.033-.049l-.371-.158a.58.58 0 0 1-.281-.272c-.102-.196-.15-.313-.15-.371 0-.051.027-.085.067-.085.014 0 .034.006.058.018a.33.33 0 0 0 .105.033c.032 0 .061-.025.061-.052s-.03-.062-.066-.089c-.31-.225-.488-.484-.488-.707 0-.062.015-.092.048-.092l.06.007.107.016h.005c.04 0 .071-.026.071-.064 0-.023-.028-.054-.068-.076l-.132-.074c-.046-.029-.085-.068-.115-.116a7.22 7.22 0 0 1-.14-.264l-.116-.223-.075-.14a.75.75 0 0 0-.05-.066.52.52 0 0 1 .257-.067c.113 0 .184.074.264.273.05.126.105.211.136.211.016 0 .021-.027.021-.097a.94.94 0 0 0-.034-.246c-.02-.075-.029-.137-.025-.182l-.016-.116-.099-.05c-.022-.01-.048-.04-.075-.082-.046-.072-.066-.133-.066-.188 0-.035.009-.092.025-.15.084-.315.108-.438.108-.54l-.001-.071.075.082z" data-v-7c4b1471></path><path d="M2.666 7.846c-.115-.017-.169-.03-.24-.058l-.157-.065c-.133-.057-.233-.084-.316-.084-.125 0-.227.042-.344.141l.074.057.165.116.157.124a.22.22 0 0 0 .136.05c.162 0 .303-.076.525-.281zm3.179 1.503c.045-.103.067-.131.165-.215.231-.198.338-.345.338-.462v-.083-.099c0-.106.018-.141.073-.141.025 0 .044.01.051.025l.041.098c.01.023.037.038.07.038s.079-.038.121-.096l.083-.115c.041-.058.06-.107.06-.161a.27.27 0 0 0-.018-.095l-.058-.132a.39.39 0 0 1-.034-.129c0-.054.027-.086.074-.086.024 0 .046.01.059.026l.066.082c.014.018.038.029.063.029.03 0 .065-.029.086-.07l.066-.132c.017-.033.026-.059.026-.074 0-.026-.027-.052-.075-.075l-.107-.05c-.142-.065-.249-.292-.249-.529a.24.24 0 0 1 .017-.09l.058-.141a.33.33 0 0 0 .025-.132c0-.086-.028-.172-.083-.256l-.108-.173c-.049-.13-.074-.167-.114-.167l-.042.01-.058.033-.091.05-.107.133-.115.057c-.069.035-.135.182-.135.303 0 .215.024.274.307.762.11.188.168.352.168.472 0 .338-.303.579-.911.725l-.38.091c-.28.067-.445.259-.504.587.116-.06.161-.068.366-.068l.122.002h.019c.11 0 .125-.008.195-.107.082-.114.162-.182.273-.23l.132-.058.107-.058.033-.009c.03 0 .046.012.046.034 0 .032-.03.086-.078.14-.095.106-.132.194-.132.32 0 .08.019.126.091.217z" data-v-7c4b1471></path><path d="M4.805 8.068a.95.95 0 0 1 .248.223c.079.108.09.116.168.116.033 0 .074-.006.121-.017l.074-.016c.136-.026.183-.046.183-.08 0-.02-.015-.045-.042-.069l-.091-.082-.091-.083a.45.45 0 0 0-.273-.093c-.099 0-.157.02-.298.101zm2.679.447l.049-.009.075-.008.074.008.036.003c.183 0 .36-.155.36-.319 0-.057-.052-.088-.146-.088-.223 0-.362.128-.449.414z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M4.402 2.81c.009.076.049.14.089.14.035 0 .075-.091.161-.237.117-.198.229-.392.229-.497 0-.035-.017-.062-.041-.062l-.008.003-.091.083c-.066.06-.118.088-.281.148-.093.036-.127.073-.127.141l.012.066.041.116.017.099z" data-v-7c4b1471></path><path d="M3.896 2.379l.14.075c.039.02.06.047.06.08l-.01.052c-.02.07-.004.147-.005.176-.007.148-.119.146-.165.146-.027 0-.013-.017-.078-.091l-.182-.206c-.056-.063-.105-.208-.124-.371l-.001-.021c0-.045.014-.062.05-.062.022 0 .063.018.091.042l.223.181zm.323.867h-.045l-.095-.009a3.81 3.81 0 0 1-.099-.059c-.037-.022-.078-.033-.118-.033-.075 0-.123.041-.123.104s.033.181.109.367l.108.264c.094.232.186.331.307.331.108 0 .123-.106.196-.289.165-.415.257-.476.257-.623 0-.1-.022-.127-.113-.127l-.077-.018-.132.084-.174.008z" data-v-7c4b1471></path></g><path d="M4.212 3.477h-.107c-.016 0-.032.009-.041.025l-.016.041.041.198.033.173c.018.095.053.133.124.133.081 0 .131-.055.14-.157l.016-.182.016-.188c0-.018-.009-.036-.025-.051s-.027-.018-.041-.018l-.008.001-.132.025z" class="C" data-v-7c4b1471></path><path d="M14.987 10.884c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3h-.705c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3h-.705c-.072-.176-.245-.3-.447-.3s-.375.124-.447.3H9.98v4.28h.81v1.641h.95v-1.64h2.399v1.64h.949v-1.64h.809v-4.28h-.911z" class="B" data-v-7c4b1471></path><g class="C" data-v-7c4b1471><path d="M10.965 15.539h.492v.632h-.492v-.632zm0 .774h.492v.2h-.492v-.2zm3.351-.774h.492v.632h-.492v-.632zm0 .774h.492v.2h-.492v-.2z" data-v-7c4b1471></path></g><g fill="#e73337" data-v-7c4b1471><path d="M10.309 11.234h5.256v.195h-5.256v-.195zm.002 3.599h5.254v-3.22h-5.256l.002 3.22z" data-v-7c4b1471></path></g><g class="C" data-v-7c4b1471><use xlink:href="#B" data-v-7c4b1471></use><path d="M12.715 11.1c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1zm1.598 0c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1zm-2.633 2.158c.339.424.766.633 1.294.633.489 0 .826-.175 1.222-.633-.395-.405-.763-.575-1.246-.575-.508 0-.966.207-1.27.575z" data-v-7c4b1471></path></g><g class="B" data-v-7c4b1471><path d="M12.938 12.873c-.364 0-.659.113-1.008.383.289.286.629.433 1.002.433.42 0 .694-.116 1.006-.425-.358-.284-.633-.391-1-.391zm-.014.483c-.195 0-.36-.052-.569-.183.174-.094.37-.142.578-.142a1.38 1.38 0 0 1 .597.142 1.03 1.03 0 0 1-.605.183zm2.079 1.191c-.157 0-.284-.127-.284-.283s.127-.284.284-.284.284.127.284.284-.127.283-.284.283z" data-v-7c4b1471></path></g><path d="M15.002 14.049c-.119 0-.216.097-.216.216s.097.215.216.215.216-.097.216-.215-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path><path d="M10.887 14.547c-.161 0-.293-.127-.293-.283s.127-.284.284-.284.284.127.284.284a.28.28 0 0 1-.275.283z" class="B" data-v-7c4b1471></path><path d="M10.876 14.049c-.119 0-.216.097-.216.216s.101.215.225.215c.114 0 .208-.097.208-.215s-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path><path d="M10.877 12.593c-.156 0-.284-.127-.284-.284s.127-.283.284-.283.284.127.284.283-.128.284-.284.284z" class="B" data-v-7c4b1471></path><use xlink:href="#C" class="C" data-v-7c4b1471></use><path d="M15.003 12.593c-.157 0-.284-.127-.284-.284s.127-.283.284-.283.284.127.284.283-.127.284-.284.284z" class="B" data-v-7c4b1471></path><path d="M15.002 12.092c-.119 0-.216.097-.216.216s.097.216.216.216.216-.097.216-.216-.097-.216-.216-.216z" class="C" data-v-7c4b1471></path></g><defs data-v-7c4b1471><clipPath id="A" data-v-7c4b1471><path fill="#fff" d="M0 0h148.235v30H0z" data-v-7c4b1471></path></clipPath><path id="B" d="M11.113 11.1c0-.124.101-.225.225-.225s.225.101.225.225v.591c0 .124-.101.225-.225.225s-.225-.101-.225-.225V11.1z" data-v-7c4b1471></path><path id="C" d="M10.876 12.092c-.119 0-.216.097-.216.216s.097.216.216.216.216-.097.216-.216-.097-.216-.216-.216z" data-v-7c4b1471></path></defs></svg></a></div><div class="gh-mobile-menu-options"><div class="gh-menu-option"><button aria-expanded="false"><div class="apl-icon apl-icon--profile" style="color:;" title="Login and registration" data-v-27c0a44c><!----></div></button></div><div class="gh-menu-option"><a href="/core/shopping-cart" rel="nofollow"><div class="apl-icon apl-icon--shopping-cart" style="color:;" title="Shopping cart" data-v-27c0a44c><!----></div><!----></a></div><div class="gh-menu-option"><button aria-expanded="false" aria-controls="AdditionalMenuOptions"><div class="apl-icon apl-icon--chevron-down" style="color:;" title="Additional menu options" data-v-27c0a44c><!----></div></button></div></div></div><!----><!----></div><div class="gh-popovers gh-desktop-nav"><div class="gh-popoverContainer gh-popoverContainer-discoverContentPopover" id="gh-discoverContentPopover"><button class="gh-popoverTrigger gh-popoverHeader" aria-expanded="false"><!--[--><div class="gh-menu-option"><svg width="18" height="18" viewbox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg" class="gh-icon"><path d="M10 19C14.9706 19 19 14.9706 19 10C19 5.02944 14.9706 1 10 1C5.02944 1 1 5.02944 1 10C1 14.9706 5.02944 19 10 19Z" class="stroke-current" fill="none" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path><path d="M13.816 6.18401L11.908 11.908L6.18401 13.816L8.09201 8.09201L13.816 6.18401Z" class="stroke-current" fill="none" stroke-width="1.4" stroke-linecap="round" stroke-linejoin="round"></path></svg> Discover Content </div><!--]--><svg width="12" height="7" viewbox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg" class="gh-icon gh-chevron"><path d="M1.28033 0.21967C0.987436 -0.0732233 0.512564 -0.0732233 0.21967 0.21967C-0.0732231 0.512563 -0.0732231 0.987437 0.21967 1.28033L5.21967 6.28033C5.51256 6.57322 5.98744 6.57322 6.28033 6.28033L11.2803 1.28033C11.5732 0.987437 11.5732 0.512563 11.2803 0.21967C10.9874 -0.0732233 10.5126 -0.0732233 10.2197 0.21967L5.75 4.68934L1.28033 0.21967Z" class="fill-current" stroke="none"></path></svg></button><!----></div><div class="gh-popoverContainer gh-popoverContainer-productsAndServicesPopover" id="gh-productsAndServicesPopover"><button class="gh-popoverTrigger gh-popoverHeader" aria-expanded="false"><!--[--><div class="gh-menu-option"><div class="apl-icon apl-icon--grid" style="color:;" data-v-27c0a44c><!----></div> Products and Services </div><!--]--><svg width="12" height="7" viewbox="0 0 12 7" fill="none" xmlns="http://www.w3.org/2000/svg" class="gh-icon gh-chevron"><path d="M1.28033 0.21967C0.987436 -0.0732233 0.512564 -0.0732233 0.21967 0.21967C-0.0732231 0.512563 -0.0732231 0.987437 0.21967 1.28033L5.21967 6.28033C5.51256 6.57322 5.98744 6.57322 6.28033 6.28033L11.2803 1.28033C11.5732 0.987437 11.5732 0.512563 11.2803 0.21967C10.9874 -0.0732233 10.5126 -0.0732233 10.2197 0.21967L5.75 4.68934L1.28033 0.21967Z" class="fill-current" stroke="none"></path></svg></button><!----></div><div class="gh-profile"><div class="gh-menu-option"><a href="https://www.cambridge.org/core/register"><div class="apl-icon apl-icon--register" style="color:;" data-v-27c0a44c><!----></div>Register</a></div><div class="gh-menu-option"><a href="/core/login"><div class="apl-icon apl-icon--sign-in" style="color:;" data-v-27c0a44c><!----></div>Log In</a></div></div><!----><div class="gh-menu-option"><a href="/core/shopping-cart" rel="nofollow"><div class="apl-icon apl-icon--shopping-cart" style="color:;" data-v-27c0a44c><!----></div> (0) Cart</a></div></div></nav></div></div></div></div><!----><!--]--></div>
</div></div><script></script>
        </div>
        <global-header
          id="global-header-wc"
          class="global-header"
          environment="prod"
          show-discovery-tool="true"
          register-url="https://www.cambridge.org/core/register?ref&#x3D;/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"
          login-url="/core/login?ref&#x3D;/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"
          logout-url="/core/logout?ref&#x3D;/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"
          my-account-url="/core/my-core"
          basket-url="/core/shopping-cart"
          basket-items-count="0"
          style="display:none"
        ></global-header>
      </div>

      <script>
          const globalHeader = $('#global-header');
          const globalHeaderWc = $('#global-header-wc');
          const maintenanceMessage = $('#maintenance-message');
          const maintenanceMessageWc = $('#maintenance-message-wc');
          const isSAPCDCEnabled = false; 
          globalHeaderWc.on('initialized', function () {
            globalHeader.hide();
            globalHeaderWc.show();

            if (!isSAPCDCEnabled) {
              return
            }
            // Ezproxy implementation to prevent rewrite for SAP CDC Register url
            const globalHeaderEl = globalHeaderWc[0];
            const menuOptions = globalHeaderEl.shadowRoot.querySelectorAll('.gh-menu-option');
            $(menuOptions).each(function() {
              const anchor = $(this).find('a')[0];
              if (anchor && anchor.textContent.trim() === 'Register') {
                const encodedHref = anchor.getAttribute('href');
                if (encodedHref) {
                    const decodedHref = atob(encodedHref);
                    const newHref = `${decodedHref}&state_redirect=${encodeURIComponent(window.location.href)}`;
                    anchor.setAttribute('href', newHref);
                }
              }
            });
          });
          
          maintenanceMessageWc.on('initialized', function () {
            maintenanceMessage.hide();
            maintenanceMessageWc.show();
          });

      </script>

</header>

<div class="off-canvas-wrap desktop overflow-visible">
    <div class="inner-wrap">

    </div>
</div>

    <div id='platform-header'>
      <div class="__shared-elements-html ShEl"><div class="__shared-elements-head">

<link rel="stylesheet" href="/aca/shared-elements/_nuxt/entry.D9LY0ri8.css">

<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-404.B06nACMW.css">


<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-500.WGRfNq7F.css">

</div><div class="__shared-elements-body"><div id="__sharedElements-fcfmc8"><!--[--><!--[--><div class="apl"><section class="apl-platform-header apl-theme--core" data-v-acbd3a3e><div tabindex="0" data-v-acbd3a3e><div class="apl-container apl-container--no-padding apl-container--full-width apl-platform-header__container--full-width" data-v-acbd3a3e><!--[--><div class="apl-container apl-platform-header__container apl-platform-header__container--mobile" data-v-2169278d data-v-acbd3a3e><!--[--><div class="apl-platform-header__branding" data-v-2169278d><!--[--><!--[--><a href="/core/" class="platform-logo" title="Cambridge Core homepage"><div><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 209 20" fill="none" role="img" width="140" height="30"><title> Logo for Cambridge Core from Cambridge University Press. Click to return to homepage. </title><path d="M173.541 7.465c-4.172 0-7.648 3.521-7.648 7.747s3.476 7.746 7.648 7.746 7.648-3.521 7.648-7.746-3.337-7.747-7.648-7.747zm0 11.408c-2.086 0-3.616-1.69-3.616-3.662s1.669-3.662 3.616-3.662c2.086 0 3.615 1.69 3.615 3.662.139 2.113-1.529 3.662-3.615 3.662zm-159.357.704c-.973.563-1.947.704-3.198.704a8.31 8.31 0 0 1-3.337-.704c-.973-.423-1.808-.986-2.503-1.831-.695-.704-1.112-1.69-1.53-2.676s-.556-1.972-.556-3.099.139-2.253.556-3.38.834-1.972 1.53-2.676 1.53-1.408 2.503-1.831 2.086-.704 3.198-.704c.973 0 1.947.141 2.781.563.973.423 1.669.986 2.225 1.831l2.364-1.831c-.834-1.127-1.947-1.972-3.198-2.394-1.391-.563-2.642-.845-4.033-.845-1.669 0-3.059.282-4.45.845S4.033 2.817 3.059 3.944C2.086 4.93 1.391 6.197.834 7.465.278 8.873 0 10.282 0 11.972c0 1.268.278 2.676.695 3.944.556 1.268 1.251 2.394 2.086 3.521.973.986 2.086 1.831 3.337 2.535 1.391.563 2.781.986 4.45.986 1.252 0 2.225-.141 3.198-.422s1.669-.704 2.364-1.127 1.112-.845 1.53-1.268.695-.704.834-.986l-2.364-1.831c-.278.986-.973 1.69-1.947 2.254zm18.773.282v-7.183c0-.563-.139-1.268-.278-1.831-.278-.563-.556-1.127-1.112-1.549-.417-.423-1.112-.845-1.808-.986-.695-.282-1.669-.423-2.642-.423s-1.947.141-2.92.563c-.973.282-1.808.845-2.642 1.549l1.53 1.69c.278-.141.417-.422.695-.563s.556-.423.834-.563.695-.282 1.112-.423S26.56 10 27.116 10s1.112.141 1.53.282.695.422.973.704.417.563.556.986.139.704.139 1.127v.563h-1.947c-1.113 0-2.086.141-3.059.282s-1.808.422-2.503.845-1.251.845-1.669 1.549c-.417.563-.556 1.409-.556 2.254 0 .704.139 1.268.417 1.831s.556.986 1.113 1.268c.417.423.973.563 1.53.845.556.141 1.252.282 1.808.282 1.112 0 1.947-.141 2.781-.563s1.53-.986 2.086-1.972c0 .704 0 1.408.139 2.113h2.364c0-.423 0-.704-.139-1.127.417-.423.278-.845.278-1.408zm-2.364-3.239c0 .423 0 .986-.139 1.408s-.417.986-.695 1.268c-.278.422-.695.704-1.251.986s-1.112.422-1.808.422c-.973 0-1.669-.141-2.225-.563s-.834-.986-.834-1.69c0-.563.139-.986.556-1.408.278-.423.834-.563 1.391-.845.556-.141 1.112-.282 1.808-.423.695 0 1.252-.141 1.947-.141h1.391v.986h-.139zM56.039 10c-.417-.563-.973-.986-1.669-1.408s-1.53-.563-2.642-.563c-.834 0-1.669.282-2.503.704s-1.53 1.127-1.947 1.972c-.417-.845-.973-1.549-1.808-1.972-.695-.423-1.53-.704-2.642-.704-.695 0-1.251.141-1.669.282-.556.141-.973.423-1.391.704s-.695.563-.973.845-.417.563-.417.704V8.31h-2.503v14.085h2.642V15.07c0-.845.139-1.549.278-2.113s.417-1.127.834-1.408c.278-.423.695-.704 1.113-.845s.834-.282 1.39-.282c.695 0 1.113.141 1.53.282.417.282.695.563.834.986s.278.845.417 1.409c0 .563.139 1.127.139 1.831v7.324h2.642v-7.746c0-1.268.278-2.253.834-3.098s1.53-1.127 2.781-1.127c.695 0 1.112.141 1.53.282.417.282.695.563.973.845.278.422.417.845.417 1.268 0 .563.139.986.139 1.549v7.887h2.642v-8.169c0-.845-.139-1.549-.278-2.254 0-.422-.278-1.127-.695-1.69zm16.826.141c-.556-.704-1.391-1.127-2.225-1.549s-1.808-.563-2.92-.563c-.973 0-1.947.282-2.781.704s-1.53.986-2.086 1.69V0h-2.642v22.394h2.642v-1.972a6.17 6.17 0 0 0 2.086 1.69c.834.422 1.808.563 2.781.563 1.112 0 2.086-.141 2.92-.563s1.669-.845 2.225-1.549 1.112-1.408 1.391-2.394c.278-.845.556-1.831.556-2.958 0-.986-.139-1.972-.556-2.958-.278-.704-.695-1.408-1.391-2.113zm-1.252 7.183c-.278.563-.556 1.127-.973 1.549s-.834.704-1.391.986-1.251.422-1.947.422-1.391-.141-1.947-.422-1.112-.563-1.391-.986c-.417-.422-.695-.986-.973-1.549s-.278-1.268-.278-1.972.139-1.408.278-1.972c.278-.563.556-1.127.973-1.549s.834-.704 1.391-.986 1.252-.423 1.947-.423 1.391.141 1.947.423 1.112.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c.139.704 0 1.409-.278 1.972zM82.46 8.169c-.417.141-.834.282-1.112.563-.417.282-.695.423-.973.845-.278.282-.556.704-.695.986V8.451h-2.642v14.085h2.642v-7.042c0-1.549.278-2.676.973-3.38s1.669-1.127 2.92-1.127h.695c.278 0 .417.141.695.141l.139-2.817c-.417-.141-.695-.141-1.112-.141a2.82 2.82 0 0 0-1.53 0zm5.145 14.225h2.642V8.31h-2.642v14.085zm1.252-20.986a1.77 1.77 0 0 0-1.391.563c-.278.423-.556.845-.556 1.408s.139.986.556 1.408a1.77 1.77 0 0 0 1.391.563 1.77 1.77 0 0 0 1.39-.563c.417-.423.556-.845.556-1.408s-.139-.986-.556-1.408a1.77 1.77 0 0 0-1.39-.563zm15.574 9.014a6.17 6.17 0 0 0-2.085-1.69c-.835-.423-1.808-.704-2.781-.704-1.112 0-2.086.141-2.92.563s-1.669.845-2.225 1.549-1.112 1.409-1.391 2.394c-.278.845-.556 1.831-.556 2.958 0 .986.139 1.972.556 2.958.278.845.834 1.69 1.391 2.394s1.391 1.268 2.225 1.549c.834.422 1.808.563 2.92.563.974 0 1.807-.141 2.781-.563a6.17 6.17 0 0 0 2.085-1.69v1.972h2.643V0h-2.643v10.423zm-.417 6.901c-.278.563-.556 1.127-.973 1.549s-.834.704-1.391.986-1.251.422-1.946.422-1.391-.141-1.947-.422-1.112-.563-1.39-.986c-.417-.422-.695-.986-.973-1.549s-.278-1.268-.278-1.972.139-1.408.278-1.972c.278-.563.556-1.127.973-1.549s.834-.704 1.39-.986 1.252-.423 1.947-.423 1.39.141 1.946.423 1.113.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c.139.704 0 1.409-.278 1.972zm17.104-6.901c-.556-.704-1.251-1.268-2.225-1.69-.834-.423-1.808-.563-2.781-.563-1.112 0-2.086.141-2.92.563s-1.669.845-2.225 1.549-1.112 1.408-1.39 2.394c-.279.845-.557 1.831-.557 2.958 0 .986.139 1.972.557 2.817s.834 1.69 1.529 2.253c.695.704 1.391 1.127 2.225 1.549s1.808.563 2.781.563 1.947-.141 2.781-.563a5.21 5.21 0 0 0 2.086-1.831h.139v2.113c0 .563-.139 1.127-.278 1.831-.139.563-.417 1.127-.834 1.549s-.835.845-1.53 1.127-1.53.422-2.503.422-1.947-.141-2.781-.563-1.53-.986-2.225-1.69l-1.808 2.253c.974.986 2.086 1.549 3.199 1.972s2.364.563 3.615.563c1.112 0 2.225-.141 3.059-.563.974-.422 1.808-.845 2.364-1.549.695-.704 1.252-1.408 1.53-2.253.417-.845.556-1.831.556-2.958V8.732h-2.642v1.69h.278zm-1.251 8.451c-.835.845-1.947 1.268-3.338 1.268s-2.503-.422-3.337-1.268-1.252-1.972-1.252-3.38c0-.704.139-1.408.279-1.972.278-.563.556-1.127.973-1.549s.834-.704 1.39-.986 1.252-.423 1.947-.423 1.391.141 1.947.423 1.112.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c0 1.408-.417 2.535-1.251 3.38zm18.633-8.451c-.556-.704-1.251-1.268-2.086-1.69s-1.947-.704-3.198-.704c-1.113 0-1.947.141-2.92.563-.835.423-1.669.845-2.225 1.549-.695.704-1.113 1.409-1.53 2.394-.417.845-.556 1.831-.556 2.958 0 .986.139 1.972.556 2.958.278.845.835 1.69 1.391 2.394s1.39 1.127 2.225 1.549 1.946.563 3.059.563c2.642 0 4.728-.986 6.257-2.817l-2.085-1.831c-.557.704-1.113 1.127-1.669 1.549s-1.391.563-2.086.563c-.556 0-1.251-.141-1.808-.282s-1.112-.422-1.529-.845-.835-.845-1.113-1.268a3.79 3.79 0 0 1-.417-1.69h11.264v-.845c0-.845-.139-1.831-.418-2.676-.139-.986-.556-1.69-1.112-2.394zm-9.734 3.803c0-.423.139-.986.278-1.408s.556-.986.835-1.268c.417-.423.834-.704 1.39-.986a3.81 3.81 0 0 1 1.808-.423c.695 0 1.251.141 1.808.282s.973.563 1.251.845c.278.422.556.845.695 1.268.139.563.278.986.278 1.69h-8.343zm30.592 3.944c-.695.422-1.529.563-2.503.563-.834 0-1.529-.141-2.364-.563-.695-.282-1.39-.845-1.807-1.408-.557-.563-.974-1.268-1.252-2.113a8.3 8.3 0 0 1-.417-2.676 8.3 8.3 0 0 1 .417-2.676c.278-.845.695-1.549 1.252-2.113s1.112-1.127 1.946-1.408c.696-.282 1.53-.563 2.364-.563s1.669.141 2.225.423c.695.282 1.252.704 1.669 1.268l3.476-2.958c-.417-.563-.973-1.127-1.529-1.408-.557-.423-1.113-.704-1.808-.986-.556-.282-1.252-.423-1.947-.563s-1.251-.141-1.808-.141c-1.668 0-3.198.282-4.449.704a12.06 12.06 0 0 0-3.616 2.254c-.973.986-1.807 2.113-2.364 3.521s-.834 2.958-.834 4.507c0 1.69.278 3.239.834 4.507.557 1.409 1.391 2.535 2.364 3.521s2.225 1.69 3.616 2.253 2.92.704 4.449.704c1.391 0 2.782-.282 4.172-.845s2.503-1.549 3.337-2.817l-3.893-2.817c-.139.845-.834 1.409-1.53 1.831zm32.539-10.563c-.973 0-1.808.282-2.503.704s-1.251 1.127-1.668 1.972V8.028h-4.45v14.366h4.45v-6.056c0-.704 0-1.408.139-1.972s.278-1.127.556-1.549.695-.704 1.112-.986c.556-.282 1.113-.422 1.947-.422.278 0 .695 0 .973.141.279 0 .557.141.974.282V7.746c-.278 0-.417-.141-.695-.141h-.835zm16.965 7.606c0-1.127-.139-2.254-.417-3.099a5.18 5.18 0 0 0-1.391-2.394c-.556-.704-1.251-1.127-2.086-1.549s-1.807-.563-2.781-.563c-1.112 0-2.086.141-3.059.563s-1.808.845-2.503 1.549-1.251 1.408-1.669 2.394-.556 1.972-.556 3.099.139 2.254.556 3.099c.418.986.974 1.69 1.669 2.394s1.53 1.127 2.503 1.549 1.947.563 3.059.563 2.225-.282 3.338-.704A8.03 8.03 0 0 0 208.166 20l-3.06-2.253c-.417.563-.834.986-1.251 1.268-.556.282-1.112.422-1.808.422-.834 0-1.529-.282-2.225-.704-.556-.563-.973-1.127-1.112-1.972H209v-1.549h-.139zm-10.29-1.549c0-.423.139-.704.278-.986s.417-.563.556-.845c.278-.282.556-.422.974-.563s.834-.282 1.251-.282c.834 0 1.53.282 1.947.845.556.563.695 1.127.695 1.972h-5.701v-.141z" fill="#fff"></path></svg></div></a><!--]--><!--]--></div><div class="apl-container apl-container--no-padding apl-platform-header__navigation-mobile-container" data-v-2169278d><!--[--><div data-v-2169278d><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--icon-only apl-platform-header__button apl-platform-header__button--icon" aria-label="Log in by institution"><!----><!--[--><!----><!--]--><div class="apl-icon apl-icon--institution apl-button__icon--center" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!----></button></div><div class="apl-search apl-search--collapse apl-search--md apl-platform-header__search" data-v-811ce219 data-v-2169278d><input class="apl-search__input" type="search" placeholder="Search..." autocomplete="off" value data-v-811ce219><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--md apl-button--icon-only apl-search__btn-search apl-search__btn-search--mobile" aria-label="Search" data-v-3dc43ba0 data-v-811ce219><!----><!--[--><!----><!--]--><div class="apl-icon apl-icon--search apl-button__icon--center" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!----></button><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--md apl-search__btn-search apl-search__btn-search--desktop" aria-label="Search" data-v-28a98aec data-v-811ce219><div class="apl-icon apl-icon--search apl-button__icon--left" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!--[--><span>Search</span><!--]--><!----><!----></button><!----></div><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--icon-only apl-platform-header__button apl-platform-header__button--icon apl-menu__options--mobile" aria-label="Expand menu options" aria-expanded="false" id="apl-menu__mobile-menu-toggle" data-v-b4a8da84 data-v-2169278d><!----><!--[--><!----><!--]--><div class="apl-icon apl-icon--menu apl-button__icon--center" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!----></button><!--]--></div><!--]--></div><div class="apl-container apl-platform-header__container apl-platform-header__container--desktop" data-v-92c77fd6 data-v-acbd3a3e><!--[--><div class="apl-container apl-container--no-padding apl-platform-header__logo-and-menu-container" data-v-92c77fd6><!--[--><div class="apl-platform-header__branding" data-v-92c77fd6><!--[--><!--[--><a href="/core/" class="platform-logo" title="Cambridge Core homepage"><div><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 209 20" fill="none" role="img" width="140" height="30"><title> Logo for Cambridge Core from Cambridge University Press. Click to return to homepage. </title><path d="M173.541 7.465c-4.172 0-7.648 3.521-7.648 7.747s3.476 7.746 7.648 7.746 7.648-3.521 7.648-7.746-3.337-7.747-7.648-7.747zm0 11.408c-2.086 0-3.616-1.69-3.616-3.662s1.669-3.662 3.616-3.662c2.086 0 3.615 1.69 3.615 3.662.139 2.113-1.529 3.662-3.615 3.662zm-159.357.704c-.973.563-1.947.704-3.198.704a8.31 8.31 0 0 1-3.337-.704c-.973-.423-1.808-.986-2.503-1.831-.695-.704-1.112-1.69-1.53-2.676s-.556-1.972-.556-3.099.139-2.253.556-3.38.834-1.972 1.53-2.676 1.53-1.408 2.503-1.831 2.086-.704 3.198-.704c.973 0 1.947.141 2.781.563.973.423 1.669.986 2.225 1.831l2.364-1.831c-.834-1.127-1.947-1.972-3.198-2.394-1.391-.563-2.642-.845-4.033-.845-1.669 0-3.059.282-4.45.845S4.033 2.817 3.059 3.944C2.086 4.93 1.391 6.197.834 7.465.278 8.873 0 10.282 0 11.972c0 1.268.278 2.676.695 3.944.556 1.268 1.251 2.394 2.086 3.521.973.986 2.086 1.831 3.337 2.535 1.391.563 2.781.986 4.45.986 1.252 0 2.225-.141 3.198-.422s1.669-.704 2.364-1.127 1.112-.845 1.53-1.268.695-.704.834-.986l-2.364-1.831c-.278.986-.973 1.69-1.947 2.254zm18.773.282v-7.183c0-.563-.139-1.268-.278-1.831-.278-.563-.556-1.127-1.112-1.549-.417-.423-1.112-.845-1.808-.986-.695-.282-1.669-.423-2.642-.423s-1.947.141-2.92.563c-.973.282-1.808.845-2.642 1.549l1.53 1.69c.278-.141.417-.422.695-.563s.556-.423.834-.563.695-.282 1.112-.423S26.56 10 27.116 10s1.112.141 1.53.282.695.422.973.704.417.563.556.986.139.704.139 1.127v.563h-1.947c-1.113 0-2.086.141-3.059.282s-1.808.422-2.503.845-1.251.845-1.669 1.549c-.417.563-.556 1.409-.556 2.254 0 .704.139 1.268.417 1.831s.556.986 1.113 1.268c.417.423.973.563 1.53.845.556.141 1.252.282 1.808.282 1.112 0 1.947-.141 2.781-.563s1.53-.986 2.086-1.972c0 .704 0 1.408.139 2.113h2.364c0-.423 0-.704-.139-1.127.417-.423.278-.845.278-1.408zm-2.364-3.239c0 .423 0 .986-.139 1.408s-.417.986-.695 1.268c-.278.422-.695.704-1.251.986s-1.112.422-1.808.422c-.973 0-1.669-.141-2.225-.563s-.834-.986-.834-1.69c0-.563.139-.986.556-1.408.278-.423.834-.563 1.391-.845.556-.141 1.112-.282 1.808-.423.695 0 1.252-.141 1.947-.141h1.391v.986h-.139zM56.039 10c-.417-.563-.973-.986-1.669-1.408s-1.53-.563-2.642-.563c-.834 0-1.669.282-2.503.704s-1.53 1.127-1.947 1.972c-.417-.845-.973-1.549-1.808-1.972-.695-.423-1.53-.704-2.642-.704-.695 0-1.251.141-1.669.282-.556.141-.973.423-1.391.704s-.695.563-.973.845-.417.563-.417.704V8.31h-2.503v14.085h2.642V15.07c0-.845.139-1.549.278-2.113s.417-1.127.834-1.408c.278-.423.695-.704 1.113-.845s.834-.282 1.39-.282c.695 0 1.113.141 1.53.282.417.282.695.563.834.986s.278.845.417 1.409c0 .563.139 1.127.139 1.831v7.324h2.642v-7.746c0-1.268.278-2.253.834-3.098s1.53-1.127 2.781-1.127c.695 0 1.112.141 1.53.282.417.282.695.563.973.845.278.422.417.845.417 1.268 0 .563.139.986.139 1.549v7.887h2.642v-8.169c0-.845-.139-1.549-.278-2.254 0-.422-.278-1.127-.695-1.69zm16.826.141c-.556-.704-1.391-1.127-2.225-1.549s-1.808-.563-2.92-.563c-.973 0-1.947.282-2.781.704s-1.53.986-2.086 1.69V0h-2.642v22.394h2.642v-1.972a6.17 6.17 0 0 0 2.086 1.69c.834.422 1.808.563 2.781.563 1.112 0 2.086-.141 2.92-.563s1.669-.845 2.225-1.549 1.112-1.408 1.391-2.394c.278-.845.556-1.831.556-2.958 0-.986-.139-1.972-.556-2.958-.278-.704-.695-1.408-1.391-2.113zm-1.252 7.183c-.278.563-.556 1.127-.973 1.549s-.834.704-1.391.986-1.251.422-1.947.422-1.391-.141-1.947-.422-1.112-.563-1.391-.986c-.417-.422-.695-.986-.973-1.549s-.278-1.268-.278-1.972.139-1.408.278-1.972c.278-.563.556-1.127.973-1.549s.834-.704 1.391-.986 1.252-.423 1.947-.423 1.391.141 1.947.423 1.112.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c.139.704 0 1.409-.278 1.972zM82.46 8.169c-.417.141-.834.282-1.112.563-.417.282-.695.423-.973.845-.278.282-.556.704-.695.986V8.451h-2.642v14.085h2.642v-7.042c0-1.549.278-2.676.973-3.38s1.669-1.127 2.92-1.127h.695c.278 0 .417.141.695.141l.139-2.817c-.417-.141-.695-.141-1.112-.141a2.82 2.82 0 0 0-1.53 0zm5.145 14.225h2.642V8.31h-2.642v14.085zm1.252-20.986a1.77 1.77 0 0 0-1.391.563c-.278.423-.556.845-.556 1.408s.139.986.556 1.408a1.77 1.77 0 0 0 1.391.563 1.77 1.77 0 0 0 1.39-.563c.417-.423.556-.845.556-1.408s-.139-.986-.556-1.408a1.77 1.77 0 0 0-1.39-.563zm15.574 9.014a6.17 6.17 0 0 0-2.085-1.69c-.835-.423-1.808-.704-2.781-.704-1.112 0-2.086.141-2.92.563s-1.669.845-2.225 1.549-1.112 1.409-1.391 2.394c-.278.845-.556 1.831-.556 2.958 0 .986.139 1.972.556 2.958.278.845.834 1.69 1.391 2.394s1.391 1.268 2.225 1.549c.834.422 1.808.563 2.92.563.974 0 1.807-.141 2.781-.563a6.17 6.17 0 0 0 2.085-1.69v1.972h2.643V0h-2.643v10.423zm-.417 6.901c-.278.563-.556 1.127-.973 1.549s-.834.704-1.391.986-1.251.422-1.946.422-1.391-.141-1.947-.422-1.112-.563-1.39-.986c-.417-.422-.695-.986-.973-1.549s-.278-1.268-.278-1.972.139-1.408.278-1.972c.278-.563.556-1.127.973-1.549s.834-.704 1.39-.986 1.252-.423 1.947-.423 1.39.141 1.946.423 1.113.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c.139.704 0 1.409-.278 1.972zm17.104-6.901c-.556-.704-1.251-1.268-2.225-1.69-.834-.423-1.808-.563-2.781-.563-1.112 0-2.086.141-2.92.563s-1.669.845-2.225 1.549-1.112 1.408-1.39 2.394c-.279.845-.557 1.831-.557 2.958 0 .986.139 1.972.557 2.817s.834 1.69 1.529 2.253c.695.704 1.391 1.127 2.225 1.549s1.808.563 2.781.563 1.947-.141 2.781-.563a5.21 5.21 0 0 0 2.086-1.831h.139v2.113c0 .563-.139 1.127-.278 1.831-.139.563-.417 1.127-.834 1.549s-.835.845-1.53 1.127-1.53.422-2.503.422-1.947-.141-2.781-.563-1.53-.986-2.225-1.69l-1.808 2.253c.974.986 2.086 1.549 3.199 1.972s2.364.563 3.615.563c1.112 0 2.225-.141 3.059-.563.974-.422 1.808-.845 2.364-1.549.695-.704 1.252-1.408 1.53-2.253.417-.845.556-1.831.556-2.958V8.732h-2.642v1.69h.278zm-1.251 8.451c-.835.845-1.947 1.268-3.338 1.268s-2.503-.422-3.337-1.268-1.252-1.972-1.252-3.38c0-.704.139-1.408.279-1.972.278-.563.556-1.127.973-1.549s.834-.704 1.39-.986 1.252-.423 1.947-.423 1.391.141 1.947.423 1.112.563 1.391.986c.417.423.695.986.973 1.549s.278 1.268.278 1.972c0 1.408-.417 2.535-1.251 3.38zm18.633-8.451c-.556-.704-1.251-1.268-2.086-1.69s-1.947-.704-3.198-.704c-1.113 0-1.947.141-2.92.563-.835.423-1.669.845-2.225 1.549-.695.704-1.113 1.409-1.53 2.394-.417.845-.556 1.831-.556 2.958 0 .986.139 1.972.556 2.958.278.845.835 1.69 1.391 2.394s1.39 1.127 2.225 1.549 1.946.563 3.059.563c2.642 0 4.728-.986 6.257-2.817l-2.085-1.831c-.557.704-1.113 1.127-1.669 1.549s-1.391.563-2.086.563c-.556 0-1.251-.141-1.808-.282s-1.112-.422-1.529-.845-.835-.845-1.113-1.268a3.79 3.79 0 0 1-.417-1.69h11.264v-.845c0-.845-.139-1.831-.418-2.676-.139-.986-.556-1.69-1.112-2.394zm-9.734 3.803c0-.423.139-.986.278-1.408s.556-.986.835-1.268c.417-.423.834-.704 1.39-.986a3.81 3.81 0 0 1 1.808-.423c.695 0 1.251.141 1.808.282s.973.563 1.251.845c.278.422.556.845.695 1.268.139.563.278.986.278 1.69h-8.343zm30.592 3.944c-.695.422-1.529.563-2.503.563-.834 0-1.529-.141-2.364-.563-.695-.282-1.39-.845-1.807-1.408-.557-.563-.974-1.268-1.252-2.113a8.3 8.3 0 0 1-.417-2.676 8.3 8.3 0 0 1 .417-2.676c.278-.845.695-1.549 1.252-2.113s1.112-1.127 1.946-1.408c.696-.282 1.53-.563 2.364-.563s1.669.141 2.225.423c.695.282 1.252.704 1.669 1.268l3.476-2.958c-.417-.563-.973-1.127-1.529-1.408-.557-.423-1.113-.704-1.808-.986-.556-.282-1.252-.423-1.947-.563s-1.251-.141-1.808-.141c-1.668 0-3.198.282-4.449.704a12.06 12.06 0 0 0-3.616 2.254c-.973.986-1.807 2.113-2.364 3.521s-.834 2.958-.834 4.507c0 1.69.278 3.239.834 4.507.557 1.409 1.391 2.535 2.364 3.521s2.225 1.69 3.616 2.253 2.92.704 4.449.704c1.391 0 2.782-.282 4.172-.845s2.503-1.549 3.337-2.817l-3.893-2.817c-.139.845-.834 1.409-1.53 1.831zm32.539-10.563c-.973 0-1.808.282-2.503.704s-1.251 1.127-1.668 1.972V8.028h-4.45v14.366h4.45v-6.056c0-.704 0-1.408.139-1.972s.278-1.127.556-1.549.695-.704 1.112-.986c.556-.282 1.113-.422 1.947-.422.278 0 .695 0 .973.141.279 0 .557.141.974.282V7.746c-.278 0-.417-.141-.695-.141h-.835zm16.965 7.606c0-1.127-.139-2.254-.417-3.099a5.18 5.18 0 0 0-1.391-2.394c-.556-.704-1.251-1.127-2.086-1.549s-1.807-.563-2.781-.563c-1.112 0-2.086.141-3.059.563s-1.808.845-2.503 1.549-1.251 1.408-1.669 2.394-.556 1.972-.556 3.099.139 2.254.556 3.099c.418.986.974 1.69 1.669 2.394s1.53 1.127 2.503 1.549 1.947.563 3.059.563 2.225-.282 3.338-.704A8.03 8.03 0 0 0 208.166 20l-3.06-2.253c-.417.563-.834.986-1.251 1.268-.556.282-1.112.422-1.808.422-.834 0-1.529-.282-2.225-.704-.556-.563-.973-1.127-1.112-1.972H209v-1.549h-.139zm-10.29-1.549c0-.423.139-.704.278-.986s.417-.563.556-.845c.278-.282.556-.422.974-.563s.834-.282 1.251-.282c.834 0 1.53.282 1.947.845.556.563.695 1.127.695 1.972h-5.701v-.141z" fill="#fff"></path></svg></div></a><!--]--><!--]--></div><hr class="apl-platform-header__logo-and-menu-divider" data-v-92c77fd6><nav data-v-92c77fd6><div class="apl-container apl-container--no-padding apl-menu__options--desktop" data-v-1e1e1426 data-v-92c77fd6><!--[--><ul class="apl-menu__list" data-v-1e1e1426><!--[--><li id="menu_item-0" class="apl-menu__list-item" data-v-1e1e1426><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--text apl-platform-header__button apl-menu__item"><!----><!--[--><span>Browse</span><!--]--><!----><div class="apl-icon apl-icon--chevron-down apl-button__icon--right" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div></button></li><li id="menu_item-1" class="apl-menu__list-item" data-v-1e1e1426><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--text apl-platform-header__button apl-menu__item"><!----><!--[--><span>Services</span><!--]--><!----><div class="apl-icon apl-icon--chevron-down apl-button__icon--right" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div></button></li><li id="menu_item-2" class="apl-menu__list-item" data-v-1e1e1426><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--text apl-platform-header__button apl-menu__item"><!----><!--[--><span>Open research</span><!--]--><!----><div class="apl-icon apl-icon--chevron-down apl-button__icon--right" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div></button></li><!--]--></ul><!--]--></div></nav><!--]--></div><div class="apl-container apl-container--no-padding apl-platform-header__account-and-search-container" data-v-92c77fd6><!--[--><div class="apl-platform-header__authenticated-by" data-v-92c77fd6><button tabindex="0" type="button" class="apl-button apl-button--secondary apl-button--md apl-button--text apl-platform-header__button"><!----><!--[--><span>Institution Login</span><!--]--><!----><!----></button></div><div class="apl-search apl-search--collapse apl-search--md apl-platform-header__search" data-v-811ce219 data-v-92c77fd6><input class="apl-search__input" type="search" placeholder="Search..." autocomplete="off" value data-v-811ce219><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--md apl-button--icon-only apl-search__btn-search apl-search__btn-search--mobile" aria-label="Search" data-v-3dc43ba0 data-v-811ce219><!----><!--[--><!----><!--]--><div class="apl-icon apl-icon--search apl-button__icon--center" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!----></button><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--md apl-search__btn-search apl-search__btn-search--desktop" aria-label="Search" data-v-28a98aec data-v-811ce219><div class="apl-icon apl-icon--search apl-button__icon--left" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!--[--><span>Search</span><!--]--><!----><!----></button><!----></div><!--]--></div><!--]--></div><!--]--></div><!----><!----></div><!----></section></div><noscript><div><div><p>Menu links</p><ol><!--[--><!--[--><li>Browse</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Subjects</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li> Subjects (A-D)</li><ol><!--[--><li><a href="/core/browse-subjects/anthropology" target="">Anthropology</a></li><li><a href="/core/browse-subjects/archaeology" target="">Archaeology</a></li><li><a href="/core/browse-subjects/area-studies" target="">Area Studies</a></li><li><a href="/core/browse-subjects/art" target="">Art</a></li><li><a href="/core/browse-subjects/chemistry" target="">Chemistry</a></li><li><a href="/core/browse-subjects/classical-studies" target="">Classical Studies</a></li><li><a href="/core/browse-subjects/computer-science" target="">Computer Science</a></li><li><a href="/core/browse-subjects/drama-and-theatre" target="">Drama, Theatre, Performance Studies</a></li><!--]--></ol><!--]--><!--[--><li> Subjects (E-K)</li><ol><!--[--><li><a href="/core/browse-subjects/earth-and-environmental-sciences" target="">Earth and Environmental Science</a></li><li><a href="/core/browse-subjects/economics" target="">Economics</a></li><li><a href="/core/browse-subjects/education" target="">Education</a></li><li><a href="/core/browse-subjects/engineering" target="">Engineering</a></li><li><a href="/core/browse-subjects/english-language-teaching-resources-for-teachers" target="">English Language Teaching – Resources for Teachers</a></li><li><a href="/core/browse-subjects/film-media-mass-ommunication" target="">Film, Media, Mass Communication</a></li><li><a href="/core/browse-subjects/general-science" target="">General Science</a></li><li><a href="/core/browse-subjects/geography" target="">Geography</a></li><li><a href="/core/browse-subjects/history" target="">History</a></li><!--]--></ol><!--]--><!--[--><li> Subjects (L-O)</li><ol><!--[--><li><a href="/core/browse-subjects/language-and-linguistics" target="">Language and Linguistics</a></li><li><a href="/core/browse-subjects/law" target="">Law</a></li><li><a href="/core/browse-subjects/life-sciences" target="">Life Sciences</a></li><li><a href="/core/browse-subjects/literature" target="">Literature</a></li><li><a href="/core/browse-subjects/management" target="">Management</a></li><li><a href="/core/browse-subjects/materials-science" target="">Materials Science</a></li><li><a href="/core/browse-subjects/mathematics" target="">Mathematics</a></li><li><a href="/core/browse-subjects/medicine" target="">Medicine</a></li><li><a href="/core/browse-subjects/music" target="">Music</a></li><li><a href="/core/browse-subjects/nutrition" target="">Nutrition</a></li><!--]--></ol><!--]--><!--[--><li> Subjects (P-Z)</li><ol><!--[--><li><a href="/core/browse-subjects/philosophy" target="">Philosophy</a></li><li><a href="/core/browse-subjects/physics" target="">Physics and Astronomy</a></li><li><a href="/core/browse-subjects/politics-and-international-relations" target="">Politics and International Relations</a></li><li><a href="/core/browse-subjects/psychiatry" target="">Psychiatry</a></li><li><a href="/core/browse-subjects/psychology" target="">Psychology</a></li><li><a href="/core/browse-subjects/religion" target="">Religion</a></li><li><a href="/core/browse-subjects/social-science-research-methods" target="">Social Science Research Methods</a></li><li><a href="/core/browse-subjects/sociology" target="">Sociology</a></li><li><a href="/core/browse-subjects/statistics-and-probability" target="">Statistics and Probability</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Open access</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>All open access publishing</li><ol><!--[--><li><a href="/core/publications/open-access" target="">Open access</a></li><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Open access journals</a></li><li><a href="/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Research open journals</a></li><li><a href="/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Journals containing open access</a></li><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE" target="">Open access articles</a></li><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&amp;sort=canonical.date:desc" target="">Open access books</a></li><li><a href="/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&amp;aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&amp;searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493" target="">Open access Elements</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Journals</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Explore</li><ol><!--[--><li><a href="/core/publications/journals" target="">All journal subjects</a></li><li><a href="/core/publications/journals" target="">Search journals</a></li><!--]--></ol><!--]--><!--[--><li>Open access</li><ol><!--[--><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Open access journals</a></li><li><a href="/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Research open journals</a></li><li><a href="/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&amp;statuses=PUBLISHED&amp;sort=titleSort:asc" target="">Journals containing open access</a></li><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE" target="">Open access articles</a></li><!--]--></ol><!--]--><!--[--><li>Collections</li><ol><!--[--><li><a href="/core/publications/collections/cambridge-forum" target="">Cambridge Forum</a></li><li><a href="/core/publications/collections/cambridge-law-reports-collection" target="">Cambridge Law Reports Collection</a></li><li><a href="/core/publications/collections/cambridge-prisms" target="">Cambridge Prisms</a></li><li><a href="/core/publications/collections/research-directions" target="">Research Directions</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Books</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Explore</li><ol><!--[--><li><a href="/core/publications/books" target="">Books</a></li><li><a href="/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&amp;sort=canonical.date:desc" target="">Open access books</a></li><li><a href="/core/publications/books/listing?aggs[productDate][filters]=Last+3+months&amp;aggs[productTypes][filters]=BOOK&amp;sort=canonical.date:desc" target="">New books</a></li><li><a href="/core/publications/collections/flip-it-open" target="">Flip it Open</a></li><!--]--></ol><!--]--><!--[--><li>Collections</li><ol><!--[--><li><a href="/core/publications/collections/cambridge-companions" target="">Cambridge Companions</a></li><li><a href="/core/publications/collections/cambridge-editions" target="">Cambridge Editions</a></li><li><a href="/core/publications/collections/cambridge-histories" target="">Cambridge Histories</a></li><li><a href="/core/publications/collections/cambridge-library-collection" target="">Cambridge Library Collection</a></li><li><a href="/core/publications/collections/cambridge-shakespeare" target="">Cambridge Shakespeare</a></li><li><a href="/core/publications/collections/cambridgehandbooks" target="">Cambridge Handbooks</a></li><!--]--></ol><!--]--><!--[--><li> Collections (cont.)</li><ol><!--[--><li><a href="/core/publications/collections/dispute-settlement-reports-online" target="">Dispute Settlement Reports Online</a></li><li><a href="/core/publications/collections/flip-it-open" target="">Flip it Open</a></li><li><a href="/core/publications/collections/hemingway-letters" target="">Hemingway Letters</a></li><li><a href="/core/publications/collections/shakespeare-survey" target="">Shakespeare Survey</a></li><li><a href="/core/publications/collections/stahl-online" target="">Stahl Online</a></li><li><a href="/core/publications/collections/the-correspondence-of-isaac-newton" target="">The Correspondence of Isaac Newton</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Elements</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Explore</li><ol><!--[--><li><a href="/core/publications/elements" target="">About Elements</a></li><li><a href="/core/publications/elements/cambridge-elements-series" target="">Elements series</a></li><li><a href="/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&amp;aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&amp;searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493" target="">Open access Elements</a></li><li><a href="/core/publications/elements/published-elements?aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&amp;aggs%5BproductDate%5D%5Bfilters%5D=Last%203%20months&amp;searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493" target="">New Elements</a></li><!--]--></ol><!--]--><!--[--><li>Subjects (A-E)</li><ol><!--[--><li><a href="/core/elements/subject/Anthropology/2E44A5AF2838E017617A26DD79FAEAEE" target="">Anthropology</a></li><li><a href="/core/elements/subject/Archaeology/63A50B5368A9F97F8AA2D6AB965B5F4C" target="">Archaeology</a></li><li><a href="/core/elements/subject/Classical%20Studies/DDC63B7F5792FE2A95D1FB15F76E3F42" target="">Classical Studies</a></li><li><a href="/core/elements/subject/Computer%20Science/A57E10708F64FB69CE78C81A5C2A6555" target="">Computer Science</a></li><li><a href="/core/elements/subject/Drama,%20Theatre,%20Performance%20Studies/2825E4E39F2D641B36543EE80FB1DEA3" target="">Drama, Theatre, Performance Studies</a></li><li><a href="/core/elements/subject/Earth%20and%20Environmental%20Sciences/F470FBF5683D93478C7CAE5A30EF9AE8" target="">Earth and Environmental Sciences</a></li><li><a href="/core/elements/subject/Economics/FA44491F1F55F917C43E9832715B9DE7" target="">Economics</a></li><li><a href="/core/elements/subject/Education/550D00F8DF590F2598CF7CC0038E24D1" target="">Education</a></li><li><a href="/core/elements/subject/Engineering/CCC62FE56DCC1D050CA1340C1CCF46F5" target="">Engineering</a></li><!--]--></ol><!--]--><!--[--><li> Subjects (F-O)</li><ol><!--[--><li><a href="/core/elements/subject/Film,%20Media,%20Mass%20Communication/4B91F10E834814A90CE718E7831E492F" target="">Film, Media, Mass Communication</a></li><li><a href="/core/elements/subject/History/66BE42A30172E280FDE64F8EE2F485B0" target="">History</a></li><li><a href="/core/elements/subject/Language%20and%20Linguistics/140D314098408C26BDF3009F7FF858E9" target="">Language and Linguistics</a></li><li><a href="/core/elements/subject/Law/7C9FB6788DD8D7E6696263BC774F4D5B" target="">Law</a></li><li><a href="/core/elements/subject/Life%20Sciences/E044EF2F61B601378786E9EDA901B2D5" target="">Life Sciences</a></li><li><a href="/core/elements/subject/Literature/F2434ADC122145767C6C3B988A8E9BD5" target="">Literature</a></li><li><a href="/core/elements/subject/Management/0EDCC0540639B06A5669BDEEF50C4CBE" target="">Management</a></li><li><a href="/core/elements/subject/Mathematics/FA1467C44B5BD46BB8AA6E58C2252153" target="">Mathematics</a></li><li><a href="/core/elements/subject/Medicine/66FF02B2A4F83D9A645001545197F287" target="">Medicine</a></li><li><a href="/core/elements/subject/Music/A370B5604591CB3C7F9AFD892DDF7BD1" target="">Music</a></li><!--]--></ol><!--]--><!--[--><li> Subjects (P-Z)</li><ol><!--[--><li><a href="/core/elements/subject/Philosophy/2D1AC3C0E174F1F1A93F8C7DE19E0FAB" target="">Philosophy</a></li><li><a href="/core/elements/subject/Physics%20and%20Astronomy/DBFB610E9FC5E012C011430C0573CC06" target="">Physics and Astronomy</a></li><li><a href="/core/elements/subject/Politics%20and%20International%20Relations/3BF83347E5E456DAC34F3FABFC8BBF4E" target="">Politics and International Relations</a></li><li><a href="/core/elements/subject/Psychology/21B42A72BA3E4CB0E3315E5B1B71B07F" target="">Psychology</a></li><li><a href="/core/elements/subject/Religion/53E51D24FB488962B9364A2C4B45D1C3" target="">Religion</a></li><li><a href="/core/elements/subject/Sociology/0E2CD53A93003DF17E52D753F6E90683" target="">Sociology</a></li><li><a href="/core/elements/subject/Statistics%20and%20Probability/3150B8B0D1B0B4E8DC17EC9EDFD9CA26" target="">Statistics and Probability</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Textbooks</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Explore</li><ol><!--[--><li><a href="/highereducation/" target="">Cambridge Higher Education</a></li><li><a href="/highereducation/services/librarians/title-list" target="">Title list</a></li><li><a href="/highereducation/search?sortBy=publication_date&amp;aggs=%24productDate%24Last%25206%2520months%3Atrue%26Last%252012%2520months%3Atrue%26Last%25203%2520years%3Atrue%26Over%25203%2520years%3Atrue%3B%3B&amp;event=SE-AU_PREF" target="">New titles</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Collections</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Book collections</li><ol><!--[--><li><a href="/core/publications/collections/cambridge-companions" target="">Cambridge Companions</a></li><li><a href="/core/publications/collections/cambridge-editions" target="">Cambridge Editions</a></li><li><a href="/core/publications/collections/cambridge-histories" target="">Cambridge Histories</a></li><li><a href="/core/publications/collections/cambridge-library-collection" target="">Cambridge Library Collection</a></li><li><a href="/core/publications/collections/cambridge-shakespeare" target="">Cambridge Shakespeare</a></li><li><a href="/core/publications/collections/cambridgehandbooks" target="">Cambridge Handbooks</a></li><!--]--></ol><!--]--><!--[--><li> Book collections (cont.)</li><ol><!--[--><li><a href="/core/publications/collections/dispute-settlement-reports-online" target="">Dispute Settlement Reports Online</a></li><li><a href="/core/publications/collections/flip-it-open" target="">Flip it Open</a></li><li><a href="/core/publications/collections/hemingway-letters" target="">Hemingway Letters</a></li><li><a href="/core/publications/collections/shakespeare-survey" target="">Shakespeare Survey</a></li><li><a href="/core/publications/collections/stahl-online" target="">Stahl Online</a></li><li><a href="/core/publications/collections/the-correspondence-of-isaac-newton" target="">The Correspondence of Isaac Newton</a></li><!--]--></ol><!--]--><!--[--><li>Journal collections</li><ol><!--[--><li><a href="/core/publications/collections/cambridge-forum" target="">Cambridge Forum</a></li><li><a href="/core/publications/collections/cambridge-law-reports-collection" target="">Cambridge Law Reports Collection</a></li><li><a href="/core/publications/collections/cambridge-prisms" target="">Cambridge Prisms</a></li><li><a href="/core/publications/collections/research-directions" target="">Research Directions</a></li><!--]--></ol><!--]--><!--[--><li>Series</li><ol><!--[--><li><a href="/core/publications/collections/series" target="">All series</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Partners</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Partners</li><ol><!--[--><li><a href="/core/publications/publishing-partners/agenda-publishing" target="">Agenda Publishing</a></li><li><a href="/core/publications/publishing-partners/amsterdam-university-press" target="">Amsterdam University Press</a></li><li><a href="/core/publications/publishing-partners/anthem-press" target="">Anthem Press</a></li><li><a href="/core/publications/publishing-partners/boydell-brewer" target="">Boydell &amp; Brewer</a></li><li><a href="/core/publications/publishing-partners/bristol-university-press" target="">Bristol University Press</a></li><li><a href="/core/publications/publishing-partners/edinburgh-university-press" target="">Edinburgh University Press</a></li><li><a href="/core/publications/publishing-partners/emirates-center" target="">Emirates Center for Strategic Studies and Research</a></li><li><a href="/core/publications/publishing-partners/facet-publishing" target="">Facet Publishing</a></li><!--]--></ol><!--]--><!--[--><li> Partners (cont.)</li><ol><!--[--><li><a href="/core/publications/publishing-partners/foundation-books" target="">Foundation Books</a></li><li><a href="/core/publications/publishing-partners/intersentia" target="">Intersentia</a></li><li><a href="/core/publications/publishing-partners/iseas" target="">ISEAS-Yusof Ishak Institute</a></li><li><a href="/core/publications/publishing-partners/jagiellonian-university-press" target="">Jagiellonian University Press</a></li><li><a href="/core/publications/publishing-partners/royal-economic-society" target="">Royal Economic Society</a></li><li><a href="/core/publications/publishing-partners/unisa-press" target="">Unisa Press</a></li><li><a href="/core/publications/publishing-partners/university-adelaide-press" target="">The University of Adelaide Press</a></li><li><a href="/core/publications/publishing-partners/wits-university-press" target="">Wits University Press</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Services</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>About</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>About Cambridge Core</li><ol><!--[--><li><a href="/core/services/about/about" target="">About</a></li><li><a href="/core/services/about/accessibility" target="">Accessibility</a></li><li><a href="/core/services/about/crossmark-policy" target="">CrossMark policy</a></li><li><a href="/core/services/about/ethical-standards" target="">Ethical Standards</a></li><!--]--></ol><!--]--><!--[--><li>Environment and sustainability</li><ol><!--[--><li><a href="/core/services/about/environment-and-sustainability" target="">Environment and sustainability</a></li><li><a href="/core/services/about/reducing-print" target="">Reducing print</a></li><li><a href="/core/services/about/journals-moving-to-online-only" target="">Journals moving to online only</a></li><!--]--></ol><!--]--><!--[--><li>Guides</li><ol><!--[--><li><a href="/core/services/about/user-guides" target="">User guides</a></li><li><a href="/core/services/about/user-guides-and-videos" target="">User Guides and Videos</a></li><li><a href="/core/services/about/support-videos" target="">Support Videos</a></li><li><a href="/core/services/about/training" target="">Training</a></li><!--]--></ol><!--]--><!--[--><li>Help</li><ol><!--[--><li><a href="https://corehelp.cambridge.org/" target="">Cambridge Core help</a></li><li><a href="https://corehelp.cambridge.org/hc/en-gb/p/contact-information" target="">Contact us</a></li><li><a href="https://corehelp.cambridge.org/hc/en-gb/requests/new" target="">Technical support</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Agents</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Services for agents</li><ol><!--[--><li><a href="/core/services/agents/services-for-agents" target="">Services for agents</a></li><li><a href="/core/services/agents/journals-for-agents" target="">Journals for agents</a></li><li><a href="/core/services/agents/books-for-agents" target="">Books for agents</a></li><li><a href="/core/services/agents/price-list" target="">Price list</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Authors</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Journals</li><ol><!--[--><li><a href="/core/services/authors/journals" target="">Journals</a></li><li><a href="/core/services/authors/journal-publishing-statistics" target="">Journal publishing statistics</a></li><li><a href="/core/services/authors/corresponding-author" target="">Corresponding author</a></li><li><a href="/core/services/authors/seeking-permission-to-use-copyrighted-material" target="">Seeking permission to use copyrighted material</a></li><li><a href="/core/services/authors/publishing-supplementary-material" target="">Publishing supplementary material</a></li><li><a href="/core/services/authors/writing-an-effective-abstract" target="">Writing an effective abstract</a></li><li><a href="/core/services/authors/journal-production-faqs" target="">Journal production - FAQs</a></li><!--]--></ol><!--]--><!--[--><li>Journals (cont.)</li><ol><!--[--><li><a href="/core/services/authors/author-affiliations" target="">Author affiliations</a></li><li><a href="/core/services/authors/co-reviewing-policy" target="">Co-reviewing policy</a></li><li><a href="/core/services/authors/digital-author-publishing-agreement-faqs" target="">Digital Author Publishing Agreement - FAQs</a></li><li><a href="/core/services/authors/anonymising-your-manuscript" target="">Anonymising your manuscript</a></li><li><a href="/core/services/authors/publishing-open-access" target="">Publishing open access</a></li><li><a href="/core/services/authors/converting-your-article-to-open-access" target="">Converting your article to open access</a></li><li><a href="/core/services/authors/publishing-open-access-webinars" target="">Publishing Open Access - webinars</a></li><!--]--></ol><!--]--><!--[--><li>Journals (cont.)</li><ol><!--[--><li><a href="/core/services/authors/preparing-and-submitting-your-paper" target="">Preparing and submitting your paper</a></li><li><a href="/core/services/authors/publishing-an-accepted-paper" target="">Publishing an accepted paper</a></li><li><a href="/core/services/authors/promoting-your-published-paper" target="">Promoting your published paper</a></li><li><a href="/core/services/authors/measuring-impact" target="">Measuring impact</a></li><li><a href="/core/services/authors/journals-artwork-guide" target="">Journals artwork guide</a></li><li><a href="/core/services/authors/using-orcid" target="">Using ORCID</a></li><!--]--></ol><!--]--><!--[--><li>Books</li><ol><!--[--><li><a href="/core/services/authors/books" target="">Books</a></li><li><a href="/core/services/authors/marketing-your-book" target="">Marketing your book</a></li><li><a href="/core/services/authors/elements-user-guides" target="">Author guides for Cambridge Elements</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Corporates</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Corporates</li><ol><!--[--><li><a href="/core/services/corporates/commercial-reprints" target="">Commercial reprints</a></li><li><a href="/core/services/corporates/advertising" target="">Advertising</a></li><li><a href="/core/services/corporates/sponsorship" target="">Sponsorship</a></li><li><a href="/core/services/corporates/book-special-sales" target="">Book special sales</a></li><li><a href="/core/services/corporates/contact-us" target="">Contact us</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Editors</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Information</li><ol><!--[--><li><a href="/core/services/editors/journal-development" target="">Journal development</a></li><li><a href="/core/services/editors/peer-review-for-editors" target="">Peer review for editors</a></li><li><a href="/core/services/editors/open-access-for-editors" target="">Open access for editors</a></li><li><a href="/core/services/editors/policies-and-guidelines" target="">Policies and guidelines</a></li><!--]--></ol><!--]--><!--[--><li>Resources</li><ol><!--[--><li><a href="/core/services/editors/the-editors-role" target="">The editor&#39;s role</a></li><li><a href="/core/services/editors/open-research-for-editors" target="">Open research for editors</a></li><li><a href="/core/services/editors/engagement-and-promotion" target="">Engagement and promotion</a></li><li><a href="/core/services/editors/blogging" target="">Blogging</a></li><li><a href="/core/services/editors/social-media" target="">Social media</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Librarians</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Information</li><ol><!--[--><li><a href="/core/services/librarians/open-access-for-librarians" target="">Open Access for Librarians</a></li><li><a href="https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements" target="">Transformative agreements</a></li><li><a href="/core/services/librarians/transformative-agreements-faqs" target="">Transformative Agreements - FAQs</a></li><li><a href="/core/services/librarians/evidence-based-acquisition" target="">Evidence based acquisition</a></li><li><a href="/core/services/librarians/ebook-news-and-updates" target="">ebook news &amp; updates</a></li><li><a href="/core/services/librarians/cambridge-libraries-of-the-world-podcast" target="">Cambridge libraries of the world podcast</a></li><li><a href="/core/services/librarians/purchasing-models" target="">Purchasing models</a></li><li><a href="/core/services/librarians/journals-publishing-updates" target="">Journals Publishing Updates</a></li><!--]--></ol><!--]--><!--[--><li>Products</li><ol><!--[--><li><a href="/core/services/librarians/cambridge-frontlist" target="">Cambridge frontlist</a></li><li><a href="/core/services/librarians/cambridge-journals-digital-archive" target="">Cambridge journals digital archive</a></li><li><a href="/core/services/librarians/hot-topics" target="">Hot topics</a></li><li><a href="/core/services/librarians/other-digital-products" target="">Other digital products</a></li><li><a href="/core/services/librarians/perpetual-access-products" target="">Perpetual access products</a></li><li><a href="/core/services/librarians/price-list" target="">Price list</a></li><li><a href="/core/services/librarians/developing-country-programme" target="">Developing country programme</a></li><li><a href="/core/services/librarians/new-content" target="">New content</a></li><!--]--></ol><!--]--><!--[--><li>Tools</li><ol><!--[--><li><a href="/core/eligibility-checker" target="">Eligibility checker</a></li><li><a href="https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements" target="">Transformative agreements</a></li><li><a href="https://www.cambridge.org/core/services/librarians/kbart" target="">KBART</a></li><li><a href="https://www.cambridge.org/core/services/librarians/marc-records" target="">MARC records</a></li><li><a href="/core/services/librarians/using-marcedit-for-marc-records" target="">Using MARCEdit for MARC records</a></li><li><a href="/core/services/librarians/inbound-openurl-specifications" target="">Inbound OpenURL specifications</a></li><li><a href="/core/services/librarians/counter-report-types" target="">COUNTER report types</a></li><!--]--></ol><!--]--><!--[--><li>Resources</li><ol><!--[--><li><a href="/core/services/librarians/catalogues-and-resources" target="">Catalogues and resources</a></li><li><a href="/core/services/librarians/making-the-most-of-your-eba" target="">Making the most of your EBA</a></li><li><a href="/core/services/librarians/posters" target="">Posters</a></li><li><a href="/core/services/librarians/leaflets-and-brochures" target="">Leaflets and brochures</a></li><li><a href="/core/services/librarians/additional-resources" target="">Additional resources</a></li><li><a href="/core/services/librarians/find-my-sales-contact" target="">Find my sales contact</a></li><li><a href="/core/services/librarians/webinars" target="">Webinars</a></li><li><a href="/core/services/librarians/read-and-publish-resources" target="">Read and publish resources</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Peer review</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Peer review</li><ol><!--[--><li><a href="/core/services/peer-review/how-to-peer-review-journal-articles" target="">How to peer review journal articles</a></li><li><a href="/core/services/peer-review/how-to-peer-review-book-proposals" target="">How to peer review book proposals</a></li><li><a href="/core/services/peer-review/how-to-peer-review-registered-reports" target="">How to peer review Registered Reports</a></li><li><a href="/core/services/peer-review/peer-review-faqs" target="">Peer review FAQs</a></li><li><a href="/core/services/peer-review/ethics-in-peer-review" target="">Ethics in peer review</a></li><li><a href="/core/services/peer-review/online-peer-review-systems" target="">Online peer review systems</a></li><li><a href="/core/services/peer-review/a-guide-to-publons" target="">A guide to Publons</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Publishing ethics</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Journals </li><ol><!--[--><li><a href="/core/services/publishing-ethics/publishing-ethics-guidelines-journals" target="">Publishing ethics guidelines for journals</a></li><li><a href="/core/services/publishing-ethics/core-editorial-policies-journals" target="">Core editorial policies for journals</a></li><li><a href="/core/services/publishing-ethics/authorship-and-contributorship-journals" target="">Authorship and contributorship for journals</a></li><li><a href="/core/services/publishing-ethics/affiliations-journals" target="">Affiliations for journals</a></li><li><a href="/core/services/publishing-ethics/research-ethics-journals" target="">Research ethics for journals</a></li><li><a href="/core/services/publishing-ethics/competing-interests-and-funding-journals" target="">Competing interests and funding for journals</a></li><!--]--></ol><!--]--><!--[--><li>Journals (cont.)</li><ol><!--[--><li><a href="/core/services/publishing-ethics/data-and-supporting-evidence-for-journals" target="">Data and supporting evidence for journals</a></li><li><a href="/core/services/publishing-ethics/misconduct-journals" target="">Misconduct for journals</a></li><li><a href="/core/services/publishing-ethics/corrections-retractions-and-removals-journals" target="">Corrections, retractions and removals for journals</a></li><li><a href="/core/services/publishing-ethics/versions-and-adaptations-journals" target="">Versions and adaptations for journals</a></li><li><a href="/core/services/publishing-ethics/libel-defamation-and-freedom-of-expression" target="">Libel, defamation and freedom of expression</a></li><li><a href="/core/services/publishing-ethics/business-ethics-journals" target="">Business ethics journals</a></li><!--]--></ol><!--]--><!--[--><li>Books</li><ol><!--[--><li><a href="/core/services/publishing-ethics/publishing-ethics-guidelines-books" target="">Publishing ethics guidelines for books</a></li><li><a href="/core/services/publishing-ethics/core-editorial-policies-books" target="">Core editorial policies for books</a></li><li><a href="/core/services/publishing-ethics/authorship-and-contributorship-books" target="">Authorship and contributorship for books</a></li><li><a href="/core/services/publishing-ethics/affiliations-books" target="">Affiliations for books</a></li><li><a href="/core/services/publishing-ethics/research-ethics-books" target="">Research ethics for books</a></li><li><a href="/core/services/publishing-ethics/competing-interests-and-funding-books" target="">Competing interests and funding for books</a></li><!--]--></ol><!--]--><!--[--><li>Books (cont.)</li><ol><!--[--><li><a href="/core/services/publishing-ethics/data-and-supporting-evidence-books" target="">Data and supporting evidence for books</a></li><li><a href="/core/services/publishing-ethics/misconduct-books" target="">Misconduct for books</a></li><li><a href="/core/services/publishing-ethics/corrections-retractions-and-removals-books" target="">Corrections, retractions and removals for books</a></li><li><a href="/core/services/publishing-ethics/versions-and-adaptations-books" target="">Versions and adaptations for books</a></li><li><a href="/core/services/publishing-ethics/libel-defamation-and-freedom-of-expression" target="">Libel, defamation and freedom of expression</a></li><li><a href="/core/services/publishing-ethics/business-ethics-books" target="">Business ethics books</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Publishing partners</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Publishing partners</li><ol><!--[--><li><a href="/core/services/publishing-partners/publishing-partnerships" target="">Publishing partnerships</a></li><li><a href="/core/services/publishing-partners/partner-books" target="">Partner books</a></li><li><a href="/core/services/publishing-partners/ebook-publishing-partnerships" target="">eBook publishing partnerships</a></li><li><a href="/core/services/publishing-partners/journal-publishing-partnerships" target="">Journal publishing partnerships</a></li><!--]--></ol><!--]--><!--[--><li>Publishing partners (cont.)</li><ol><!--[--><li><a href="/core/services/publishing-partners/journals-publishing" target="">Journals publishing</a></li><li><a href="/core/services/publishing-partners/customer-support" target="">Customer support</a></li><li><a href="/core/services/publishing-partners/membership-services" target="">Membership Services</a></li><li><a href="/core/services/publishing-partners/our-team" target="">Our Team</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Open research</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Open access policies</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Open access policies</li><ol><!--[--><li><a href="/core/services/open-research-policies/open-research" target="">Open research</a></li><li><a href="/core/services/open-research-policies/open-access-policies" target="">Open access policies</a></li><li><a href="/core/services/open-research-policies/cambridge-university-press-and-plan-s" target="">Cambridge University Press and Plan S</a></li><li><a href="/core/services/open-research-policies/text-and-data-mining" target="">Text and data mining</a></li><li><a href="/core/services/open-research-policies/preprint-policy" target="">Preprint policy</a></li><li><a href="/core/services/open-research-policies/social-sharing" target="">Social sharing</a></li><!--]--></ol><!--]--><!--[--><li>Journals</li><ol><!--[--><li><a href="/core/services/open-research-policies/open-access-journals" target="">Open access journals</a></li><li><a href="/core/services/open-research-policies/gold-open-access-journals" target="">Gold Open Access journals</a></li><li><a href="/core/services/open-research-policies/transformative-journals" target="">Transformative journals</a></li><li><a href="/core/services/open-research-policies/green-open-access-policy-for-journals" target="">Green Open Access policy for journals</a></li><li><a href="/core/services/open-research-policies/transparent-pricing-policy-for-journals" target="">Transparent pricing policy for journals</a></li><!--]--></ol><!--]--><!--[--><li>Books and Elements</li><ol><!--[--><li><a href="/core/services/open-research-policies/open-access-books" target="">Open access books</a></li><li><a href="/core/services/open-research-policies/gold-open-access-books" target="">Gold open access books</a></li><li><a href="/core/services/open-research-policies/green-open-access-policy-for-books" target="">Green Open Access policy for books</a></li><li><a href="/core/services/open-research-policies/open-access-elements" target="">Open access Elements</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Open access publishing</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>About open access</li><ol><!--[--><li><a href="/core/services/open-access-publishing/open-research" target="">Open research</a></li><li><a href="/core/services/open-access-publishing/open-access-week" target="">Open Access Week</a></li><li><a href="/core/services/open-access-publishing/open-access" target="">What is open access?</a></li><li><a href="/core/services/open-access-publishing/open-access-glossary" target="">Open access glossary</a></li><li><a href="/core/services/open-access-publishing/open-access-myths" target="">Open access myths</a></li><li><a href="/core/services/open-access-publishing/hybrid-open-access-faqs" target="">Hybrid Open Access FAQs</a></li><li><a href="/core/eligibility-checker" target="">Eligibility checker</a></li><!--]--></ol><!--]--><!--[--><li>Open access resources</li><ol><!--[--><li><a href="/core/services/open-access-publishing/open-access-resources" target="">Open access resources</a></li><li><a href="/core/services/open-access-publishing/benefits-of-open-access" target="">Benefits of open access</a></li><li><a href="/core/services/open-access-publishing/creative-commons-licenses" target="">Creative commons licences</a></li><li><a href="/core/services/open-access-publishing/funder-policies-and-mandates" target="">Funder policies and mandates</a></li><li><a href="/core/services/open-access-publishing/article-type-definitions" target="">Article type definitions</a></li><li><a href="/core/services/open-access-publishing/convert-your-article-to-open-access" target="">Convert your article to Open Access</a></li><li><a href="/core/services/open-access-publishing/open-access-video-resources" target="">Open access video resources</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Open research initiatives</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Research transparency</li><ol><!--[--><li><a href="/core/services/open-research-initiatives/transparency-and-openness" target="">Transparency and openness</a></li><li><a href="/core/services/open-research-initiatives/open-practice-badges" target="">Open Practice Badges</a></li><li><a href="/core/services/open-research-initiatives/oa-organisations-initiatives-and-directories" target="">OA organisations, initiatives &amp; directories</a></li><li><a href="/core/services/open-research-initiatives/registered-reports" target="">Registered Reports</a></li><li><a href="/core/services/open-research-initiatives/annotation-for-transparent-inquiry-ati" target="">Annotation for Transparent Inquiry (ATI)</a></li><!--]--></ol><!--]--><!--[--><li>Journal flips</li><ol><!--[--><li><a href="/core/services/open-research-initiatives/open-access-journal-flips" target="">Open access journal flips</a></li><li><a href="/core/services/open-research-initiatives/oa-journal-flip-faqs" target="">OA Journal Flip FAQs</a></li><!--]--></ol><!--]--><!--[--><li>Flip it Open</li><ol><!--[--><li><a href="/core/services/open-research-initiatives/flip-it-open" target="">Flip it Open</a></li><li><a href="/core/services/open-research-initiatives/flip-it-open-faqs" target="">Flip it Open FAQs</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li>Open access funding</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>Open access funding</li><ol><!--[--><li><a href="/core/services/open-access-funding/funding-open-access-publication" target="">Funding open access publication</a></li><li><a href="/core/services/open-access-funding/cambridge-open-equity-initiative" target="">Cambridge Open Equity Initiative</a></li><li><a href="/core/services/open-access-funding/completing-a-rightslink-open-access-transaction" target="">Completing a RightsLink (open access) transaction</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--[--><li> Cambridge Open Engage</li><ol><!--[--><!--]--></ol><ol><!--[--><!--[--><li>  Cambridge Open Engage</li><ol><!--[--><li><a href="/core/services/cambridge-open-engage/cambridge-open-engage" target="">Cambridge Open Engage</a></li><li><a href="/core/services/cambridge-open-engage/engage-partner-with-us" target="">Partner With Us</a></li><li><a href="/core/services/cambridge-open-engage/engage-branded-hubs" target="">Branded Hubs</a></li><li><a href="/core/services/cambridge-open-engage/engage-event-workspaces" target="">Event Workspaces</a></li><li><a href="/core/services/cambridge-open-engage/engage-partner-resources" target="">Partner Resources</a></li><li><a href="/core/services/cambridge-open-engage/engage-apsa-preprints" target="">APSA Preprints</a></li><li><a href="/core/services/cambridge-open-engage/engage-apsa-preprints-faqs" target="">APSA Preprints FAQs</a></li><!--]--></ol><!--]--><!--]--></ol><!--]--><!--]--></ol><!--]--><!--]--></ol></div></div></noscript><!--]--><!----><!----><!----><!--]--></div>
</div></div><script>window.__PLATFORM_HEADER_DATA__ = {"megaMenuData":{"menuItems":[{"id":"menu_item-0","label":"Browse","openExternal":null,"url":"","isMegaMenu":true,"menuCats":[{"id":"menu_item-0_cat-0","label":"Subjects","openExternal":null,"url":"/core/browse-subjects","menuTopics":[{"id":"menu_item-0_cat-0_subCat-0","label":" Subjects (A-D)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-0_subCat-0_link-0","label":"Anthropology","openExternal":false,"url":"/core/browse-subjects/anthropology"},{"id":"menu_item-0_cat-0_subCat-0_link-1","label":"Archaeology","openExternal":false,"url":"/core/browse-subjects/archaeology"},{"id":"menu_item-0_cat-0_subCat-0_link-2","label":"Area Studies","openExternal":false,"url":"/core/browse-subjects/area-studies"},{"id":"menu_item-0_cat-0_subCat-0_link-3","label":"Art","openExternal":false,"url":"/core/browse-subjects/art"},{"id":"menu_item-0_cat-0_subCat-0_link-4","label":"Chemistry","openExternal":false,"url":"/core/browse-subjects/chemistry"},{"id":"menu_item-0_cat-0_subCat-0_link-5","label":"Classical Studies","openExternal":false,"url":"/core/browse-subjects/classical-studies"},{"id":"menu_item-0_cat-0_subCat-0_link-6","label":"Computer Science","openExternal":false,"url":"/core/browse-subjects/computer-science"},{"id":"menu_item-0_cat-0_subCat-0_link-7","label":"Drama, Theatre, Performance Studies","openExternal":false,"url":"/core/browse-subjects/drama-and-theatre"}]},{"id":"menu_item-0_cat-0_subCat-1","label":" Subjects (E-K)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-0_subCat-1_link-0","label":"Earth and Environmental Science","openExternal":false,"url":"/core/browse-subjects/earth-and-environmental-sciences"},{"id":"menu_item-0_cat-0_subCat-1_link-1","label":"Economics","openExternal":false,"url":"/core/browse-subjects/economics"},{"id":"menu_item-0_cat-0_subCat-1_link-2","label":"Education","openExternal":false,"url":"/core/browse-subjects/education"},{"id":"menu_item-0_cat-0_subCat-1_link-3","label":"Engineering","openExternal":false,"url":"/core/browse-subjects/engineering"},{"id":"menu_item-0_cat-0_subCat-1_link-4","label":"English Language Teaching – Resources for Teachers","openExternal":false,"url":"/core/browse-subjects/english-language-teaching-resources-for-teachers"},{"id":"menu_item-0_cat-0_subCat-1_link-5","label":"Film, Media, Mass Communication","openExternal":false,"url":"/core/browse-subjects/film-media-mass-ommunication"},{"id":"menu_item-0_cat-0_subCat-1_link-6","label":"General Science","openExternal":false,"url":"/core/browse-subjects/general-science"},{"id":"menu_item-0_cat-0_subCat-1_link-7","label":"Geography","openExternal":false,"url":"/core/browse-subjects/geography"},{"id":"menu_item-0_cat-0_subCat-1_link-8","label":"History","openExternal":false,"url":"/core/browse-subjects/history"}]},{"id":"menu_item-0_cat-0_subCat-2","label":" Subjects (L-O)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-0_subCat-2_link-0","label":"Language and Linguistics","openExternal":false,"url":"/core/browse-subjects/language-and-linguistics"},{"id":"menu_item-0_cat-0_subCat-2_link-1","label":"Law","openExternal":false,"url":"/core/browse-subjects/law"},{"id":"menu_item-0_cat-0_subCat-2_link-2","label":"Life Sciences","openExternal":false,"url":"/core/browse-subjects/life-sciences"},{"id":"menu_item-0_cat-0_subCat-2_link-3","label":"Literature","openExternal":false,"url":"/core/browse-subjects/literature"},{"id":"menu_item-0_cat-0_subCat-2_link-4","label":"Management","openExternal":false,"url":"/core/browse-subjects/management"},{"id":"menu_item-0_cat-0_subCat-2_link-5","label":"Materials Science","openExternal":false,"url":"/core/browse-subjects/materials-science"},{"id":"menu_item-0_cat-0_subCat-2_link-6","label":"Mathematics","openExternal":false,"url":"/core/browse-subjects/mathematics"},{"id":"menu_item-0_cat-0_subCat-2_link-7","label":"Medicine","openExternal":false,"url":"/core/browse-subjects/medicine"},{"id":"menu_item-0_cat-0_subCat-2_link-8","label":"Music","openExternal":false,"url":"/core/browse-subjects/music"},{"id":"menu_item-0_cat-0_subCat-2_link-9","label":"Nutrition","openExternal":false,"url":"/core/browse-subjects/nutrition"}]},{"id":"menu_item-0_cat-0_subCat-3","label":" Subjects (P-Z)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-0_subCat-3_link-0","label":"Philosophy","openExternal":false,"url":"/core/browse-subjects/philosophy"},{"id":"menu_item-0_cat-0_subCat-3_link-1","label":"Physics and Astronomy","openExternal":false,"url":"/core/browse-subjects/physics"},{"id":"menu_item-0_cat-0_subCat-3_link-2","label":"Politics and International Relations","openExternal":false,"url":"/core/browse-subjects/politics-and-international-relations"},{"id":"menu_item-0_cat-0_subCat-3_link-3","label":"Psychiatry","openExternal":false,"url":"/core/browse-subjects/psychiatry"},{"id":"menu_item-0_cat-0_subCat-3_link-4","label":"Psychology","openExternal":false,"url":"/core/browse-subjects/psychology"},{"id":"menu_item-0_cat-0_subCat-3_link-5","label":"Religion","openExternal":false,"url":"/core/browse-subjects/religion"},{"id":"menu_item-0_cat-0_subCat-3_link-6","label":"Social Science Research Methods","openExternal":false,"url":"/core/browse-subjects/social-science-research-methods"},{"id":"menu_item-0_cat-0_subCat-3_link-7","label":"Sociology","openExternal":false,"url":"/core/browse-subjects/sociology"},{"id":"menu_item-0_cat-0_subCat-3_link-8","label":"Statistics and Probability","openExternal":false,"url":"/core/browse-subjects/statistics-and-probability"}]}]},{"id":"menu_item-0_cat-1","label":"Open access","openExternal":null,"url":"/core/publications/open-access","menuTopics":[{"id":"menu_item-0_cat-1_subCat-0","label":"All open access publishing","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-1_subCat-0_link-0","label":"Open access","openExternal":false,"url":"/core/publications/open-access"},{"id":"menu_item-0_cat-1_subCat-0_link-1","label":"Open access journals","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-1_subCat-0_link-2","label":"Research open journals","openExternal":false,"url":"/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-1_subCat-0_link-3","label":"Journals containing open access","openExternal":false,"url":"/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-1_subCat-0_link-4","label":"Open access articles","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE"},{"id":"menu_item-0_cat-1_subCat-0_link-5","label":"Open access books","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&sort=canonical.date:desc"},{"id":"menu_item-0_cat-1_subCat-0_link-6","label":"Open access Elements","openExternal":false,"url":"/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493"}]}]},{"id":"menu_item-0_cat-2","label":"Journals","openExternal":null,"url":"/core/publications/journals","menuTopics":[{"id":"menu_item-0_cat-2_subCat-0","label":"Explore","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-2_subCat-0_link-0","label":"All journal subjects","openExternal":false,"url":"/core/publications/journals"},{"id":"menu_item-0_cat-2_subCat-0_link-1","label":"Search journals","openExternal":false,"url":"/core/publications/journals"}]},{"id":"menu_item-0_cat-2_subCat-1","label":"Open access","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-2_subCat-1_link-0","label":"Open access journals","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-2_subCat-1_link-1","label":"Research open journals","openExternal":false,"url":"/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-2_subCat-1_link-2","label":"Journals containing open access","openExternal":false,"url":"/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc"},{"id":"menu_item-0_cat-2_subCat-1_link-3","label":"Open access articles","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE"}]},{"id":"menu_item-0_cat-2_subCat-2","label":"Collections","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-2_subCat-2_link-0","label":"Cambridge Forum","openExternal":false,"url":"/core/publications/collections/cambridge-forum"},{"id":"menu_item-0_cat-2_subCat-2_link-1","label":"Cambridge Law Reports Collection","openExternal":false,"url":"/core/publications/collections/cambridge-law-reports-collection"},{"id":"menu_item-0_cat-2_subCat-2_link-2","label":"Cambridge Prisms","openExternal":false,"url":"/core/publications/collections/cambridge-prisms"},{"id":"menu_item-0_cat-2_subCat-2_link-3","label":"Research Directions","openExternal":false,"url":"/core/publications/collections/research-directions"}]}]},{"id":"menu_item-0_cat-3","label":"Books","openExternal":null,"url":"/core/publications/books","menuTopics":[{"id":"menu_item-0_cat-3_subCat-0","label":"Explore","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-3_subCat-0_link-0","label":"Books","openExternal":false,"url":"/core/publications/books"},{"id":"menu_item-0_cat-3_subCat-0_link-1","label":"Open access books","openExternal":false,"url":"/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&sort=canonical.date:desc"},{"id":"menu_item-0_cat-3_subCat-0_link-2","label":"New books","openExternal":false,"url":"/core/publications/books/listing?aggs[productDate][filters]=Last+3+months&aggs[productTypes][filters]=BOOK&sort=canonical.date:desc"},{"id":"menu_item-0_cat-3_subCat-0_link-3","label":"Flip it Open","openExternal":false,"url":"/core/publications/collections/flip-it-open"}]},{"id":"menu_item-0_cat-3_subCat-1","label":"Collections","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-3_subCat-1_link-0","label":"Cambridge Companions","openExternal":false,"url":"/core/publications/collections/cambridge-companions"},{"id":"menu_item-0_cat-3_subCat-1_link-1","label":"Cambridge Editions","openExternal":false,"url":"/core/publications/collections/cambridge-editions"},{"id":"menu_item-0_cat-3_subCat-1_link-2","label":"Cambridge Histories","openExternal":false,"url":"/core/publications/collections/cambridge-histories"},{"id":"menu_item-0_cat-3_subCat-1_link-3","label":"Cambridge Library Collection","openExternal":false,"url":"/core/publications/collections/cambridge-library-collection"},{"id":"menu_item-0_cat-3_subCat-1_link-4","label":"Cambridge Shakespeare","openExternal":false,"url":"/core/publications/collections/cambridge-shakespeare"},{"id":"menu_item-0_cat-3_subCat-1_link-5","label":"Cambridge Handbooks","openExternal":false,"url":"/core/publications/collections/cambridgehandbooks"}]},{"id":"menu_item-0_cat-3_subCat-2","label":" Collections (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-3_subCat-2_link-0","label":"Dispute Settlement Reports Online","openExternal":false,"url":"/core/publications/collections/dispute-settlement-reports-online"},{"id":"menu_item-0_cat-3_subCat-2_link-1","label":"Flip it Open","openExternal":false,"url":"/core/publications/collections/flip-it-open"},{"id":"menu_item-0_cat-3_subCat-2_link-2","label":"Hemingway Letters","openExternal":false,"url":"/core/publications/collections/hemingway-letters"},{"id":"menu_item-0_cat-3_subCat-2_link-3","label":"Shakespeare Survey","openExternal":false,"url":"/core/publications/collections/shakespeare-survey"},{"id":"menu_item-0_cat-3_subCat-2_link-4","label":"Stahl Online","openExternal":false,"url":"/core/publications/collections/stahl-online"},{"id":"menu_item-0_cat-3_subCat-2_link-5","label":"The Correspondence of Isaac Newton","openExternal":false,"url":"/core/publications/collections/the-correspondence-of-isaac-newton"}]}]},{"id":"menu_item-0_cat-4","label":"Elements","openExternal":null,"url":"/core/publications/elements","menuTopics":[{"id":"menu_item-0_cat-4_subCat-0","label":"Explore","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-4_subCat-0_link-0","label":"About Elements","openExternal":false,"url":"/core/publications/elements"},{"id":"menu_item-0_cat-4_subCat-0_link-1","label":"Elements series","openExternal":false,"url":"/core/publications/elements/cambridge-elements-series"},{"id":"menu_item-0_cat-4_subCat-0_link-2","label":"Open access Elements","openExternal":false,"url":"/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493"},{"id":"menu_item-0_cat-4_subCat-0_link-3","label":"New Elements","openExternal":false,"url":"/core/publications/elements/published-elements?aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&aggs%5BproductDate%5D%5Bfilters%5D=Last%203%20months&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493"}]},{"id":"menu_item-0_cat-4_subCat-1","label":"Subjects (A-E)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-4_subCat-1_link-0","label":"Anthropology","openExternal":false,"url":"/core/elements/subject/Anthropology/2E44A5AF2838E017617A26DD79FAEAEE"},{"id":"menu_item-0_cat-4_subCat-1_link-1","label":"Archaeology","openExternal":false,"url":"/core/elements/subject/Archaeology/63A50B5368A9F97F8AA2D6AB965B5F4C"},{"id":"menu_item-0_cat-4_subCat-1_link-2","label":"Classical Studies","openExternal":false,"url":"/core/elements/subject/Classical%20Studies/DDC63B7F5792FE2A95D1FB15F76E3F42"},{"id":"menu_item-0_cat-4_subCat-1_link-3","label":"Computer Science","openExternal":false,"url":"/core/elements/subject/Computer%20Science/A57E10708F64FB69CE78C81A5C2A6555"},{"id":"menu_item-0_cat-4_subCat-1_link-4","label":"Drama, Theatre, Performance Studies","openExternal":false,"url":"/core/elements/subject/Drama,%20Theatre,%20Performance%20Studies/2825E4E39F2D641B36543EE80FB1DEA3"},{"id":"menu_item-0_cat-4_subCat-1_link-5","label":"Earth and Environmental Sciences","openExternal":false,"url":"/core/elements/subject/Earth%20and%20Environmental%20Sciences/F470FBF5683D93478C7CAE5A30EF9AE8"},{"id":"menu_item-0_cat-4_subCat-1_link-6","label":"Economics","openExternal":false,"url":"/core/elements/subject/Economics/FA44491F1F55F917C43E9832715B9DE7"},{"id":"menu_item-0_cat-4_subCat-1_link-7","label":"Education","openExternal":false,"url":"/core/elements/subject/Education/550D00F8DF590F2598CF7CC0038E24D1"},{"id":"menu_item-0_cat-4_subCat-1_link-8","label":"Engineering","openExternal":false,"url":"/core/elements/subject/Engineering/CCC62FE56DCC1D050CA1340C1CCF46F5"}]},{"id":"menu_item-0_cat-4_subCat-2","label":" Subjects (F-O)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-4_subCat-2_link-0","label":"Film, Media, Mass Communication","openExternal":false,"url":"/core/elements/subject/Film,%20Media,%20Mass%20Communication/4B91F10E834814A90CE718E7831E492F"},{"id":"menu_item-0_cat-4_subCat-2_link-1","label":"History","openExternal":false,"url":"/core/elements/subject/History/66BE42A30172E280FDE64F8EE2F485B0"},{"id":"menu_item-0_cat-4_subCat-2_link-2","label":"Language and Linguistics","openExternal":false,"url":"/core/elements/subject/Language%20and%20Linguistics/140D314098408C26BDF3009F7FF858E9"},{"id":"menu_item-0_cat-4_subCat-2_link-3","label":"Law","openExternal":false,"url":"/core/elements/subject/Law/7C9FB6788DD8D7E6696263BC774F4D5B"},{"id":"menu_item-0_cat-4_subCat-2_link-4","label":"Life Sciences","openExternal":false,"url":"/core/elements/subject/Life%20Sciences/E044EF2F61B601378786E9EDA901B2D5"},{"id":"menu_item-0_cat-4_subCat-2_link-5","label":"Literature","openExternal":false,"url":"/core/elements/subject/Literature/F2434ADC122145767C6C3B988A8E9BD5"},{"id":"menu_item-0_cat-4_subCat-2_link-6","label":"Management","openExternal":false,"url":"/core/elements/subject/Management/0EDCC0540639B06A5669BDEEF50C4CBE"},{"id":"menu_item-0_cat-4_subCat-2_link-7","label":"Mathematics","openExternal":false,"url":"/core/elements/subject/Mathematics/FA1467C44B5BD46BB8AA6E58C2252153"},{"id":"menu_item-0_cat-4_subCat-2_link-8","label":"Medicine","openExternal":false,"url":"/core/elements/subject/Medicine/66FF02B2A4F83D9A645001545197F287"},{"id":"menu_item-0_cat-4_subCat-2_link-9","label":"Music","openExternal":false,"url":"/core/elements/subject/Music/A370B5604591CB3C7F9AFD892DDF7BD1"}]},{"id":"menu_item-0_cat-4_subCat-3","label":" Subjects (P-Z)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-4_subCat-3_link-0","label":"Philosophy","openExternal":false,"url":"/core/elements/subject/Philosophy/2D1AC3C0E174F1F1A93F8C7DE19E0FAB"},{"id":"menu_item-0_cat-4_subCat-3_link-1","label":"Physics and Astronomy","openExternal":false,"url":"/core/elements/subject/Physics%20and%20Astronomy/DBFB610E9FC5E012C011430C0573CC06"},{"id":"menu_item-0_cat-4_subCat-3_link-2","label":"Politics and International Relations","openExternal":false,"url":"/core/elements/subject/Politics%20and%20International%20Relations/3BF83347E5E456DAC34F3FABFC8BBF4E"},{"id":"menu_item-0_cat-4_subCat-3_link-3","label":"Psychology","openExternal":false,"url":"/core/elements/subject/Psychology/21B42A72BA3E4CB0E3315E5B1B71B07F"},{"id":"menu_item-0_cat-4_subCat-3_link-4","label":"Religion","openExternal":false,"url":"/core/elements/subject/Religion/53E51D24FB488962B9364A2C4B45D1C3"},{"id":"menu_item-0_cat-4_subCat-3_link-5","label":"Sociology","openExternal":false,"url":"/core/elements/subject/Sociology/0E2CD53A93003DF17E52D753F6E90683"},{"id":"menu_item-0_cat-4_subCat-3_link-6","label":"Statistics and Probability","openExternal":false,"url":"/core/elements/subject/Statistics%20and%20Probability/3150B8B0D1B0B4E8DC17EC9EDFD9CA26"}]}]},{"id":"menu_item-0_cat-5","label":"Textbooks","openExternal":null,"url":"/core/publications/textbooks","menuTopics":[{"id":"menu_item-0_cat-5_subCat-0","label":"Explore","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-5_subCat-0_link-0","label":"Cambridge Higher Education","openExternal":false,"url":"/highereducation/"},{"id":"menu_item-0_cat-5_subCat-0_link-1","label":"Title list","openExternal":false,"url":"/highereducation/services/librarians/title-list"},{"id":"menu_item-0_cat-5_subCat-0_link-2","label":"New titles","openExternal":false,"url":"/highereducation/search?sortBy=publication_date&aggs=%24productDate%24Last%25206%2520months%3Atrue%26Last%252012%2520months%3Atrue%26Last%25203%2520years%3Atrue%26Over%25203%2520years%3Atrue%3B%3B&event=SE-AU_PREF"}]}]},{"id":"menu_item-0_cat-6","label":"Collections","openExternal":null,"url":"/core/publications/collections","menuTopics":[{"id":"menu_item-0_cat-6_subCat-0","label":"Book collections","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-6_subCat-0_link-0","label":"Cambridge Companions","openExternal":false,"url":"/core/publications/collections/cambridge-companions"},{"id":"menu_item-0_cat-6_subCat-0_link-1","label":"Cambridge Editions","openExternal":false,"url":"/core/publications/collections/cambridge-editions"},{"id":"menu_item-0_cat-6_subCat-0_link-2","label":"Cambridge Histories","openExternal":false,"url":"/core/publications/collections/cambridge-histories"},{"id":"menu_item-0_cat-6_subCat-0_link-3","label":"Cambridge Library Collection","openExternal":false,"url":"/core/publications/collections/cambridge-library-collection"},{"id":"menu_item-0_cat-6_subCat-0_link-4","label":"Cambridge Shakespeare","openExternal":false,"url":"/core/publications/collections/cambridge-shakespeare"},{"id":"menu_item-0_cat-6_subCat-0_link-5","label":"Cambridge Handbooks","openExternal":false,"url":"/core/publications/collections/cambridgehandbooks"}]},{"id":"menu_item-0_cat-6_subCat-1","label":" Book collections (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-6_subCat-1_link-0","label":"Dispute Settlement Reports Online","openExternal":false,"url":"/core/publications/collections/dispute-settlement-reports-online"},{"id":"menu_item-0_cat-6_subCat-1_link-1","label":"Flip it Open","openExternal":false,"url":"/core/publications/collections/flip-it-open"},{"id":"menu_item-0_cat-6_subCat-1_link-2","label":"Hemingway Letters","openExternal":false,"url":"/core/publications/collections/hemingway-letters"},{"id":"menu_item-0_cat-6_subCat-1_link-3","label":"Shakespeare Survey","openExternal":false,"url":"/core/publications/collections/shakespeare-survey"},{"id":"menu_item-0_cat-6_subCat-1_link-4","label":"Stahl Online","openExternal":false,"url":"/core/publications/collections/stahl-online"},{"id":"menu_item-0_cat-6_subCat-1_link-5","label":"The Correspondence of Isaac Newton","openExternal":false,"url":"/core/publications/collections/the-correspondence-of-isaac-newton"}]},{"id":"menu_item-0_cat-6_subCat-2","label":"Journal collections","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-6_subCat-2_link-0","label":"Cambridge Forum","openExternal":false,"url":"/core/publications/collections/cambridge-forum"},{"id":"menu_item-0_cat-6_subCat-2_link-1","label":"Cambridge Law Reports Collection","openExternal":false,"url":"/core/publications/collections/cambridge-law-reports-collection"},{"id":"menu_item-0_cat-6_subCat-2_link-2","label":"Cambridge Prisms","openExternal":false,"url":"/core/publications/collections/cambridge-prisms"},{"id":"menu_item-0_cat-6_subCat-2_link-3","label":"Research Directions","openExternal":false,"url":"/core/publications/collections/research-directions"}]},{"id":"menu_item-0_cat-6_subCat-3","label":"Series","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-6_subCat-3_link-0","label":"All series","openExternal":false,"url":"/core/publications/collections/series"}]}]},{"id":"menu_item-0_cat-7","label":"Partners","openExternal":null,"url":"/core/publications/publishing-partners","menuTopics":[{"id":"menu_item-0_cat-7_subCat-0","label":"Partners","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-7_subCat-0_link-0","label":"Agenda Publishing","openExternal":false,"url":"/core/publications/publishing-partners/agenda-publishing"},{"id":"menu_item-0_cat-7_subCat-0_link-1","label":"Amsterdam University Press","openExternal":false,"url":"/core/publications/publishing-partners/amsterdam-university-press"},{"id":"menu_item-0_cat-7_subCat-0_link-2","label":"Anthem Press","openExternal":false,"url":"/core/publications/publishing-partners/anthem-press"},{"id":"menu_item-0_cat-7_subCat-0_link-3","label":"Boydell & Brewer","openExternal":false,"url":"/core/publications/publishing-partners/boydell-brewer"},{"id":"menu_item-0_cat-7_subCat-0_link-4","label":"Bristol University Press","openExternal":false,"url":"/core/publications/publishing-partners/bristol-university-press"},{"id":"menu_item-0_cat-7_subCat-0_link-5","label":"Edinburgh University Press","openExternal":false,"url":"/core/publications/publishing-partners/edinburgh-university-press"},{"id":"menu_item-0_cat-7_subCat-0_link-6","label":"Emirates Center for Strategic Studies and Research","openExternal":false,"url":"/core/publications/publishing-partners/emirates-center"},{"id":"menu_item-0_cat-7_subCat-0_link-7","label":"Facet Publishing","openExternal":false,"url":"/core/publications/publishing-partners/facet-publishing"}]},{"id":"menu_item-0_cat-7_subCat-1","label":" Partners (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-0_cat-7_subCat-1_link-0","label":"Foundation Books","openExternal":false,"url":"/core/publications/publishing-partners/foundation-books"},{"id":"menu_item-0_cat-7_subCat-1_link-1","label":"Intersentia","openExternal":false,"url":"/core/publications/publishing-partners/intersentia"},{"id":"menu_item-0_cat-7_subCat-1_link-2","label":"ISEAS-Yusof Ishak Institute","openExternal":false,"url":"/core/publications/publishing-partners/iseas"},{"id":"menu_item-0_cat-7_subCat-1_link-3","label":"Jagiellonian University Press","openExternal":false,"url":"/core/publications/publishing-partners/jagiellonian-university-press"},{"id":"menu_item-0_cat-7_subCat-1_link-4","label":"Royal Economic Society","openExternal":false,"url":"/core/publications/publishing-partners/royal-economic-society"},{"id":"menu_item-0_cat-7_subCat-1_link-5","label":"Unisa Press","openExternal":false,"url":"/core/publications/publishing-partners/unisa-press"},{"id":"menu_item-0_cat-7_subCat-1_link-6","label":"The University of Adelaide Press","openExternal":false,"url":"/core/publications/publishing-partners/university-adelaide-press"},{"id":"menu_item-0_cat-7_subCat-1_link-7","label":"Wits University Press","openExternal":false,"url":"/core/publications/publishing-partners/wits-university-press"}]}]}]},{"id":"menu_item-1","label":"Services","openExternal":null,"url":"","isMegaMenu":true,"menuCats":[{"id":"menu_item-1_cat-0","label":"About","openExternal":null,"url":"/core/services/about/about","menuTopics":[{"id":"menu_item-1_cat-0_subCat-0","label":"About Cambridge Core","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-0_subCat-0_link-0","label":"About","openExternal":false,"url":"/core/services/about/about"},{"id":"menu_item-1_cat-0_subCat-0_link-1","label":"Accessibility","openExternal":false,"url":"/core/services/about/accessibility"},{"id":"menu_item-1_cat-0_subCat-0_link-2","label":"CrossMark policy","openExternal":false,"url":"/core/services/about/crossmark-policy"},{"id":"menu_item-1_cat-0_subCat-0_link-3","label":"Ethical Standards","openExternal":false,"url":"/core/services/about/ethical-standards"}]},{"id":"menu_item-1_cat-0_subCat-1","label":"Environment and sustainability","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-0_subCat-1_link-0","label":"Environment and sustainability","openExternal":false,"url":"/core/services/about/environment-and-sustainability"},{"id":"menu_item-1_cat-0_subCat-1_link-1","label":"Reducing print","openExternal":false,"url":"/core/services/about/reducing-print"},{"id":"menu_item-1_cat-0_subCat-1_link-2","label":"Journals moving to online only","openExternal":false,"url":"/core/services/about/journals-moving-to-online-only"}]},{"id":"menu_item-1_cat-0_subCat-2","label":"Guides","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-0_subCat-2_link-0","label":"User guides","openExternal":false,"url":"/core/services/about/user-guides"},{"id":"menu_item-1_cat-0_subCat-2_link-1","label":"User Guides and Videos","openExternal":false,"url":"/core/services/about/user-guides-and-videos"},{"id":"menu_item-1_cat-0_subCat-2_link-2","label":"Support Videos","openExternal":false,"url":"/core/services/about/support-videos"},{"id":"menu_item-1_cat-0_subCat-2_link-3","label":"Training","openExternal":false,"url":"/core/services/about/training"}]},{"id":"menu_item-1_cat-0_subCat-3","label":"Help","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-0_subCat-3_link-0","label":"Cambridge Core help","openExternal":false,"url":"https://corehelp.cambridge.org/"},{"id":"menu_item-1_cat-0_subCat-3_link-1","label":"Contact us","openExternal":false,"url":"https://corehelp.cambridge.org/hc/en-gb/p/contact-information"},{"id":"menu_item-1_cat-0_subCat-3_link-2","label":"Technical support","openExternal":false,"url":"https://corehelp.cambridge.org/hc/en-gb/requests/new"}]}]},{"id":"menu_item-1_cat-1","label":"Agents","openExternal":null,"url":"/core/services/agents/services-for-agents","menuTopics":[{"id":"menu_item-1_cat-1_subCat-0","label":"Services for agents","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-1_subCat-0_link-0","label":"Services for agents","openExternal":false,"url":"/core/services/agents/services-for-agents"},{"id":"menu_item-1_cat-1_subCat-0_link-1","label":"Journals for agents","openExternal":false,"url":"/core/services/agents/journals-for-agents"},{"id":"menu_item-1_cat-1_subCat-0_link-2","label":"Books for agents","openExternal":false,"url":"/core/services/agents/books-for-agents"},{"id":"menu_item-1_cat-1_subCat-0_link-3","label":"Price list","openExternal":false,"url":"/core/services/agents/price-list"}]}]},{"id":"menu_item-1_cat-2","label":"Authors","openExternal":null,"url":"/core/services/authors/authors","menuTopics":[{"id":"menu_item-1_cat-2_subCat-0","label":"Journals","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-2_subCat-0_link-0","label":"Journals","openExternal":false,"url":"/core/services/authors/journals"},{"id":"menu_item-1_cat-2_subCat-0_link-1","label":"Journal publishing statistics","openExternal":false,"url":"/core/services/authors/journal-publishing-statistics"},{"id":"menu_item-1_cat-2_subCat-0_link-2","label":"Corresponding author","openExternal":false,"url":"/core/services/authors/corresponding-author"},{"id":"menu_item-1_cat-2_subCat-0_link-3","label":"Seeking permission to use copyrighted material","openExternal":false,"url":"/core/services/authors/seeking-permission-to-use-copyrighted-material"},{"id":"menu_item-1_cat-2_subCat-0_link-4","label":"Publishing supplementary material","openExternal":false,"url":"/core/services/authors/publishing-supplementary-material"},{"id":"menu_item-1_cat-2_subCat-0_link-5","label":"Writing an effective abstract","openExternal":false,"url":"/core/services/authors/writing-an-effective-abstract"},{"id":"menu_item-1_cat-2_subCat-0_link-6","label":"Journal production - FAQs","openExternal":false,"url":"/core/services/authors/journal-production-faqs"}]},{"id":"menu_item-1_cat-2_subCat-1","label":"Journals (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-2_subCat-1_link-0","label":"Author affiliations","openExternal":false,"url":"/core/services/authors/author-affiliations"},{"id":"menu_item-1_cat-2_subCat-1_link-1","label":"Co-reviewing policy","openExternal":false,"url":"/core/services/authors/co-reviewing-policy"},{"id":"menu_item-1_cat-2_subCat-1_link-2","label":"Digital Author Publishing Agreement - FAQs","openExternal":false,"url":"/core/services/authors/digital-author-publishing-agreement-faqs"},{"id":"menu_item-1_cat-2_subCat-1_link-3","label":"Anonymising your manuscript","openExternal":false,"url":"/core/services/authors/anonymising-your-manuscript"},{"id":"menu_item-1_cat-2_subCat-1_link-4","label":"Publishing open access","openExternal":false,"url":"/core/services/authors/publishing-open-access"},{"id":"menu_item-1_cat-2_subCat-1_link-5","label":"Converting your article to open access","openExternal":false,"url":"/core/services/authors/converting-your-article-to-open-access"},{"id":"menu_item-1_cat-2_subCat-1_link-6","label":"Publishing Open Access - webinars","openExternal":false,"url":"/core/services/authors/publishing-open-access-webinars"}]},{"id":"menu_item-1_cat-2_subCat-2","label":"Journals (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-2_subCat-2_link-0","label":"Preparing and submitting your paper","openExternal":false,"url":"/core/services/authors/preparing-and-submitting-your-paper"},{"id":"menu_item-1_cat-2_subCat-2_link-1","label":"Publishing an accepted paper","openExternal":false,"url":"/core/services/authors/publishing-an-accepted-paper"},{"id":"menu_item-1_cat-2_subCat-2_link-2","label":"Promoting your published paper","openExternal":false,"url":"/core/services/authors/promoting-your-published-paper"},{"id":"menu_item-1_cat-2_subCat-2_link-3","label":"Measuring impact","openExternal":false,"url":"/core/services/authors/measuring-impact"},{"id":"menu_item-1_cat-2_subCat-2_link-4","label":"Journals artwork guide","openExternal":false,"url":"/core/services/authors/journals-artwork-guide"},{"id":"menu_item-1_cat-2_subCat-2_link-5","label":"Using ORCID","openExternal":false,"url":"/core/services/authors/using-orcid"}]},{"id":"menu_item-1_cat-2_subCat-3","label":"Books","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-2_subCat-3_link-0","label":"Books","openExternal":false,"url":"/core/services/authors/books"},{"id":"menu_item-1_cat-2_subCat-3_link-1","label":"Marketing your book","openExternal":false,"url":"/core/services/authors/marketing-your-book"},{"id":"menu_item-1_cat-2_subCat-3_link-2","label":"Author guides for Cambridge Elements","openExternal":false,"url":"/core/services/authors/elements-user-guides"}]}]},{"id":"menu_item-1_cat-3","label":"Corporates","openExternal":null,"url":"/core/services/corporates/services-for-corporates","menuTopics":[{"id":"menu_item-1_cat-3_subCat-0","label":"Corporates","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-3_subCat-0_link-0","label":"Commercial reprints","openExternal":false,"url":"/core/services/corporates/commercial-reprints"},{"id":"menu_item-1_cat-3_subCat-0_link-1","label":"Advertising","openExternal":false,"url":"/core/services/corporates/advertising"},{"id":"menu_item-1_cat-3_subCat-0_link-2","label":"Sponsorship","openExternal":false,"url":"/core/services/corporates/sponsorship"},{"id":"menu_item-1_cat-3_subCat-0_link-3","label":"Book special sales","openExternal":false,"url":"/core/services/corporates/book-special-sales"},{"id":"menu_item-1_cat-3_subCat-0_link-4","label":"Contact us","openExternal":false,"url":"/core/services/corporates/contact-us"}]}]},{"id":"menu_item-1_cat-4","label":"Editors","openExternal":null,"url":"/core/services/editors/editors","menuTopics":[{"id":"menu_item-1_cat-4_subCat-0","label":"Information","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-4_subCat-0_link-0","label":"Journal development","openExternal":false,"url":"/core/services/editors/journal-development"},{"id":"menu_item-1_cat-4_subCat-0_link-1","label":"Peer review for editors","openExternal":false,"url":"/core/services/editors/peer-review-for-editors"},{"id":"menu_item-1_cat-4_subCat-0_link-2","label":"Open access for editors","openExternal":false,"url":"/core/services/editors/open-access-for-editors"},{"id":"menu_item-1_cat-4_subCat-0_link-3","label":"Policies and guidelines","openExternal":false,"url":"/core/services/editors/policies-and-guidelines"}]},{"id":"menu_item-1_cat-4_subCat-1","label":"Resources","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-4_subCat-1_link-0","label":"The editor's role","openExternal":false,"url":"/core/services/editors/the-editors-role"},{"id":"menu_item-1_cat-4_subCat-1_link-1","label":"Open research for editors","openExternal":false,"url":"/core/services/editors/open-research-for-editors"},{"id":"menu_item-1_cat-4_subCat-1_link-2","label":"Engagement and promotion","openExternal":false,"url":"/core/services/editors/engagement-and-promotion"},{"id":"menu_item-1_cat-4_subCat-1_link-3","label":"Blogging","openExternal":false,"url":"/core/services/editors/blogging"},{"id":"menu_item-1_cat-4_subCat-1_link-4","label":"Social media","openExternal":false,"url":"/core/services/editors/social-media"}]}]},{"id":"menu_item-1_cat-5","label":"Librarians","openExternal":null,"url":"/core/services/librarians/librarians","menuTopics":[{"id":"menu_item-1_cat-5_subCat-0","label":"Information","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-5_subCat-0_link-0","label":"Open Access for Librarians","openExternal":false,"url":"/core/services/librarians/open-access-for-librarians"},{"id":"menu_item-1_cat-5_subCat-0_link-1","label":"Transformative agreements","openExternal":false,"url":"https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements"},{"id":"menu_item-1_cat-5_subCat-0_link-2","label":"Transformative Agreements - FAQs","openExternal":false,"url":"/core/services/librarians/transformative-agreements-faqs"},{"id":"menu_item-1_cat-5_subCat-0_link-3","label":"Evidence based acquisition","openExternal":false,"url":"/core/services/librarians/evidence-based-acquisition"},{"id":"menu_item-1_cat-5_subCat-0_link-4","label":"ebook news & updates","openExternal":false,"url":"/core/services/librarians/ebook-news-and-updates"},{"id":"menu_item-1_cat-5_subCat-0_link-5","label":"Cambridge libraries of the world podcast","openExternal":false,"url":"/core/services/librarians/cambridge-libraries-of-the-world-podcast"},{"id":"menu_item-1_cat-5_subCat-0_link-6","label":"Purchasing models","openExternal":false,"url":"/core/services/librarians/purchasing-models"},{"id":"menu_item-1_cat-5_subCat-0_link-7","label":"Journals Publishing Updates","openExternal":false,"url":"/core/services/librarians/journals-publishing-updates"}]},{"id":"menu_item-1_cat-5_subCat-1","label":"Products","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-5_subCat-1_link-0","label":"Cambridge frontlist","openExternal":false,"url":"/core/services/librarians/cambridge-frontlist"},{"id":"menu_item-1_cat-5_subCat-1_link-1","label":"Cambridge journals digital archive","openExternal":false,"url":"/core/services/librarians/cambridge-journals-digital-archive"},{"id":"menu_item-1_cat-5_subCat-1_link-2","label":"Hot topics","openExternal":false,"url":"/core/services/librarians/hot-topics"},{"id":"menu_item-1_cat-5_subCat-1_link-3","label":"Other digital products","openExternal":false,"url":"/core/services/librarians/other-digital-products"},{"id":"menu_item-1_cat-5_subCat-1_link-4","label":"Perpetual access products","openExternal":false,"url":"/core/services/librarians/perpetual-access-products"},{"id":"menu_item-1_cat-5_subCat-1_link-5","label":"Price list","openExternal":false,"url":"/core/services/librarians/price-list"},{"id":"menu_item-1_cat-5_subCat-1_link-6","label":"Developing country programme","openExternal":false,"url":"/core/services/librarians/developing-country-programme"},{"id":"menu_item-1_cat-5_subCat-1_link-7","label":"New content","openExternal":false,"url":"/core/services/librarians/new-content"}]},{"id":"menu_item-1_cat-5_subCat-2","label":"Tools","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-5_subCat-2_link-0","label":"Eligibility checker","openExternal":false,"url":"/core/eligibility-checker"},{"id":"menu_item-1_cat-5_subCat-2_link-1","label":"Transformative agreements","openExternal":false,"url":"https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements"},{"id":"menu_item-1_cat-5_subCat-2_link-2","label":"KBART","openExternal":false,"url":"https://www.cambridge.org/core/services/librarians/kbart"},{"id":"menu_item-1_cat-5_subCat-2_link-3","label":"MARC records","openExternal":false,"url":"https://www.cambridge.org/core/services/librarians/marc-records"},{"id":"menu_item-1_cat-5_subCat-2_link-4","label":"Using MARCEdit for MARC records","openExternal":false,"url":"/core/services/librarians/using-marcedit-for-marc-records"},{"id":"menu_item-1_cat-5_subCat-2_link-5","label":"Inbound OpenURL specifications","openExternal":false,"url":"/core/services/librarians/inbound-openurl-specifications"},{"id":"menu_item-1_cat-5_subCat-2_link-6","label":"COUNTER report types","openExternal":false,"url":"/core/services/librarians/counter-report-types"}]},{"id":"menu_item-1_cat-5_subCat-3","label":"Resources","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-5_subCat-3_link-0","label":"Catalogues and resources","openExternal":false,"url":"/core/services/librarians/catalogues-and-resources"},{"id":"menu_item-1_cat-5_subCat-3_link-1","label":"Making the most of your EBA","openExternal":false,"url":"/core/services/librarians/making-the-most-of-your-eba"},{"id":"menu_item-1_cat-5_subCat-3_link-2","label":"Posters","openExternal":false,"url":"/core/services/librarians/posters"},{"id":"menu_item-1_cat-5_subCat-3_link-3","label":"Leaflets and brochures","openExternal":false,"url":"/core/services/librarians/leaflets-and-brochures"},{"id":"menu_item-1_cat-5_subCat-3_link-4","label":"Additional resources","openExternal":false,"url":"/core/services/librarians/additional-resources"},{"id":"menu_item-1_cat-5_subCat-3_link-5","label":"Find my sales contact","openExternal":false,"url":"/core/services/librarians/find-my-sales-contact"},{"id":"menu_item-1_cat-5_subCat-3_link-6","label":"Webinars","openExternal":false,"url":"/core/services/librarians/webinars"},{"id":"menu_item-1_cat-5_subCat-3_link-7","label":"Read and publish resources","openExternal":false,"url":"/core/services/librarians/read-and-publish-resources"}]}]},{"id":"menu_item-1_cat-6","label":"Peer review","openExternal":null,"url":"/core/services/peer-review/peer-review","menuTopics":[{"id":"menu_item-1_cat-6_subCat-0","label":"Peer review","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-6_subCat-0_link-0","label":"How to peer review journal articles","openExternal":false,"url":"/core/services/peer-review/how-to-peer-review-journal-articles"},{"id":"menu_item-1_cat-6_subCat-0_link-1","label":"How to peer review book proposals","openExternal":false,"url":"/core/services/peer-review/how-to-peer-review-book-proposals"},{"id":"menu_item-1_cat-6_subCat-0_link-2","label":"How to peer review Registered Reports","openExternal":false,"url":"/core/services/peer-review/how-to-peer-review-registered-reports"},{"id":"menu_item-1_cat-6_subCat-0_link-3","label":"Peer review FAQs","openExternal":false,"url":"/core/services/peer-review/peer-review-faqs"},{"id":"menu_item-1_cat-6_subCat-0_link-4","label":"Ethics in peer review","openExternal":false,"url":"/core/services/peer-review/ethics-in-peer-review"},{"id":"menu_item-1_cat-6_subCat-0_link-5","label":"Online peer review systems","openExternal":false,"url":"/core/services/peer-review/online-peer-review-systems"},{"id":"menu_item-1_cat-6_subCat-0_link-6","label":"A guide to Publons","openExternal":false,"url":"/core/services/peer-review/a-guide-to-publons"}]}]},{"id":"menu_item-1_cat-7","label":"Publishing ethics","openExternal":null,"url":"/core/services/publishing-ethics/publishing-ethics","menuTopics":[{"id":"menu_item-1_cat-7_subCat-0","label":"Journals ","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-7_subCat-0_link-0","label":"Publishing ethics guidelines for journals","openExternal":false,"url":"/core/services/publishing-ethics/publishing-ethics-guidelines-journals"},{"id":"menu_item-1_cat-7_subCat-0_link-1","label":"Core editorial policies for journals","openExternal":false,"url":"/core/services/publishing-ethics/core-editorial-policies-journals"},{"id":"menu_item-1_cat-7_subCat-0_link-2","label":"Authorship and contributorship for journals","openExternal":false,"url":"/core/services/publishing-ethics/authorship-and-contributorship-journals"},{"id":"menu_item-1_cat-7_subCat-0_link-3","label":"Affiliations for journals","openExternal":false,"url":"/core/services/publishing-ethics/affiliations-journals"},{"id":"menu_item-1_cat-7_subCat-0_link-4","label":"Research ethics for journals","openExternal":false,"url":"/core/services/publishing-ethics/research-ethics-journals"},{"id":"menu_item-1_cat-7_subCat-0_link-5","label":"Competing interests and funding for journals","openExternal":false,"url":"/core/services/publishing-ethics/competing-interests-and-funding-journals"}]},{"id":"menu_item-1_cat-7_subCat-1","label":"Journals (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-7_subCat-1_link-0","label":"Data and supporting evidence for journals","openExternal":false,"url":"/core/services/publishing-ethics/data-and-supporting-evidence-for-journals"},{"id":"menu_item-1_cat-7_subCat-1_link-1","label":"Misconduct for journals","openExternal":false,"url":"/core/services/publishing-ethics/misconduct-journals"},{"id":"menu_item-1_cat-7_subCat-1_link-2","label":"Corrections, retractions and removals for journals","openExternal":false,"url":"/core/services/publishing-ethics/corrections-retractions-and-removals-journals"},{"id":"menu_item-1_cat-7_subCat-1_link-3","label":"Versions and adaptations for journals","openExternal":false,"url":"/core/services/publishing-ethics/versions-and-adaptations-journals"},{"id":"menu_item-1_cat-7_subCat-1_link-4","label":"Libel, defamation and freedom of expression","openExternal":false,"url":"/core/services/publishing-ethics/libel-defamation-and-freedom-of-expression"},{"id":"menu_item-1_cat-7_subCat-1_link-5","label":"Business ethics journals","openExternal":false,"url":"/core/services/publishing-ethics/business-ethics-journals"}]},{"id":"menu_item-1_cat-7_subCat-2","label":"Books","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-7_subCat-2_link-0","label":"Publishing ethics guidelines for books","openExternal":false,"url":"/core/services/publishing-ethics/publishing-ethics-guidelines-books"},{"id":"menu_item-1_cat-7_subCat-2_link-1","label":"Core editorial policies for books","openExternal":false,"url":"/core/services/publishing-ethics/core-editorial-policies-books"},{"id":"menu_item-1_cat-7_subCat-2_link-2","label":"Authorship and contributorship for books","openExternal":false,"url":"/core/services/publishing-ethics/authorship-and-contributorship-books"},{"id":"menu_item-1_cat-7_subCat-2_link-3","label":"Affiliations for books","openExternal":false,"url":"/core/services/publishing-ethics/affiliations-books"},{"id":"menu_item-1_cat-7_subCat-2_link-4","label":"Research ethics for books","openExternal":false,"url":"/core/services/publishing-ethics/research-ethics-books"},{"id":"menu_item-1_cat-7_subCat-2_link-5","label":"Competing interests and funding for books","openExternal":false,"url":"/core/services/publishing-ethics/competing-interests-and-funding-books"}]},{"id":"menu_item-1_cat-7_subCat-3","label":"Books (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-7_subCat-3_link-0","label":"Data and supporting evidence for books","openExternal":false,"url":"/core/services/publishing-ethics/data-and-supporting-evidence-books"},{"id":"menu_item-1_cat-7_subCat-3_link-1","label":"Misconduct for books","openExternal":false,"url":"/core/services/publishing-ethics/misconduct-books"},{"id":"menu_item-1_cat-7_subCat-3_link-2","label":"Corrections, retractions and removals for books","openExternal":false,"url":"/core/services/publishing-ethics/corrections-retractions-and-removals-books"},{"id":"menu_item-1_cat-7_subCat-3_link-3","label":"Versions and adaptations for books","openExternal":false,"url":"/core/services/publishing-ethics/versions-and-adaptations-books"},{"id":"menu_item-1_cat-7_subCat-3_link-4","label":"Libel, defamation and freedom of expression","openExternal":false,"url":"/core/services/publishing-ethics/libel-defamation-and-freedom-of-expression"},{"id":"menu_item-1_cat-7_subCat-3_link-5","label":"Business ethics books","openExternal":false,"url":"/core/services/publishing-ethics/business-ethics-books"}]}]},{"id":"menu_item-1_cat-8","label":"Publishing partners","openExternal":null,"url":"/core/services/publishing-partners/publishing-partnerships","menuTopics":[{"id":"menu_item-1_cat-8_subCat-0","label":"Publishing partners","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-8_subCat-0_link-0","label":"Publishing partnerships","openExternal":false,"url":"/core/services/publishing-partners/publishing-partnerships"},{"id":"menu_item-1_cat-8_subCat-0_link-1","label":"Partner books","openExternal":false,"url":"/core/services/publishing-partners/partner-books"},{"id":"menu_item-1_cat-8_subCat-0_link-2","label":"eBook publishing partnerships","openExternal":false,"url":"/core/services/publishing-partners/ebook-publishing-partnerships"},{"id":"menu_item-1_cat-8_subCat-0_link-3","label":"Journal publishing partnerships","openExternal":false,"url":"/core/services/publishing-partners/journal-publishing-partnerships"}]},{"id":"menu_item-1_cat-8_subCat-1","label":"Publishing partners (cont.)","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-1_cat-8_subCat-1_link-0","label":"Journals publishing","openExternal":false,"url":"/core/services/publishing-partners/journals-publishing"},{"id":"menu_item-1_cat-8_subCat-1_link-1","label":"Customer support","openExternal":false,"url":"/core/services/publishing-partners/customer-support"},{"id":"menu_item-1_cat-8_subCat-1_link-2","label":"Membership Services","openExternal":false,"url":"/core/services/publishing-partners/membership-services"},{"id":"menu_item-1_cat-8_subCat-1_link-3","label":"Our Team","openExternal":false,"url":"/core/services/publishing-partners/our-team"}]}]}]},{"id":"menu_item-2","label":"Open research","openExternal":null,"url":"","isMegaMenu":true,"menuCats":[{"id":"menu_item-2_cat-0","label":"Open access policies","openExternal":null,"url":"/core/services/open-research-policies/open-access-policies","menuTopics":[{"id":"menu_item-2_cat-0_subCat-0","label":"Open access policies","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-0_subCat-0_link-0","label":"Open research","openExternal":false,"url":"/core/services/open-research-policies/open-research"},{"id":"menu_item-2_cat-0_subCat-0_link-1","label":"Open access policies","openExternal":false,"url":"/core/services/open-research-policies/open-access-policies"},{"id":"menu_item-2_cat-0_subCat-0_link-2","label":"Cambridge University Press and Plan S","openExternal":false,"url":"/core/services/open-research-policies/cambridge-university-press-and-plan-s"},{"id":"menu_item-2_cat-0_subCat-0_link-3","label":"Text and data mining","openExternal":false,"url":"/core/services/open-research-policies/text-and-data-mining"},{"id":"menu_item-2_cat-0_subCat-0_link-4","label":"Preprint policy","openExternal":false,"url":"/core/services/open-research-policies/preprint-policy"},{"id":"menu_item-2_cat-0_subCat-0_link-5","label":"Social sharing","openExternal":false,"url":"/core/services/open-research-policies/social-sharing"}]},{"id":"menu_item-2_cat-0_subCat-1","label":"Journals","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-0_subCat-1_link-0","label":"Open access journals","openExternal":false,"url":"/core/services/open-research-policies/open-access-journals"},{"id":"menu_item-2_cat-0_subCat-1_link-1","label":"Gold Open Access journals","openExternal":false,"url":"/core/services/open-research-policies/gold-open-access-journals"},{"id":"menu_item-2_cat-0_subCat-1_link-2","label":"Transformative journals","openExternal":false,"url":"/core/services/open-research-policies/transformative-journals"},{"id":"menu_item-2_cat-0_subCat-1_link-3","label":"Green Open Access policy for journals","openExternal":false,"url":"/core/services/open-research-policies/green-open-access-policy-for-journals"},{"id":"menu_item-2_cat-0_subCat-1_link-4","label":"Transparent pricing policy for journals","openExternal":false,"url":"/core/services/open-research-policies/transparent-pricing-policy-for-journals"}]},{"id":"menu_item-2_cat-0_subCat-2","label":"Books and Elements","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-0_subCat-2_link-0","label":"Open access books","openExternal":false,"url":"/core/services/open-research-policies/open-access-books"},{"id":"menu_item-2_cat-0_subCat-2_link-1","label":"Gold open access books","openExternal":false,"url":"/core/services/open-research-policies/gold-open-access-books"},{"id":"menu_item-2_cat-0_subCat-2_link-2","label":"Green Open Access policy for books","openExternal":false,"url":"/core/services/open-research-policies/green-open-access-policy-for-books"},{"id":"menu_item-2_cat-0_subCat-2_link-3","label":"Open access Elements","openExternal":false,"url":"/core/services/open-research-policies/open-access-elements"}]}]},{"id":"menu_item-2_cat-1","label":"Open access publishing","openExternal":null,"url":"/core/services/open-access-publishing/open-access","menuTopics":[{"id":"menu_item-2_cat-1_subCat-0","label":"About open access","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-1_subCat-0_link-0","label":"Open research","openExternal":false,"url":"/core/services/open-access-publishing/open-research"},{"id":"menu_item-2_cat-1_subCat-0_link-1","label":"Open Access Week","openExternal":false,"url":"/core/services/open-access-publishing/open-access-week"},{"id":"menu_item-2_cat-1_subCat-0_link-2","label":"What is open access?","openExternal":false,"url":"/core/services/open-access-publishing/open-access"},{"id":"menu_item-2_cat-1_subCat-0_link-3","label":"Open access glossary","openExternal":false,"url":"/core/services/open-access-publishing/open-access-glossary"},{"id":"menu_item-2_cat-1_subCat-0_link-4","label":"Open access myths","openExternal":false,"url":"/core/services/open-access-publishing/open-access-myths"},{"id":"menu_item-2_cat-1_subCat-0_link-5","label":"Hybrid Open Access FAQs","openExternal":false,"url":"/core/services/open-access-publishing/hybrid-open-access-faqs"},{"id":"menu_item-2_cat-1_subCat-0_link-6","label":"Eligibility checker","openExternal":false,"url":"/core/eligibility-checker"}]},{"id":"menu_item-2_cat-1_subCat-1","label":"Open access resources","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-1_subCat-1_link-0","label":"Open access resources","openExternal":false,"url":"/core/services/open-access-publishing/open-access-resources"},{"id":"menu_item-2_cat-1_subCat-1_link-1","label":"Benefits of open access","openExternal":false,"url":"/core/services/open-access-publishing/benefits-of-open-access"},{"id":"menu_item-2_cat-1_subCat-1_link-2","label":"Creative commons licences","openExternal":false,"url":"/core/services/open-access-publishing/creative-commons-licenses"},{"id":"menu_item-2_cat-1_subCat-1_link-3","label":"Funder policies and mandates","openExternal":false,"url":"/core/services/open-access-publishing/funder-policies-and-mandates"},{"id":"menu_item-2_cat-1_subCat-1_link-4","label":"Article type definitions","openExternal":false,"url":"/core/services/open-access-publishing/article-type-definitions"},{"id":"menu_item-2_cat-1_subCat-1_link-5","label":"Convert your article to Open Access","openExternal":false,"url":"/core/services/open-access-publishing/convert-your-article-to-open-access"},{"id":"menu_item-2_cat-1_subCat-1_link-6","label":"Open access video resources","openExternal":false,"url":"/core/services/open-access-publishing/open-access-video-resources"}]}]},{"id":"menu_item-2_cat-2","label":"Open research initiatives","openExternal":null,"url":"","menuTopics":[{"id":"menu_item-2_cat-2_subCat-0","label":"Research transparency","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-2_subCat-0_link-0","label":"Transparency and openness","openExternal":false,"url":"/core/services/open-research-initiatives/transparency-and-openness"},{"id":"menu_item-2_cat-2_subCat-0_link-1","label":"Open Practice Badges","openExternal":false,"url":"/core/services/open-research-initiatives/open-practice-badges"},{"id":"menu_item-2_cat-2_subCat-0_link-2","label":"OA organisations, initiatives & directories","openExternal":false,"url":"/core/services/open-research-initiatives/oa-organisations-initiatives-and-directories"},{"id":"menu_item-2_cat-2_subCat-0_link-3","label":"Registered Reports","openExternal":false,"url":"/core/services/open-research-initiatives/registered-reports"},{"id":"menu_item-2_cat-2_subCat-0_link-4","label":"Annotation for Transparent Inquiry (ATI)","openExternal":false,"url":"/core/services/open-research-initiatives/annotation-for-transparent-inquiry-ati"}]},{"id":"menu_item-2_cat-2_subCat-1","label":"Journal flips","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-2_subCat-1_link-0","label":"Open access journal flips","openExternal":false,"url":"/core/services/open-research-initiatives/open-access-journal-flips"},{"id":"menu_item-2_cat-2_subCat-1_link-1","label":"OA Journal Flip FAQs","openExternal":false,"url":"/core/services/open-research-initiatives/oa-journal-flip-faqs"}]},{"id":"menu_item-2_cat-2_subCat-2","label":"Flip it Open","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-2_subCat-2_link-0","label":"Flip it Open","openExternal":false,"url":"/core/services/open-research-initiatives/flip-it-open"},{"id":"menu_item-2_cat-2_subCat-2_link-1","label":"Flip it Open FAQs","openExternal":false,"url":"/core/services/open-research-initiatives/flip-it-open-faqs"}]}]},{"id":"menu_item-2_cat-3","label":"Open access funding","openExternal":null,"url":"/core/services/open-access-funding/funding-open-access-publication","menuTopics":[{"id":"menu_item-2_cat-3_subCat-0","label":"Open access funding","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-3_subCat-0_link-0","label":"Funding open access publication","openExternal":false,"url":"/core/services/open-access-funding/funding-open-access-publication"},{"id":"menu_item-2_cat-3_subCat-0_link-1","label":"Cambridge Open Equity Initiative","openExternal":false,"url":"/core/services/open-access-funding/cambridge-open-equity-initiative"},{"id":"menu_item-2_cat-3_subCat-0_link-2","label":"Completing a RightsLink (open access) transaction","openExternal":false,"url":"/core/services/open-access-funding/completing-a-rightslink-open-access-transaction"}]}]},{"id":"menu_item-2_cat-4","label":" Cambridge Open Engage","openExternal":null,"url":"/core/services/cambridge-open-engage/cambridge-open-engage","menuTopics":[{"id":"menu_item-2_cat-4_subCat-0","label":"  Cambridge Open Engage","openExternal":null,"url":"","menuLinks":[{"id":"menu_item-2_cat-4_subCat-0_link-0","label":"Cambridge Open Engage","openExternal":false,"url":"/core/services/cambridge-open-engage/cambridge-open-engage"},{"id":"menu_item-2_cat-4_subCat-0_link-1","label":"Partner With Us","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-partner-with-us"},{"id":"menu_item-2_cat-4_subCat-0_link-2","label":"Branded Hubs","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-branded-hubs"},{"id":"menu_item-2_cat-4_subCat-0_link-3","label":"Event Workspaces","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-event-workspaces"},{"id":"menu_item-2_cat-4_subCat-0_link-4","label":"Partner Resources","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-partner-resources"},{"id":"menu_item-2_cat-4_subCat-0_link-5","label":"APSA Preprints","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-apsa-preprints"},{"id":"menu_item-2_cat-4_subCat-0_link-6","label":"APSA Preprints FAQs","openExternal":false,"url":"/core/services/cambridge-open-engage/engage-apsa-preprints-faqs"}]}]}]}]}}</script>
    </div>
    <platform-header
      id='platform-header-wc'
      platform='core'
      env='prod'
      institution-log-in-url='https://shibboleth.cambridge.org/CJOShibb2/index?app&#x3D;https%3A%2F%2Fwww.cambridge.org%2Fcore%2Fshibboleth%3Fref%3D%2Fcore%2Fjournals%2Fnatural-language-processing%2Farticle%2Ftopic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic%2F0FFF33B18E284DAB8FE8DCF69A963A30'
      manage-account-url='/core/login'
      base-url='/core/services/'
      class='platform-header-wc'
      search-phrase=''
      is-preview='false'
      hide-search='false'
      style="display: none"
    ></platform-header>
    
    <script>
        const platformHeader = $('#platform-header');
        const platformHeaderWc = $('#platform-header-wc');
        platformHeaderWc.prop('initialData', window.__PLATFORM_HEADER_DATA__);
        platformHeaderWc.on('initialized', function () {
          platformHeader.hide();
          platformHeaderWc.show();
        });
    
        const createForm = function (searchPhrase) {
          const form = document.createElement('form');
          form.style.display = 'none';
          form.method = 'GET';
          form.action = '/core/search';
    
          const input = document.createElement('input');
          input.type = 'hidden';
          input.name = 'q';
          input.value = searchPhrase;
    
          form.appendChild(input);
          return form;
        };
    
        platformHeaderWc.on('search', function (e) {
          const searchPhrase = e?.originalEvent?.detail[0]?.phrase;
    
          if (!searchPhrase) {
            return;
          }
    
          const form = createForm(searchPhrase);
          document.body.appendChild(form);
          $('form').submit();
    
          document.body.removeChild(form);
        });
    </script><div class="flash-message-container" role="alert">
    <div class="flash-message">
        
        
        
        
        <script>
          var AOP = AOP || {};
          
          $( document ).ready(function() {
        
            if (AOP.onLoadfocusTriggered !== true) {
              if($(".alert-box").length) {
                AOP.enableKeyboardAccess($(".alert-box"));
              }
              AOP.onLoadfocusTriggered = true; 
            }
        });
          
        </script>          <div id="ajaxMessages" class="ajaxMessages"></div>
    </div>
</div>
<div class="article-wrapper">
  <link rel="preload" href="/core/page-component/06cd607.js" as="script"><link rel="preload" href="/core/page-component/20147ec.js" as="script"><link rel="preload" href="/core/page-component/css/styles.css?hash=5ecdf40" as="style"><link rel="preload" href="/core/page-component/66884fb.js" as="script"><link rel="preload" href="/core/page-component/css/app.css?hash=f1e6144" as="style"><link rel="preload" href="/core/page-component/6f4302c.js" as="script"><link rel="stylesheet" href="/core/page-component/css/styles.css?hash=5ecdf40"><link rel="stylesheet" href="/core/page-component/css/app.css?hash=f1e6144">
<div role="main" class="page-component">
  <div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div><div data-v-01274b1d><div style="display: none;" data-v-01274b1d>
    Hostname: page-component-6bf8c574d5-r8w4l
    Total loading time: 0
    Render date: 2025-03-03T21:28:34.214Z
    Has data issue: false
    hasContentIssue false
  </div> <div class="container-fluid breadcrumbs-wrapper" data-v-3692cf84 data-v-01274b1d><div class="row crumbs-row" data-v-3692cf84><div class="breadcrumbs-wrapper__list" data-v-3692cf84><div class="breadcrumbs-wrapper__list__wrapper" data-v-3692cf84><!----> <div role="navigation" class="breadcrumbs-wrapper__list__wrapper__crumbs" data-v-3692cf84><ul data-test-id="breadcrumbs" class="page-breadcrumbs" data-v-3692cf84><li class="page-breadcrumbs__item" data-v-3692cf84><!----><a href="/core" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-3692cf84><!----><span data-v-63dfaf6e>Home</span> <!----></a></li><li class="page-breadcrumbs__item" data-v-3692cf84><span aria-hidden="true" class="breadcrumbs-wrapper__arrow" data-v-3692cf84>&gt;</span><a href="/core/publications/journals" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-3692cf84><!----><span data-v-63dfaf6e>Journals</span> <!----></a></li><li class="page-breadcrumbs__item" data-v-3692cf84><span aria-hidden="true" class="breadcrumbs-wrapper__arrow" data-v-3692cf84>&gt;</span><a href="/core/journals/natural-language-processing" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-3692cf84><!----><span data-v-63dfaf6e>Natural Language Processing</span> <!----></a></li><li class="page-breadcrumbs__item" data-v-3692cf84><span aria-hidden="true" class="breadcrumbs-wrapper__arrow" data-v-3692cf84>&gt;</span><a href="/core/journals/natural-language-processing/firstview" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-3692cf84><!----><span data-v-63dfaf6e>FirstView</span> <!----></a></li><li class="page-breadcrumbs__item" data-v-3692cf84><span aria-hidden="true" class="breadcrumbs-wrapper__arrow" data-v-3692cf84>&gt;</span><span data-v-3692cf84>Topic aware probing: From sentence length prediction...</span></li></ul></div></div> <div class="language" data-v-3692cf84><ul class="language-switch" data-v-6b1118dd data-v-3692cf84><li aria-label="English" data-v-6b1118dd><span class="language-option current divider" data-v-6b1118dd>English</span></li><li aria-label="Français" data-v-6b1118dd><span role="button" tabindex="0" href="#" lang="fr" class="language-option" data-v-6b1118dd>
      Français
    </span></li></ul></div></div></div></div> <div class="container container__modified" data-v-01274b1d><!----> <!----> <div class="row" data-v-01274b1d><div role="complementary" aria-label="table of content" class="column__left" data-v-01274b1d><div class="col journal-container row" data-v-146270e8 data-v-01274b1d><img src="https://static.cambridge.org/covers/NLP_0_0_0/natural-language-processing.jpg" alt="" class="journal__image" data-v-146270e8> <a href="/core/journals/natural-language-processing" class="app-link journal__title app-link__text app-link--underlined" data-v-63dfaf6e data-v-146270e8><!----><span class="text" data-v-63dfaf6e>Natural Language Processing
    <!----></span> <!----></a> <hr aria-hidden="true" class="separator default" data-v-7036083a data-v-146270e8></div> <!----> <div id="toc" class="table-of-content" data-v-01274b1d><h2>Article contents</h2> <div id="toc-list-wrapper" class="table-of-content__wrapper"><ul id="toc-list" class="list"><li class="list__item"><a href="#sec0" class="list__item__link"><span class="toc-title">Abstract</span></a></li> <li class="list__item"><a href="#s1" class="list__item__link"><!----> <span><div class="toc-title">Introduction</div></span></a></li><li class="list__item"><a href="#s2" class="list__item__link"><!----> <span><div class="toc-title">Related work</div></span></a></li><li class="list__item"><a href="#s3" class="list__item__link"><!----> <span><div class="toc-title">Topic-aware probing</div></span></a></li><li class="list__item"><a href="#s4" class="list__item__link"><!----> <span><div class="toc-title">Experimentation</div></span></a></li><li class="list__item"><a href="#s5" class="list__item__link"><!----> <span><div class="toc-title">Results and discussion</div></span></a></li><li class="list__item"><a href="#s6" class="list__item__link"><!----> <span><div class="toc-title">Other probing tasks</div></span></a></li><li class="list__item"><a href="#s7" class="list__item__link"><!----> <span><div class="toc-title">Conclusions</div></span></a></li><li class="list__item"><a href="#s50" class="list__item__link"><!----> <span><div class="toc-title">Competing interests</div></span></a></li> <li class="list__item"><a href="#footnotes-list" class="list__item__link"><span class="toc-title">Footnotes</span></a></li> <li class="list__item"><a href="#references-list" class="list__item__link"><span class="toc-title">References</span></a></li></ul></div></div></div> <div class="column__main" data-v-01274b1d><div class="row" data-v-01274b1d><div class="column__main__left" data-v-01274b1d><div id="maincontent" class="col" data-v-862424e6 data-v-01274b1d><!----> <hgroup data-v-862424e6><h1 data-v-862424e6>Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?</h1> <!----></hgroup> <!----> <!----> <!----> <!----> <div class="row published-date" data-v-862424e6><p data-v-862424e6>
      Published online by Cambridge University Press: 
      <strong data-v-862424e6>25 October 2024</strong></p></div> <!----> <!----> <div class="contributors-details" data-v-99f6eb26 data-v-862424e6><div class="row contributors" data-v-99f6eb26><div class="col" data-v-99f6eb26><div class="row contributor-type" data-v-792406ce data-v-99f6eb26><!----> <div class="contributor-type__contributor" data-v-792406ce><a href="/core/search?filters%5BauthorTerms%5D=Vasudevan%20Nedumpozhimana&amp;eventCode=SE-AU" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-792406ce><!----><span data-v-63dfaf6e>Vasudevan Nedumpozhimana</span> <!----></a> <a target="_blank" href="https://orcid.org/0000-0001-5161-8925" data-test-orcid="Vasudevan Nedumpozhimana" class="app-link contributor-type__contributor__orcid app-link__icon app-link--" data-v-63dfaf6e data-v-792406ce><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPHN2ZyB3aWR0aD0iMjIiIGhlaWdodD0iMjIiIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4IgoJIHZpZXdCb3g9IjAgMCAyNTYgMjU2IiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCAyNTYgMjU2OyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+CjxzdHlsZSB0eXBlPSJ0ZXh0L2NzcyI+Cgkuc3Qwe2ZpbGw6I0E2Q0UzOTt9Cgkuc3Qxe2ZpbGw6I0ZGRkZGRjt9Cjwvc3R5bGU+CjxwYXRoIGNsYXNzPSJzdDAiIGQ9Ik0yNTYsMTI4YzAsNzAuNy01Ny4zLDEyOC0xMjgsMTI4QzU3LjMsMjU2LDAsMTk4LjcsMCwxMjhDMCw1Ny4zLDU3LjMsMCwxMjgsMEMxOTguNywwLDI1Niw1Ny4zLDI1NiwxMjh6Ii8+CjxnPgoJPHBhdGggY2xhc3M9InN0MSIgZD0iTTg2LjMsMTg2LjJINzAuOVY3OS4xaDE1LjR2NDguNFYxODYuMnoiLz4KCTxwYXRoIGNsYXNzPSJzdDEiIGQ9Ik0xMDguOSw3OS4xaDQxLjZjMzkuNiwwLDU3LDI4LjMsNTcsNTMuNmMwLDI3LjUtMjEuNSw1My42LTU2LjgsNTMuNmgtNDEuOFY3OS4xeiBNMTI0LjMsMTcyLjRoMjQuNQoJCWMzNC45LDAsNDIuOS0yNi41LDQyLjktMzkuN2MwLTIxLjUtMTMuNy0zOS43LTQzLjctMzkuN2gtMjMuN1YxNzIuNHoiLz4KCTxwYXRoIGNsYXNzPSJzdDEiIGQ9Ik04OC43LDU2LjhjMCw1LjUtNC41LDEwLjEtMTAuMSwxMC4xYy01LjYsMC0xMC4xLTQuNi0xMC4xLTEwLjFjMC01LjYsNC41LTEwLjEsMTAuMS0xMC4xCgkJQzg0LjIsNDYuNyw4OC43LDUxLjMsODguNyw1Ni44eiIvPgo8L2c+Cjwvc3ZnPgo=" alt="Open the ORCID record for Vasudevan Nedumpozhimana" class="app-icon icon orcid" data-v-d2c09870 data-v-63dfaf6e><!----> <span class="sr-only" data-v-63dfaf6e>[Opens in a new window]</span></a> <span data-v-792406ce>&nbsp;and</span></div><div class="contributor-type__contributor" data-v-792406ce><a href="/core/search?filters%5BauthorTerms%5D=John%20D.%20Kelleher&amp;eventCode=SE-AU" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-792406ce><!----><span data-v-63dfaf6e>John D. Kelleher</span> <!----></a> <!----> <span data-v-792406ce></span></div> <!----></div> <!----></div> <div class="col-2 collapse-link" data-v-99f6eb26><a href="#authors-details" data-toggle="collapse" aria-expanded="false" aria-controls="authors-details" class="app-link collapsed app-link__text-icon app-link--secondary reverse" data-v-63dfaf6e data-v-99f6eb26><img src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTEiIGhlaWdodD0iNiIgdmlld0JveD0iMCAwIDExIDYiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNNS41MDAwNiA2QzUuMzI4NDYgNiA1LjE2Mzk4IDUuOTMzMzkgNS4wNDI1MiA1LjgxNTAxTDAuMTg5NDQ4IDEuMDc3OEMtMC4wNjMxNzYzIDAuODMxMjU3IC0wLjA2MzE3NjMgMC40MzE0NTIgMC4xODk2MSAwLjE4NDkwOEMwLjQ0MjM5NiAtMC4wNjE2MzYgMC44NTIwNjIgLTAuMDYxNjM2IDEuMTA0NTIgMC4xODQ5MDhMNS41MDAwNiA0LjQ3NTc1TDkuODk1NiAwLjE4NDkwOEMxMC4xNDgyIC0wLjA2MTYzNiAxMC41NTc5IC0wLjA2MTYzNiAxMC44MTA1IDAuMTg0OTA4QzExLjA2MzEgMC40MzE0NTIgMTEuMDYzMSAwLjgzMTEgMTAuODEwNyAxLjA3NzhMNS45NTc2IDUuODE1MDFDNS44MzYxNCA1LjkzMzM5IDUuNjcxNjYgNiA1LjUwMDA2IDZaIiBmaWxsPSIjNzA3MDcwIi8+Cjwvc3ZnPgo=" alt="" class="app-icon icon arrow-down" data-v-d2c09870 data-v-63dfaf6e><span class="text" data-v-63dfaf6e>Show author details
    <!----></span> <!----></a></div></div> <hr aria-hidden="true" class="separator default" data-v-7036083a data-v-99f6eb26> <dl id="authors-details" class="authors-details collapse" data-v-2edb8da6 data-v-99f6eb26><div data-test-author="Vasudevan Nedumpozhimana" class="row author" data-v-2edb8da6><dt class="col-12 col-sm-2 title" data-v-2edb8da6>Vasudevan Nedumpozhimana*</dt> <dd class="col content d-inline d-sm-flex" data-v-2edb8da6><span class="content__title" data-v-2edb8da6>Affiliation:</span> <div class="d-sm-flex flex-column flex-sm-1 d-inline" data-v-2edb8da6><span data-v-2edb8da6><span data-v-2edb8da6>ADAPT Research Centre, Technological University Dublin, Dublin, Ireland</span>
          </span></div></dd></div><div data-test-author="John D. Kelleher" class="row author" data-v-2edb8da6><dt class="col-12 col-sm-2 title" data-v-2edb8da6>John D. Kelleher</dt> <dd class="col content d-inline d-sm-flex" data-v-2edb8da6><span class="content__title" data-v-2edb8da6>Affiliation:</span> <div class="d-sm-flex flex-column flex-sm-1 d-inline" data-v-2edb8da6><span data-v-2edb8da6><span data-v-2edb8da6>ADAPT Research Centre, School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland</span>
          </span></div></dd></div> <div class="row" data-v-2edb8da6><dt class="col-sm-2 col-12 title" data-v-2edb8da6>
      *
    </dt> <dd class="col content" data-v-2edb8da6><div class="row" data-v-2edb8da6><div class="d-sm-flex d-inline flex-sm-1 flex-sm-wrap" data-v-2edb8da6><div class="d-inline" data-v-2edb8da6><span data-v-2edb8da6><div class="corresp"><span class="bold">Corresponding author:</span> Vasudevan Nedumpozhimana; Email: <a href="mailto:vasudevan.nedumpozhimana@tudublin.ie">vasudevan.nedumpozhimana@tudublin.ie</a></div></span></div></div></div></dd></div> <hr aria-hidden="true" class="separator default" data-v-7036083a data-v-2edb8da6></dl></div></div> <div id="app-tabs" class="tabs" data-v-1d90c6ce data-v-01274b1d><div id="app-tabs-wrapper" class="tabs__wrapper" data-v-1d90c6ce><div role="navigation" aria-label="tab navigation" class="container" data-v-1d90c6ce><a data-toggle="collapse" href="#appTabs" role="button" aria-expanded="false" aria-controls="appTabs" class="tabs__collapse collapsed d-sm-none d-print-none" data-v-1d90c6ce><span data-v-1d90c6ce></span> <span class="tabs-arrow-up" data-v-1d90c6ce></span> <span class="tabs-arrow-down" data-v-1d90c6ce></span></a> <ul id="appTabs" role="tablist" class="nav nav-tabs tabs__list collapse show" data-v-1d90c6ce><li role="none" class="tabs__tab" data-v-1d90c6ce><a aria-selected="true" href="#article-tab" role="tab" aria-controls="article-tab" tabindex="-1" class="active" data-v-1d90c6ce>Article
          </a></li><li role="none" class="tabs__tab" data-v-1d90c6ce><a href="#figures-tab" role="tab" aria-controls="figures-tab" tabindex="-1" data-v-1d90c6ce>Figures
          </a></li><li role="none" class="tabs__tab" data-v-1d90c6ce><a href="#metrics-tab" role="tab" aria-controls="metrics-tab" tabindex="-1" data-v-1d90c6ce>Metrics
          </a></li></ul></div></div> <div tabindex="0" role="region" aria-label="Tab content" class="tab-content" data-v-1d90c6ce><div id="article-tab" class="tab-pane active" data-v-43a4d572><div id="toc-mobile" class="table-of-content-mobile d-sm-none" data-v-43a4d572><a data-toggle="collapse" href="#toc-list-mobile" role="button" aria-expanded="false" class="toc-collapse-switch collapsed"><span class="default-heading">Article contents</span> <!----> <span class="toc-arrow-up"></span> <span class="toc-arrow-down"></span></a> <ul id="toc-list-mobile" class="list collapse"><li class="list__item"><a href="#sec0" class="list__item__link"><span class="toc-title">Abstract</span></a></li> <li class="list__item"><a href="#s1" class="list__item__link"><!----> <span>Introduction</span></a></li><li class="list__item"><a href="#s2" class="list__item__link"><!----> <span>Related work</span></a></li><li class="list__item"><a href="#s3" class="list__item__link"><!----> <span>Topic-aware probing</span></a></li><li class="list__item"><a href="#s4" class="list__item__link"><!----> <span>Experimentation</span></a></li><li class="list__item"><a href="#s5" class="list__item__link"><!----> <span>Results and discussion</span></a></li><li class="list__item"><a href="#s6" class="list__item__link"><!----> <span>Other probing tasks</span></a></li><li class="list__item"><a href="#s7" class="list__item__link"><!----> <span>Conclusions</span></a></li><li class="list__item"><a href="#s50" class="list__item__link"><!----> <span>Competing interests</span></a></li> <li class="list__item"><a href="#footnotes-list" class="list__item__link"><span class="toc-title">Footnotes</span></a></li> <li class="list__item"><a href="#references-list" class="list__item__link"><span class="toc-title">References</span></a></li></ul></div> <div class="action-bar" data-v-43a4d572><div class="row items"><!----> <div class="app-dropdown d-print-none" data-v-fab090b8 data-v-113567da data-v-43a4d572><button aria-expanded="false" data-test-id="buttonSavePDFOptions" id="save-pdf-dropdown" class="app-button dropdown-menu-button app-button__text-icon app-button--secondary" data-v-2a038744 data-v-fab090b8><img src="/core/page-component/img/save-pdf-icon.080470e.svg" alt="" class="app-icon icon save-pdf" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Save PDF</span></button> <div aria-labelledby="save-pdf-dropdown" class="app-dropdown__menu" style="display:none;" data-v-fab090b8><div class="pdf-buttons" data-v-fab090b8 data-v-113567da><a href="/core/services/aop-cambridge-core/content/view/0FFF33B18E284DAB8FE8DCF69A963A30/S2977042424000438a.pdf/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic.pdf" download="" role="link" class="app-link dropdown-item app-link__text-icon app-link--" data-v-63dfaf6e data-v-113567da><img src="/core/page-component/img/pdf-download-icon.c7fb40c.svg" alt="" class="app-icon icon pdf-download" data-v-d2c09870 data-v-63dfaf6e><span class="text" data-v-63dfaf6e>Save PDF (1 mb)
    <!----></span> <!----></a> <a target="_blank" href="/core/services/aop-cambridge-core/content/view/0FFF33B18E284DAB8FE8DCF69A963A30/S2977042424000438a.pdf/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic.pdf" role="link" class="app-link dropdown-item app-link__text-icon app-link--" data-v-63dfaf6e data-v-113567da><img src="/core/page-component/img/pdf-download-icon.c7fb40c.svg" alt="" class="app-icon icon pdf-download" data-v-d2c09870 data-v-63dfaf6e><span class="text" data-v-63dfaf6e>View PDF
    <!----></span> <span class="sr-only" data-v-63dfaf6e>[Opens in a new window]</span></a> <!----> <!----> <button aria-expanded="false" data-reveal-id="dropboxModal" role="button" class="app-button dropdown-item app-button__text-icon app-button--" data-v-2a038744 data-v-113567da><img src="/core/page-component/img/dropbox-icon.3d57046.svg" alt="" class="app-icon icon dropbox" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Save to Dropbox</span></button> <button aria-expanded="false" data-reveal-id="googleDriveModal" role="button" class="app-button dropdown-item app-button__text-icon app-button--" data-v-2a038744 data-v-113567da><img src="/core/page-component/img/google-drive-icon.a50193b.svg" alt="" class="app-icon icon google-drive" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Save to Google Drive</span></button> <button aria-expanded="false" data-reveal-id="kindleModal" data-location="landingPage" role="button" class="app-button dropdown-item app-button__text-icon app-button--" data-v-2a038744 data-v-113567da><img src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzRweCIgaGVpZ2h0PSIzNHB4IiB2aWV3Qm94PSIwIDAgMzQgMzQiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8dGl0bGU+REI4RjgzNzMtNDExMS00OTNCLUI0QzItQkY5MTYxMENBQ0MxPC90aXRsZT4KICAgIDxnIGlkPSJTdHlsZXNoZWV0IiBzdHJva2U9Im5vbmUiIHN0cm9rZS13aWR0aD0iMSIgZmlsbD0ibm9uZSIgZmlsbC1ydWxlPSJldmVub2RkIj4KICAgICAgICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMTIxOC4wMDAwMDAsIC03OTYuMDAwMDAwKSIgaWQ9IkJVVFRPTlMiPgogICAgICAgICAgICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMDAuMDAwMDAwLCA1OTcuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8ZyBpZD0iS2luZGxlLWljb24iIHRyYW5zZm9ybT0idHJhbnNsYXRlKDExMTkuMDAwMDAwLCAyMDAuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICAgICAgPGNpcmNsZSBpZD0iT3ZhbCIgc3Ryb2tlPSIjNTk1OTU5IiBjeD0iMTYiIGN5PSIxNiIgcj0iMTYiPjwvY2lyY2xlPgogICAgICAgICAgICAgICAgICAgIDxwb2x5Z29uIGlkPSJGaWxsLTEiIGZpbGw9IiM1OTU5NTkiIHBvaW50cz0iMTAgNCAxMCAyOCAxMi41MjI4OTY3IDI4IDEyLjUyMjg5NjcgMjAuMzY3MzQ2NCAxOS4yMTg0MTczIDI4IDIyLjU3NDE3MyAyOCAxNC42NDkzNTI1IDE4Ljk2NjQ3NzQgMjEuNTc3NzQwNSAxMi4wNDAwODgzIDE4LjE0MDM2NDUgMTEuOTA4ODI5NyAxMi41MjI4OTY3IDE3LjUyNTYzMTIgMTIuNTIyODk2NyA0Ij48L3BvbHlnb24+CiAgICAgICAgICAgICAgICA8L2c+CiAgICAgICAgICAgIDwvZz4KICAgICAgICA8L2c+CiAgICA8L2c+Cjwvc3ZnPg==" alt="" class="app-icon icon kindle" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Save to Kindle</span></button> <!----></div> <button aria-label="Close Save PDF dropdown" aria-expanded="false" class="app-button close-icon app-button__icon app-button--" data-v-2a038744 data-v-fab090b8><img src="/core/page-component/img/close-icon.194b28a.svg" alt="" class="app-icon icon close" data-v-d2c09870 data-v-2a038744> <!----></button></div></div> <div class="app-dropdown d-print-none share-dropdown" data-v-fab090b8 data-v-beffd87c data-v-43a4d572><button aria-expanded="false" data-test-id="buttonShareOptions" id="share-dropdown" class="app-button dropdown-menu-button app-button__text-icon app-button--secondary" data-v-2a038744 data-v-fab090b8><img src="/core/page-component/img/share-icon.cbcfad8.svg" alt="" class="app-icon icon share" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Share</span></button> <div aria-labelledby="share-dropdown" class="app-dropdown__menu" style="display:none;" data-v-fab090b8><div aria-labelledby="dropdown-share-button" data-v-fab090b8 data-v-beffd87c><!----> <div class="social-share-container" data-v-fab090b8 data-v-beffd87c><!----></div></div> <button aria-label="Close Share dropdown" aria-expanded="false" class="app-button close-icon app-button__icon app-button--" data-v-2a038744 data-v-fab090b8><img src="/core/page-component/img/close-icon.194b28a.svg" alt="" class="app-icon icon close" data-v-d2c09870 data-v-2a038744> <!----></button></div></div> <button aria-expanded="false" data-test-id="buttonCiteOptions" data-prod-id="0FFF33B18E284DAB8FE8DCF69A963A30" data-toggle="modal" href="#cite-modal" class="app-button cite-button d-print-none app-button__text-icon app-button--secondary export-citation-product" data-v-2a038744 data-v-2ce5f61d data-v-43a4d572><img src="/core/page-component/img/cite-icon.44eaaa4.svg" alt="" class="app-icon icon cite" data-v-d2c09870 data-v-2a038744> <span class="text" data-v-2a038744>Cite</span></button> <!----> <a target="_blank" href="https://s100.copyright.com/AppDispatchServlet?publisherName=CUP&amp;publication=NLP&amp;title=Topic%20aware%20probing%3A%20From%20sentence%20length%20prediction%20to%20idiom%20identification%20how%20reliant%20are%20neural%20language%20models%20on%20topic%3F&amp;publicationDate=25%20October%202024&amp;author=Vasudevan%20Nedumpozhimana%2C%20John%20D.%20Kelleher©right=%C2%A9%20The%20Author(s)%2C%202024.%20Published%20by%20Cambridge%20University%20Press&amp;contentID=10.1017%2Fnlp.2024.43&amp;startPage=1&amp;endPage=29&amp;orderBeanReset=True&amp;volumeNum=&amp;issueNum=&amp;oa=CC-BY" data-test-id="buttonRightLink" class="app-link rights-link d-print-none app-link__text-icon app-link--accent" data-v-63dfaf6e data-v-92ee52a2 data-v-43a4d572><img src="/core/page-component/img/rights-icon.d4a677c.svg" alt="" class="app-icon icon rights" data-v-d2c09870 data-v-63dfaf6e><span class="text" data-v-63dfaf6e>Rights &amp; Permissions
    <!----></span> <span class="sr-only" data-v-63dfaf6e>[Opens in a new window]</span></a></div> <hr aria-hidden="true" class="separator default" data-v-7036083a></div> <div class="share-modal-overlay" style="display:none;" data-v-43a4d572><!----></div> <div data-spy="scroll" data-target="#toc" class="scrollspy-content" data-v-43a4d572><!----> <div id="sec0" class="col article-abstract sec" data-v-2fa8b348 data-v-43a4d572><div class="abstract-text-container" data-v-2fa8b348><div lang="en"><h2>Abstract</h2> <!----> <div class="abstract-content"><div class="abstract" data-abstract-type="normal"><p>Transformer-based neural language models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order/syntactic or word co-occurrence/topic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models’ (BERT and RoBERTa’s) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call <span class="italic">topic-aware probing</span>. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers, but also that the facility of these models to distinguish idiomatic usage is primarily based on their ability to identify and encode topic. Furthermore, our analysis of these models’ performance on other standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for these models.</p></div></div> <hr aria-hidden="true" class="abstract-divider separator default" data-v-7036083a></div></div> <!----> <!----> <!----></div> <!----> <div class="keywords" data-v-86c27100 data-v-43a4d572><h2 data-v-86c27100>Keywords</h2> <div class="row keywords__pills" data-v-86c27100><a href="/core/search?filters[keywords]=Semantics" data-v-f0b31360 data-v-86c27100><span data-v-f0b31360 data-v-86c27100>Semantics</span></a><a href="/core/search?filters[keywords]=topic modelling" data-v-f0b31360 data-v-86c27100><span data-v-f0b31360 data-v-86c27100>topic modelling</span></a><a href="/core/search?filters[keywords]=idiom token identification" data-v-f0b31360 data-v-86c27100><span data-v-f0b31360 data-v-86c27100>idiom token identification</span></a><a href="/core/search?filters[keywords]=neural language model" data-v-f0b31360 data-v-86c27100><span data-v-f0b31360 data-v-86c27100>neural language model</span></a><a href="/core/search?filters[keywords]=transformer" data-v-f0b31360 data-v-86c27100><span data-v-f0b31360 data-v-86c27100>transformer</span></a></div> <hr aria-hidden="true" class="separator default" data-v-7036083a data-v-86c27100></div> <!----> <dl class="article-details" data-v-6e32a161 data-v-43a4d572><div class="row" data-v-6e32a161><dt class="col-12 col-sm-3 col-md-2_5 title" data-v-6e32a161>
      Type
    </dt> <dd class="col content" data-v-6e32a161>Article</dd></div> <div class="row" data-v-6e32a161><dt class="col-12 col-sm-3 col-md-2_5 title" data-v-6e32a161>
      Information
    </dt> <dd class="col content" data-v-6e32a161><div class="content__journal" data-v-6e32a161><a href="/core/journals/natural-language-processing" class="app-link app-link__text app-link--underlined" data-v-63dfaf6e data-v-6e32a161><!----><span class="text" data-v-63dfaf6e>Natural Language Processing
    <!----></span> <!----></a> <span data-v-6e32a161>
          ,
          <a href="/core/journals/natural-language-processing/firstview" class="app-link app-link__text app-link--underlined" data-v-63dfaf6e data-v-6e32a161><!----><span class="text" data-v-63dfaf6e>First View
    <!----></span> <!----></a></span> <!----> <span data-v-6e32a161>, pp. 1 - 29</span> <!----></div> <div class="doi-data" data-v-6e32a161><div data-v-6e32a161>DOI: <a target="_blank" href="https://doi.org/10.1017/nlp.2024.43" class="app-link app-link__text app-link--accent" data-v-63dfaf6e data-v-6e32a161><!----><span class="text" data-v-63dfaf6e>https://doi.org/10.1017/nlp.2024.43
    <!----></span> <span class="sr-only" data-v-63dfaf6e>[Opens in a new window]</span></a></div> <!----></div> <!----> <!----></dd></div> <div class="row creative-commons" data-v-6e32a161><dt class="col-12 col-sm-3 col-md-2_5 creative-commons__title">
    Creative Commons
  </dt> <dd class="col"><div class="creative-commons__content"><div class="creative-commons__icons"><img src="/core/page-component/img/license-cc-icon.e3a74ed.svg" alt="Creative Common License - CC" class="app-icon creative-commons__icon license-cc" data-v-d2c09870><img src="/core/page-component/img/license-by-icon.33e212c.svg" alt="Creative Common License - BY" class="app-icon creative-commons__icon license-by" data-v-d2c09870></div> <div class="creative-commons__description">This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (<a href="http://creativecommons.org/licenses/by/4.0/" target="_blank">http://creativecommons.org/licenses/by/4.0/</a>), which permits unrestricted re-use, distribution and reproduction, provided the original article is properly cited.</div></div></dd></div> <!----> <div class="row" data-v-6e32a161><dt class="col-12 col-sm-3 col-md-2_5 title" data-v-6e32a161>
      Copyright
    </dt> <dd class="col content" data-v-6e32a161><div data-v-6e32a161>
        © The Author(s), 2024. Published by Cambridge University Press
      </div></dd></div></dl> <!----> <div id="content-container" class="content-container" data-v-43a4d572><div class="content-box"><div class="article research-article NLM">

<div class="body">
<div class="sec intro" data-magellan-destination="s1" id="s1">

<h2 class="A"><span class="label">1.</span> Introduction</h2>
<p class="p"> Pre-trained deep neural language models such as BERT (Devlin <em class="italic">et al.</em> <a class="xref bibr" href="#ref12"><span class="show-for-sr">Reference Devlin, Chang, Lee and Toutanova</span>2018</a>) and RoBERTa (Liu <em class="italic">et al.</em> <a class="xref bibr" href="#ref39"><span class="show-for-sr">Reference Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer and Stoyanov</span>2019b</a>) are used to generate contextualised distributed representations (vector embeddings) of natural language text. Models based on these contextualised embeddings have achieved excellent performance across a range of NLP tasks. Consequently, what type of information is encoded in the embeddings generated by these deep neural language models is an interesting research question.</p>
<p class="p"> Conneau <em class="italic">et al.</em> (<a class="xref bibr" href="#ref8"><span class="show-for-sr">Reference Conneau, Kruszewski, Lample, Barrault and Baroni</span>2018</a>) proposed the probing methodology as a way to understand what types of information are present in an embedding. A probing task is a classification problem where a model is trained on embeddings of sentences with the goal of categorising sentences based on a linguistic property. Examples of the types of properties that might act as the basis for the classification include the tense of the sentence, the length of the sentence, the depth of a parse tree, or the presence of particular pre-selected terms within a sentence. The probing method assumes that the success of a trained classification model (i.e., a probe) on a task indicates whether the embeddings the probe is trained on encode information relevant to the linguistic property the probe is attempting to identify.</p>
<p class="p"> A number of probing studies of Transformer-based language models have suggested that these models encode word-order and syntactic information in their embeddings (Raganato and Tiedemann <a class="xref bibr" href="#ref52"><span class="show-for-sr">Reference Raganato and Tiedemann</span>2018</a>; Hewitt and Manning <a class="xref bibr" href="#ref27"><span class="show-for-sr">Reference Hewitt and Manning</span>2019</a>; Clark <em class="italic">et al.</em> <a class="xref bibr" href="#ref7"><span class="show-for-sr">Reference Clark, Khandelwal, Levy and Manning</span>2019</a>; Reif <em class="italic">et al.</em> <a class="xref bibr" href="#ref55"><span class="show-for-sr">Reference Reif, Yuan, Wattenberg, Viegas, Coenen, Pearce and Kim</span>2019</a>; Jawahar <em class="italic">et al.</em> <a class="xref bibr" href="#ref28"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019a</a>; Lin <em class="italic">et al.</em> <a class="xref bibr" href="#ref37"><span class="show-for-sr">Reference Lin, Tan and Frank</span>2019</a>; Manning <em class="italic">et al.</em> <a class="xref bibr" href="#ref41"><span class="show-for-sr">Reference Manning, Clark, Hewitt, Khandelwal and Levy</span>2020</a>; Arps <em class="italic">et al.</em> <a class="xref bibr" href="#ref4"><span class="show-for-sr">Reference Arps, Samih, Kallmeyer and Sajjad</span>2022</a>; Pimentel <em class="italic">et al.</em> <a class="xref bibr" href="#ref51"><span class="show-for-sr">Reference Pimentel, Valvoda, Stoehr and Cotterell</span>2022</a>). Indeed, it has been claimed that these models rediscover the classical NLP pipeline, with the earlier layers encoding syntactic information and later layers semantic (Tenney, Das, and Pavlick, <a class="xref bibr" href="#ref68"><span class="show-for-sr">Reference Tenney, Das and Pavlick</span>2019</a>a) (although this claim has been questioned (Niu, Lu, and Penn, <a class="xref bibr" href="#ref46"><span class="show-for-sr">Reference Niu, Lu and Penn</span>2022</a>)). Recently, a number of studies (Sinha <em class="italic">et al.</em> <a class="xref bibr" href="#ref64"><span class="show-for-sr">Reference Sinha, Parthasarathi, Pineau and Williams</span>2021b</a>; Pham <em class="italic">et al.</em> <a class="xref bibr" href="#ref50"><span class="show-for-sr">Reference Pham, Bui, Mai and Nguyen</span>2021</a>; Gupta <em class="italic">et al.</em> <a class="xref bibr" href="#ref20"><span class="show-for-sr">Reference Gupta, Kvernadze and Srikumar</span>2021</a>; Hessel and Schofield <a class="xref bibr" href="#ref25"><span class="show-for-sr">Reference Hessel and Schofield</span>2021</a>; Sinha <em class="italic">et al.</em> <a class="xref bibr" href="#ref63"><span class="show-for-sr">Reference Sinha, Jia, Hupkes, Pineau, Williams and Kiela</span>2021a</a>) have examined the sensitivity of neural language models to word-order perturbations during pretraining, fine-tuning, and/or inference across standard benchmarks such as GLUE and PAWS and found the performance of these models is relatively insensitive to word order (although the results reported by Abdou <em class="italic">et al.</em> (<a class="xref bibr" href="#ref1"><span class="show-for-sr">Reference Abdou, Ravishankar, Kulmizev and Søgaard</span>2022</a>) indicate that even when trained on shuffled text neural language models do encode some word order information). One interpretation of these word-permutation results is that we need to develop more challenging benchmarks in order to really assess the linguistic abilities of modern NLP models (Sinha <em class="italic">et al.</em> <a class="xref bibr" href="#ref63"><span class="show-for-sr">Reference Sinha, Jia, Hupkes, Pineau, Williams and Kiela</span>2021a</a>). A parallel interpretation is that much of the performance of these models on current benchmarks is based on shallow surface-level information such as word co-occurrence/topic, rather than syntactic information.</p>
<p class="p"> Given that these language models encode multiple forms of linguistic information and yet their performance on a range of tasks seems to be insensitive to word order perturbations, we are interested in examining the relative contribution of these different types of information to the improvement in NLP that Transformer-based language models have achieved in the last number of years. In particular, is this improvement primarily based on simply more effective topic modelling or is it that Transformer language models rely more on combining a variety of other linguistics signals? The concept of a topic is directly related to the idea of a coherent group of concepts, or entities in the world, that are likely to co-occur (Manning and Schutze <a class="xref bibr" href="#ref40"><span class="show-for-sr">Reference Manning and Schutze</span>1999</a>) and so share a non-taxonomic semantic association (Kacmajor and Kelleher <a class="xref bibr" href="#ref30"><span class="show-for-sr">Reference Kacmajor and Kelleher</span>2020</a>). Consequently, words that refer to entities/concepts that belong to the same topic are more likely to co-occur than words that refer to entities/concepts from different topics. We focus on the relative contribution of topic versus non-topic information because the topic information, understood in terms of word co-occurrence, is directly related to the masked language modelling and next-word prediction objective functions used to train many language models like BERT, RoBERTa, and GPT. However, as Mickus <em class="italic">et al.</em> (<a class="xref bibr" href="#ref42"><span class="show-for-sr">Reference Mickus, Paperno, Constant and van Deemter</span>2020</a>) pointed out, models like BERT which are also trained on next-sentence prediction objective are not purely based on distributional semantics (i.e., word co-occurrence). On the other hand, models like RoBERTa which are trained only on masked language model objective may be more focused on encoding distributional semantics. Consequently, our study of the relative contribution of topic versus non-topic information to both BERT’s and RoBERTa’s performance is relevant both to the current debate on the extent to which neural language models encode and use syntactic information, and also sheds light on the ongoing theoretical debate about whether these models are based on distributional semantics or not.</p>
<p class="p"> We approach the research question of the extent to which Transformer-based pre-trained language models rely on the topic information from two complementary directions. First, we propose a new methodology for probing which we call topic-aware probing. We also experiment using a variety of probing tasks some of which we expect to be less sensitive to topic information and others to be more sensitive. Combining the novel topic-aware probing methodology with an analysis across a range of probing tasks enables us to explore the extent to which Transformer-based models are reliant on topic versus non-topic information. We selected BERT (Devlin <em class="italic">et al.</em> <a class="xref bibr" href="#ref12"><span class="show-for-sr">Reference Devlin, Chang, Lee and Toutanova</span>2018</a>) and RoBERTa (Liu <em class="italic">et al.</em> <a class="xref bibr" href="#ref39"><span class="show-for-sr">Reference Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer and Stoyanov</span>2019b</a>) base models to conduct our experiments.</p>
<p class="p"> Within the set of probing tasks that we examine, we foreground the task of idiom token identification because the encoding of idiomatic information in neural models is relatively understudied (e.g., it is not one of the standard probing tasks proposed by Conneau <em class="italic">et al.</em> (<a class="xref bibr" href="#ref8"><span class="show-for-sr">Reference Conneau, Kruszewski, Lample, Barrault and Baroni</span>2018</a>)), and prior research suggests that identifying idiomatic usage requires the encoding of lexical, syntactic and topic information (Nedumpozhimana, Klubička, and Kelleher <a class="xref bibr" href="#ref45"><span class="show-for-sr">Reference Nedumpozhimana, Klubička and Kelleher</span>2022</a>). An idiom is a multiword expression with a meaning that cannot be composed of its parts (Sporleder <em class="italic">et al.</em> <a class="xref bibr" href="#ref66"><span class="show-for-sr">Reference Sporleder, Li, Gorinski and Koch</span>2010</a>). It is hard to find a single agreed-upon definition for idioms in the literature, but they are often defined as sequences of words involving some degree of semantic idiosyncrasy or non-compositionality (Fazly, Cook, and Stevenson, <a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>). Idioms appear in all languages and text genres, prototypical examples from English include expressions such as <em class="italic">by and large</em> and <em class="italic">kick the bucket</em> (Sag <em class="italic">et al.</em> <a class="xref bibr" href="#ref58"><span class="show-for-sr">Reference Sag, Baldwin, Bond, Copestake and Flickinger</span>2002</a>). An idiomatic expression can have both idiomatic and literal meanings associated with it. Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>) highlight this aspect of idiomatic expressions and illustrate it with the expression <em class="italic">make a face</em> which has an idiomatic sense in the sentence <em class="italic">The little girl made a funny face at her mother</em> and has a literal sense in the sentence <em class="italic">she made a face on the snowman using a carrot and two buttons</em>. Building on this distinction, Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>) define the task of idiom token identification as deciding whether a particular usage of a given idiomatic expression is an idiomatic usage or a literal usage. While this idiom token identification task by Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>) identifies the idiomatic usage of a specific idiomatic expression in a sentence, we generalise the problem to identify the idiomatic usage within a sentence of any expression from a target category of multiword expressions. In this case, we considered the category of verb-noun idiomatic expressions from VNIC dataset (Cook, Fazly, and Stevenson <a class="xref bibr" href="#ref10"><span class="show-for-sr">Reference Cook, Fazly and Stevenson</span>2008</a>) as our target set, but in principle, it could be any set of multiword expressions that the model is trained on.<a class="xref fn" href="#fn1"><span class="show-for-sr">Footnote </span>
<sup class="sup">a</sup>
</a> As shown in Table <a class="xref table" href="#tbl1">1</a>, the input of this task is a sentence that contains a usage of one of the target expressions in it (in our case, a VNIC expression) and we do not explicitly provide any information to the model regarding which idiomatic expression is present in the sentence. The same model is used to process all sentences irrespective of which expression is present in the sentence. This task of general idiom token identification is a sentence-level binary classification task and the model is required to label the input sentence as ‘Idiomatic’ if it contains an expression from the target category that is being used idiomatically, and ‘Literal’ if the expression is being used literally. We pay particular attention to the task of idiom token identification because a review of the literature on idiom token identification (see Section <a class="xref sec" href="#s2-2">2.2</a>) reveals that this task is sensitive not only to the topic information, but also to a variety of other types of information, such as lexical and syntactic fixedness, or fluency based information. Consequently, this task provides an ideal case study to explore the relative contribution of different types of linguistics information to the performance of Transformer-based neural language models on a task.</p>
<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl1" id="tbl1">

<div class="caption">
<p class="p"><span class="label">Table 1.</span> Sample input and corresponding target output for the general idiom token identification task</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png?pub-status=live" class="aop-lazy-load-image" width="666" height="135" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png" data-zoomable="true"></div>
</span>
</div>

<p class="p"> To summarise, the main contributions of this work are: (a) we present an extension to the probing method called topic-aware probing, (b) we assess the contribution of topic-based information to the performance of a Transformer-based probing on the task of general idiom token identification, and (c) more generally we explore the relationship between topic and the performance of Transformer-based neural language models across a range of probing tasks and find that tasks that are relatively insensitive to topic are also tasks that Transformer-based neural language models find relatively difficult.<a class="xref fn" href="#fn2"><span class="show-for-sr">Footnote </span>
<sup class="sup">b</sup>
</a>
</p>
</div>
<div class="sec other" data-magellan-destination="s2" id="s2">

<h2 class="A"><span class="label">2.</span> Related work</h2>
<div class="sec" data-magellan-destination="s2-1" id="s2-1">

<h3 class="B"><span class="label">2.1</span> Transformer-based neural language models</h3>
<p class="p"> Most of the recent neural language models are Transformer-based and many pre-trained Transformer-based language models achieve very good performance on most of the downstream natural language processing tasks. However, due to the distributed nature of the representations used by these models and the opacity of their processing of information arising from the internal complexity of Transformer neural architecture, the specific types of information these models extract from language are unclear. In response to this, there is a growing body of work focused on understanding the basis for the state-of-the-art performance of these models. For example, Rogers <em class="italic">et al.</em> (<a class="xref bibr" href="#ref56"><span class="show-for-sr">Reference Rogers, Kovaleva and Rumshisky</span>2020</a>) surveyed over 150 papers related to the BERT model, one of the foundational Transformer-based neural language models and reviewed various kinds of information learned by BERT.</p>
<p class="p"> Focusing first on BERT’s ability to encode syntactic information, a number of studies in the literature investigate whether BERT learns, or internally represents, any syntactic information about the input sentence. These investigations are interesting because BERT is pre-trained on a sequence of words and the pretraining objectives are not syntactic tasks. Lin <em class="italic">et al.</em> (<a class="xref bibr" href="#ref37"><span class="show-for-sr">Reference Lin, Tan and Frank</span>2019</a>) showed that BERT internally represents the syntactic tree structure or an input sentence, and Hewitt and Manning (<a class="xref bibr" href="#ref27"><span class="show-for-sr">Reference Hewitt and Manning</span>2019</a>) showed that it is possible to learn transformation matrices from BERT representations which can be used to recover syntactic dependency relations within the sentences from the PennTreebank. Other studies have also shown that enough syntactic information is encoded in BERT sentence representations to allow the recovery of the parse tree structure of an input sentence (Vilares <em class="italic">et al.</em> <a class="xref bibr" href="#ref70"><span class="show-for-sr">Reference Vilares, Strzyz, Søgaard and G’omez-Rodr’iguez</span>2020</a>; Kim <em class="italic">et al.</em> <a class="xref bibr" href="#ref31"><span class="show-for-sr">Reference Kim, Choi, Edmiston and goo Lee</span>2020</a>; Rosa and Mareček, <a class="xref bibr" href="#ref57"><span class="show-for-sr">Reference Rosa and Mareček</span>2019</a>). However, Ettinger (<a class="xref bibr" href="#ref14"><span class="show-for-sr">Reference Ettinger</span>2020</a>) shows that BERT is insensitive to malformed input and argues that therefore the syntactic knowledge in BERT is either incomplete, or else BERT doesn’t rely on it for solving tasks. Furthermore, Glavaš and Vulić (<a class="xref bibr" href="#ref18"><span class="show-for-sr">Reference Glavaš and Vulić</span>2021</a>) show that intermediate fine-tuning of BERT for a supervised parsing task does not improve BERT’s performance, suggesting that BERT does not rely on traditional syntactic knowledge for solving tasks. The divergence between studies demonstrating BERT’s ability to encode syntactic information and those that question this ability may be explained through the work of Wu <em class="italic">et al.</em> (<a class="xref bibr" href="#ref72"><span class="show-for-sr">Reference Wu, Chen, Kao and Liu</span>2020</a>). Wu <em class="italic">et al.</em> (<a class="xref bibr" href="#ref72"><span class="show-for-sr">Reference Wu, Chen, Kao and Liu</span>2020</a>) conducted a probing experiment to assess the impact of each word on predicting other words in a masked language model. Their study found that words in the same syntactic sub-tree have a larger impact on each other. Interestingly their results also show that although BERT learns some syntactic information it is not very similar to linguistically annotated resources and that the impact of performance on downstream NLP tasks achieved by using the syntactic structural information encoded by BERT is comparable, and even superior, to the human-designed syntactic structural information.</p>
<p class="p"> These studies on BERT have been extended to other BERT-like neural language models. Arps <em class="italic">et al.</em> (<a class="xref bibr" href="#ref4"><span class="show-for-sr">Reference Arps, Samih, Kallmeyer and Sajjad</span>2022</a>) investigated the extent to which neural language models (namely BERT, XLNet, RoBERTa, and DistilBERT) implicitly learn syntactic structure. They found that constituency parse trees of sentences can be extracted from distributed representations generated by these language models. Their results show that if the syntactic structure of data is correct then tree structures are extractable even if the data is semantically ill-formed. This suggests that without the help of semantic information, syntactic information can be extracted from these language models which indicates that these language models do encode syntactic information. By using a novel probing method based on the architectural bottleneck principle, Pimentel <em class="italic">et al.</em> (<a class="xref bibr" href="#ref51"><span class="show-for-sr">Reference Pimentel, Valvoda, Stoehr and Cotterell</span>2022</a>) also showed that the syntactic structure of a sentence is mostly extractable from BERT, ALBERT, and RoBERTa language models. They also point out that even though syntactic information is extractable from language models it is not clear whether this information is actually used by these models.</p>
<p class="p"> There are also several studies in the literature which investigate the presence of semantic information in Transformer-based neural language models. Studies conducted by Ettinger (<a class="xref bibr" href="#ref14"><span class="show-for-sr">Reference Ettinger</span>2020</a>) and Tenney <em class="italic">et al.</em> (<a class="xref bibr" href="#ref69"><span class="show-for-sr">Reference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick</span>2019b</a>) show that BERT encodes some knowledge about semantic roles and entity relations. However, Balasubramanian <em class="italic">et al.</em> (<a class="xref bibr" href="#ref5"><span class="show-for-sr">Reference Balasubramanian, Jain, Jindal, Awasthi and Sarawagi</span>2020</a>) showed that although a BERT-based model achieves a good performance in Named Entity Recognition, there is a huge performance drop after replacing the names in the dataset, which indicates that BERT does not actually form a generic idea about named entities. By using a novel methodology to probe linguistic information for logical inference Chen and Gao (<a class="xref bibr" href="#ref6"><span class="show-for-sr">Reference Chen and Gao</span>2022</a>) observed that RoBERTa and BERT language models encode information on simple semantic phenomena rather than complex semantic phenomena.</p>
<p class="p"> Given that Transformer-based neural language models appear to partially encode both syntactic and semantic information about natural language text a natural question to ask is where in the Transformer architecture this information is encoded? Moreover, is the encoding of different types of information localised to specific layers in a Transformer or is it spread across multiple layers? A number of studies have investigated where the encoding of information occurs within the Transformer architecture of BERT: Lin <em class="italic">et al.</em> (<a class="xref bibr" href="#ref37"><span class="show-for-sr">Reference Lin, Tan and Frank</span>2019</a>) report that word order information decreases after the 4th layer of the base-BERT model; Hewitt and Manning (<a class="xref bibr" href="#ref27"><span class="show-for-sr">Reference Hewitt and Manning</span>2019</a>) report that the reconstruction of tree depth is most successful using the middle-layer embeddings of BERT (6th to 9th layers of base-BERT); Goldberg (<a class="xref bibr" href="#ref19"><span class="show-for-sr">Reference Goldberg</span>2019</a>) show that the best subject-verb agreement is obtained by using 8th and 9th layer of the base-BERT model, and Jawahar <em class="italic">et al.</em> (<a class="xref bibr" href="#ref29"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019b</a>) observed that the best performances of various high-level syntactic probing tasks are achieved with middle-layer embeddings of BERT. This set of results suggests that the initial layers of BERT encode information about linear word order and the later layers encode more hierarchical and syntactic information.</p>
<p class="p"> There are, however, conflicting results about the presence of syntactic information in various layers of BERT. For example, Tenney <em class="italic">et al.</em> (<a class="xref bibr" href="#ref69"><span class="show-for-sr">Reference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick</span>2019b</a>) and Jawahar <em class="italic">et al.</em> (<a class="xref bibr" href="#ref29"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019b</a>) observed that the best performance on basic syntactic tasks such as POS tagging and chunking is achieved using the initial layer embeddings of BERT and good performance on high-level tasks like parsing and other semantic tasks can be achieved using embeddings from the middle layers of BERT. On the contrary, Liu <em class="italic">et al.</em> (<a class="xref bibr" href="#ref38"><span class="show-for-sr">Reference Liu, Gardner, Belinkov, Peters and Smith</span>2019a</a>) observed that the best performance for both POS Tagging and chunking is obtained using middle-layer embeddings of BERT, and Tenney <em class="italic">et al.</em> (<a class="xref bibr" href="#ref69"><span class="show-for-sr">Reference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick</span>2019b</a>) observed that syntactic information is located in early and middle layers but semantic information is spread across all layers of the model. While the initial and middle layers of BERT encode syntactic and semantic information, the final layers of BERT are more task-specific, and Kovaleva <em class="italic">et al.</em> (<a class="xref bibr" href="#ref34"><span class="show-for-sr">Reference Kovaleva, Romanov, Rogers and Rumshisky</span>2019</a>) observed that these layers change the most while fine-tuning. This is in agreement with the observation of Liu <em class="italic">et al.</em> (<a class="xref bibr" href="#ref38"><span class="show-for-sr">Reference Liu, Gardner, Belinkov, Peters and Smith</span>2019a</a>) that overall the best performance is generally obtained using middle-layer embeddings and that embeddings from these layers are the most transferable across different tasks.</p>
<p class="p"> Extending this body of work, we investigate the relationship between topic and the performance of Transformer-based language models (BERT and RoBERTa) across a number of tasks. We also examine in which layers of these Transformer models the encoding of topic and non-topic information is located.</p>
</div>
<div class="sec" data-magellan-destination="s2-2" id="s2-2">

<h3 class="B"><span class="label">2.2</span> Idiom token identification</h3>
<p class="p"> Idioms are a sub-type of multiword expression (MWE). Other types of MWEs include compound nouns and verb particle constructions. Consequently, research on MWE is also relevant to the aspects of this research that is focused on idiom token identification. The MWE identification problem has been widely addressed within the NLP research community via the development and release of multiple shared tasks. To support research on understanding, modelling, and processing of MWEs, Savary <em class="italic">et al.</em> (<a class="xref bibr" href="#ref61"><span class="show-for-sr">Reference Savary, Ramisch, Cordeiro, Sangati, Vincze, QasemiZadeh, Candito, Cap, Giouli, Stoyanova and Doucet</span>2017</a>) introduced a shared task called PARSEME. This shared task released annotated datasets for 18 languages. Ramisch <em class="italic">et al.</em> (<a class="xref bibr" href="#ref53"><span class="show-for-sr">Reference Ramisch, Cordeiro, Savary, Vincze, Barbu Mititelu, Bhatia, Buljan, Candito, Gantar, Giouli, Güngör, Hawwari, Iñurrieta, Kovalevskaitė, Krek, Lichte, Liebeskind, Monti, Parra Escartín, QasemiZadeh, Ramisch, Schneider, Stoyanova, Vaidya and Walsh</span>2018</a>) extended this shared task to the PARSEME 1.1 task by updating the annotation methodology and releasing annotated data for 20 languages. The PARSEME shared task was further extended to the PARSEME 1.2 edition by Ramisch <em class="italic">et al.</em> (<a class="xref bibr" href="#ref54"><span class="show-for-sr">Reference Ramisch, Savary, Guillaume, Waszczuk, Candito, Vaidya, Barbu Mititelu, Bhatia, Iñurrieta, Giouli, Güngör, Jiang, Lichte, Liebeskind, Monti, Ramisch, Stymne, Walsh and Xu</span>2020</a>) in which the task involved identifying unseen MWEs and they released annotated data for 14 languages for this new task.</p>
<p class="p"> Schneider <em class="italic">et al.</em> (<a class="xref bibr" href="#ref62"><span class="show-for-sr">Reference Schneider, Hovy, Johannsen and Carpuat</span>2016</a>) proposed a task (SemEval-2016 Task 10) which combines the labelling of multiword expressions and supersenses by the assumption that MWE and supersenses are tightly coupled. Recently Tayyar Madabushi <em class="italic">et al.</em> (<a class="xref bibr" href="#ref67"><span class="show-for-sr">Reference Tayyar Madabushi, Gow-Smith, Garcia, Scarton, Idiart and Villavicencio</span>2022</a>) proposed a task of multilingual idiomaticity detection and idiomaticity representation. The proposed idiomaticity detection is a binary task to identify whether a sentence contains an idiomatic expression with the help of two context sentences. This task has a zero-shot setting in which MWEs in the training set and test set are disjoint. It also has a One Shot setting in which the training set has one idiomatic and one non-idiomatic example of each MWE in the test set. The idiomaticity representation task is an idiomatic semantic textual similarity task, in which the semantic similarity between sentences with an idiomatic phrase, correct literal paraphrase of the idiomatic phrase, and incorrect literal paraphrase of the idiomatic phrase should be predicted.</p>
<p class="p"> Constant <em class="italic">et al.</em> (<a class="xref bibr" href="#ref9"><span class="show-for-sr">Reference Constant, Eryiğit, Monti, van der Plas, Ramisch, Rosner and Todirascu</span>2017</a>) did a detailed survey on multiword expression (MWE) processing by dividing it into two subtasks: MWE (type) discovery and MWE (token) identification. The MWE (type) discovery subtask is focused on identifying new MWEs from text and the MWE (token) identification subtask involves automatically annotating multiword expressions in running text by associating them with known multiword expression types. Our general idiom token identification task has aspects of both of these tasks in it. On the one hand, we are interested in identifying whether a particular piece of text contains a non-compositional usage from a given category of MWEs. So from this perspective, our task is similar to MWE (token) identification in that we are annotating text, although in our case the annotation is a binary label applied to the entire sentence rather than an annotation at the token level. However, because the models we train are in principle able to identify new idiomatic expressions from a given category our work also has application in the area of MWE (type) discovery, although this aspect of our work is not the primary focus in this paper (see Nedumpozhimana <em class="italic">et al.</em> (<a class="xref bibr" href="#ref45"><span class="show-for-sr">Reference Nedumpozhimana, Klubička and Kelleher</span>2022</a>) for work on generalising to unknown—i.e., unseen during training—expressions within a given category).</p>
<p class="p"> Hashimoto and Kawahara (<a class="xref bibr" href="#ref24"><span class="show-for-sr">Reference Hashimoto and Kawahara</span>2008</a>) report research on idiom token identification for Japanese idioms and found that features normally used in word sense disambiguation that are defined over the context surrounding an expression worked well. Around the same time, Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>) proposed methods for idiom token identification for English based on two assumptions. First, they assumed that each idiomatic expression has a relatively fixed canonical syntactic form and that idiomatic usages of an expression tend to have this canonical syntactic form, whereas literal usages are less syntactically restricted. They also assumed that literal and idiomatic usages of an expression tend to occur with different sets of words in the surrounding context. Their results indicate that their idiom token identification model based on the syntactic form of an expression outperformed their model based on the words in the surrounding context. However, they note that this somewhat surprising result may have been affected by the fact that the definition of the typical word sets for the surrounding context of idiomatic and non-idiomatic usages used in their experiments was created using an unsupervised approach that may have resulted in noisy definitions of surrounding contexts.</p>
<p class="p"> Li and Sporleder (<a class="xref bibr" href="#ref35"><span class="show-for-sr">Reference Li and Sporleder</span>2010</a>a) examined the efficacy of feature sets based on global lexical context, discourse cohesion and local lexical features, such as cue words, for idiom token identification. They found that features based on global lexical context and discourse cohesion were the most effective. Li and Sporleder (<a class="xref bibr" href="#ref36"><span class="show-for-sr">Reference Li and Sporleder</span>2010</a>b) confirmed the efficacy of discourse cohesion features for idiom token identification. Following this theme of contextual approaches to idiom token identification, Feldman and Peng (<a class="xref bibr" href="#ref16"><span class="show-for-sr">Reference Feldman and Peng</span>2013</a>) and Peng <em class="italic">et al.</em> (<a class="xref bibr" href="#ref48"><span class="show-for-sr">Reference Peng, Feldman and Vylomova</span>2014</a>) explored topic features for the idiom token identification problem. Feldman <em class="italic">et al.</em> (<a class="xref bibr" href="#ref16"><span class="show-for-sr">Reference Feldman and Peng</span>2013</a>) and Peng <em class="italic">et al.</em> (<a class="xref bibr" href="#ref48"><span class="show-for-sr">Reference Peng, Feldman and Vylomova</span>2014</a>) based their work on the assumption that idiomatic usages will be semantically distant from the topics of the discourses in which they are present and so a candidate expression should be identified as an idiomatic usage if it is a semantic outlier with respect to the surrounding context.</p>
<p class="p"> Salton <em class="italic">et al.</em> (<a class="xref bibr" href="#ref59"><span class="show-for-sr">Reference Salton, Ross and Kelleher</span>2016</a>) demonstrated the viability of building an idiom token identification model using a distributed sentence representation, specifically the sentence embeddings generated by Skip-Thought Kiros <em class="italic">et al.</em> (<a class="xref bibr" href="#ref32"><span class="show-for-sr">Reference Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba and Fidler</span>2015</a>). They proposed a model for both per-expression and general idiom token identification problems (i.e., developing a single idiom token identification model that works across multiple expressions from a given category of MWE). Unlike previous work which required separate contextual/topic-based models for each expression being assessed, a distinctive aspect of the work by Salton <em class="italic">et al.</em> (<a class="xref bibr" href="#ref59"><span class="show-for-sr">Reference Salton, Ross and Kelleher</span>2016</a>) is that, unlike previous work which required separate contextual/topic-based models for each expression being assessed, their approach used a single model across all expressions within a category. Furthermore, their model only required the distributed embedding of the sentence the expression occurs within and did not need access to the surrounding context of the sentence. A natural question arising from the results reported by Salton <em class="italic">et al.</em> (<a class="xref bibr" href="#ref59"><span class="show-for-sr">Reference Salton, Ross and Kelleher</span>2016</a>) is what are the kinds of information that the distributed representation of a sentence encodes which are so useful for idiom token identification? For example, these embeddings may be capturing syntactic or lexical fixedness features, similar to those proposed by Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>), or be efficiently encoding some form of topic-based signal (efficient both in the sense of only requiring a small sample of text—i.e., the sentence—to pick up the relevant context, and also in terms of being able to do this across multiple expressions with, presumably, variation in the topic signals associated with each expression). Hashempour and Villavicencio (<a class="xref bibr" href="#ref23"><span class="show-for-sr">Reference Hashempour and Villavicencio</span>2020</a>) also performed an idiom token identification experiment and found that contextual word embeddings, such as those generated by BERT, outperform non-contextual word embeddings. However, probing experiments by Garcia <em class="italic">et al.</em> (<a class="xref bibr" href="#ref17"><span class="show-for-sr">Reference Garcia, Kramer Vieira, Scarton, Idiart and Villavicencio</span>2021</a>) on contextualised vector space models like ELMo and BERT concluded that idiomatic usage is not yet accurately represented by these contextualised models. Extending the work on using neural sentence embeddings for idiom token identification, Nedumpozhimana and Kelleher (<a class="xref bibr" href="#ref44"><span class="show-for-sr">Reference Nedumpozhimana and Kelleher</span>2021</a>) showed that more recent contextual distributed representations such as those generated by BERT models encode idiomatic information, and their result suggest that a topic signal might be the key information encoded in the BERT representation.</p>
<p class="p"> More recently, Nedumpozhimana <em class="italic">et al.</em> (<a class="xref bibr" href="#ref45"><span class="show-for-sr">Reference Nedumpozhimana, Klubička and Kelleher</span>2022</a>) report experiments using the game theory concept of Shapley Values to analyse the type of information that idiom token identification models based on BERT find useful. They first report expression-wise experiments using Shapley Value analysis that analysed the relative contribution of different expressions to the generalisation ability of an idiom token identification model. Then they used the results of these experiments together with an expression-wise analysis of the association between different expressions and different linguistic phenomena (syntactic and/or lexical fixedness, topic, and so on) to assess which types of linguistic information are more useful for idiom token identification. They find that a combination of idiom-intrinsic and topic-based features are useful for achieving generalisability, and argue that their results point to BERT encoding different types of linguistic information, including topic, lexical and syntactic information. Prompted by these findings, within our examination of the role of the topic information in the distributed representations generated by Transformer-based neural language models, we put a particular focus on the extent to which Transformer-based models (BERT and RoBERTa) rely on the topic information to identify idiomatic usage. Accordingly building on previous work that performed expression-wise analysis here we directly analyse the contribution of the topic information in general by assessing the ability of general idiom token identification models trained on sentences from one topic to generalise to sentences from other topics.</p>
</div>
</div>
<div class="sec other" data-magellan-destination="s3" id="s3">

<h2 class="A"><span class="label">3.</span> Topic-aware probing</h2>
<p class="p"> Our topic-aware probing method is designed to investigate the role of topic signals in a probing task. The basic idea is to train the probing model on samples from a particular topic and then test it on samples from the topic the training data were sampled from and separately on samples from other topics. We then analyse the difference in performance on the samples from the topic seen during training and the samples from unseen topics. A large difference in performance would indicate that the topic information is an important factor in determining the performance of a model on a task.</p>
<p class="p"> The first step in topic-aware probing is to partition the dataset into <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline1.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline1.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> different topics using a topic model. Next, we split the set of samples in each topic into <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline2.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="8" height="13" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline2.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$k$
</span></span>
</span>
</span> folds, for cross-fold validation. We then iterate through the topics and for each fold <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline3.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="5" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline3.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$i$
</span></span>
</span>
</span> in a topic we:</p><ol class="list number nomark">
<li class="list-item">

<p class="p"><span class="label">(1)</span> train the probe (e.g., the general idiom token identification model) using the data from the other folds in the topic;</p>
</li>
<li class="list-item">

<p class="p"><span class="label">(2)</span> evaluate the probe on the <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline4.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="15" height="17" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline4.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$i^{th}$
</span></span>
</span>
</span> fold in the topic and record the performance of the probe as a <em class="italic">seen topic score</em>;</p>
</li>
<li class="list-item">

<p class="p"><span class="label">(3)</span> iterate through the other <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline5.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="29" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline5.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n{-}1$
</span></span>
</span>
</span> topics in the topic model, evaluate the probe on the corresponding <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline6.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="15" height="17" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline6.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$i^{th}$
</span></span>
</span>
</span> fold in these other topics, and record the performance of the probe on each of these folds as an <em class="italic">unseen topic score</em>.</p>
</li>
</ol>

<p class="p"> At the end of this process, for each of the <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline7.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline7.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> topics we have calculated <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline8.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="8" height="13" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline8.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$k$
</span></span>
</span>
</span> seen topic scores and <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline9.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="76" height="17" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline9.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$k \times (n-1)$
</span></span>
</span>
</span> unseen topic scores. We then calculate for each topic the average seen topic score and the average unseen topic score. Figure <a class="xref fig" href="#f1">1</a> illustrates the topic-aware probing method.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f1" id="f1">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig1.png?pub-status=live" class="aop-lazy-load-image" width="2250" height="2370" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig1.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 1.</span> Topic-Aware probing method.</p>
</div></div></section>

<p class="p"> If the topic signal is important in terms of helping the probe to predict the label for the task then the performance of the probe model should be significantly better on samples from seen topics compared to unseen topics (i.e., the seen topic scores should be higher than the unseen topic scores). On the other hand, if we do not observe a significant difference in performance between samples from seen topics compared to unseen topics this would indicate that the topic is not an important signal in terms of the probing task.</p>
<p class="p"> In our experiments, we use Latent Semantic Indexing (LSI) (also known as Latent Semantic Analysis) for topic modelling: LSI is an unsupervised topic modelling approach based on the distributional hypothesis (Deerwester <em class="italic">et al.</em> <a class="xref bibr" href="#ref11"><span class="show-for-sr">Reference Deerwester, Dumais, Landauer, Furnas and Harshman</span>1990</a>). We use LSI because it captures word co-occurrence (Manning and Schutze <a class="xref bibr" href="#ref40"><span class="show-for-sr">Reference Manning and Schutze</span>1999</a>; Eisenstein <a class="xref bibr" href="#ref13"><span class="show-for-sr">Reference Eisenstein</span>2019</a>) which is essentially the topic signal that we are assessing in this experiment.</p>
<p class="p"> For cross-fold validation, we use 5 folds, and in order to maintain the label distribution within a topic across the 5 folds we use stratified sampling to create the folds. Note that some of the topics identified by the topic model can have less than 5 samples from some label categories and so for these topics, we would not be able to split the samples into 5 stratified folds. One option for dealing with these small topics is to discard them. This, however, would result in some labelled examples being discarded, something that may be undesirable if we are working with a small dataset. Consequently, we consider any topics with less than 5 samples from some label as a tail topic, and we iteratively merge the tail topics with other topics (preferably with some other tail topic) to reduce the number of tail topics. The iterative procedure for merging tail topics is as follows: if there is only one tail topic then we randomly select one of the non-tail topics and merge the tail topic with the selected non-tail topic, and if there is more than one tail topic then we randomly select two of the tail topics and merge them. This process of tail topic merging continues until there is no tail topic left. This tail-reduction process may reduce the semantic coherence within some topics because it involves merging unrelated topics into a single topic. Furthermore, this dilution in the topic signal may result in the topic-aware probing method underestimating the importance of the topic signal on a given task (i.e., it may increase the likelihood of a Type II error-false-negative-in a topic-aware probing experiment). In other words, if topic merging has an effect on an experimental analysis the effect is to reduce the sensitivity of the method to the topic signal by reducing the difference between seen and unseen scores. Consequently, in situations where we do see a difference between seen and unseen scores the merging of topics will only have weakened this difference, and not caused it. So when we see a difference, topic merging won’t be the cause of the difference. The more problematic case is where we don’t see a difference between seen and unseen scores. In this case, a topic difference may in fact exist but the merging may have diluted it. Fortunately, however, the problem of tail topics typically only arises for runs of the topic modelling process where we extract a large number of topics from a relatively small dataset. The number of tail topics obtained from different topic models on different datasets are shown in the coming sections in Tables <a class="xref table" href="#tbl2">2</a> and <a class="xref table" href="#tbl5">5</a>. Consequently, the topic merging process does not affect all the runs of a topic modelling process, and so one way to mitigate the effect of topic merging is to report the average seen versus unseen difference across multiple runs of a topic model with different numbers of topics identified in each run. This is one of the reasons why in our experiments we report the average difference between seen and unseen topic scores across 10 topic models with the number of topics ranging from 5 to 50. The other reason is that by reporting average differences across multiple topic models we reduce the sensitivity of the analysis to the number of topics chosen by a single topic modelling process. Once the tail topics are merged we split samples from each topic into 5 stratified folds.</p>
<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl2" id="tbl2">

<div class="caption">
<p class="p"><span class="label">Table 2.</span> Number of tail topics from 10 topic models on Bigram shift and VNIC dataset</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png?pub-status=live" class="aop-lazy-load-image" width="582" height="92" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png" data-zoomable="false"></div>
</span>
</div>

<p class="p"> Multilayered perceptron (MLP) models with one hidden layer are one of the standard models used in the probing literature (Conneau <em class="italic">et al.</em> <a class="xref bibr" href="#ref8"><span class="show-for-sr">Reference Conneau, Kruszewski, Lample, Barrault and Baroni</span>2018</a>). Furthermore, probing experiments have demonstrated that dense embeddings, such as BERT embeddings, can encode information in a distributed manner (e.g., in the embedding norm (Klubička and Kelleher <a class="xref bibr" href="#ref33"><span class="show-for-sr">Reference Klubička and Kelleher</span>2022</a>)) and so using a model type that is able to integrate information from across an embedding (such as an MLP) allows the probe to utilise this distributed information. Therefore we use an MLP model, with one hidden layer using ReLU as the hidden layer activation function, as the probing model for predicting the label from the distributed representation of a sample sentence.</p>
</div>
<div class="sec other" data-magellan-destination="s4" id="s4">

<h2 class="A"><span class="label">4.</span> Experimentation</h2>
<p class="p"> In order to confirm that topic-aware probing functions as expected, we first apply it to a probing task that we expect will not be sensitive to the topic information, namely bigram shift—a probing task introduced by Conneau <em class="italic">et al.</em> (<a class="xref bibr" href="#ref8"><span class="show-for-sr">Reference Conneau, Kruszewski, Lample, Barrault and Baroni</span>2018</a>). The bigram shift task is to predict whether any two consecutive words within a sentence have been swapped. We do not expect the topic signal to be a useful information source for this task because swapping two consecutive words in a sentence will not change the topic of the sentence (at least not from the perspective of a word co-occurrence-based topic model because the sentence will contain the same set of words after the swapping as the original sentence). Consequently, the bigram shift task will enable us to validate topic-aware probing. Our expectation is that because the bigram shift task is not sensitive to topic information we should observe similar seen and unseen topic scores if the methodology is working as expected.</p>
<p class="p"> Having checked the topic-aware probing methodology works as expected for the bigram shift task, we switch our focus to investigate the role of the topic as a signal in BERT-based and RoBERTa-based general idiom token identification. To do this we first confirm that general idiom token identification is sensitive to topic signals by using topic-aware probing, and then isolate the contribution of the topic signal to the performance of BERT and RoBERTa on this task by comparing the performance of BERT and RoBERTa to a (primarily) topic-based embedding model. We use GloVe embeddings to act as this topic baseline because GloVe embeddings are trained on the nonzero elements in a word-word co-occurrence matrix Pennington <em class="italic">et al.</em> (<a class="xref bibr" href="#ref49"><span class="show-for-sr">Reference Pennington, Socher and Manning</span>2014</a>), and so the primary information captured by GloVe embeddings directly relates to the concept of the topic we are examining here. Furthermore, as we shall see in Section <a class="xref sec" href="#s5">5</a>, our analysis of the results from applying topic-aware probing to the bigram shift task not only confirms that the topic-aware probing method is functioning as expected but also supports the assumption that GloVe embeddings primarily encode topic (word co-occurrence) information.</p>
<div class="sec" data-magellan-destination="s4-1" id="s4-1">

<h3 class="B"><span class="label">4.1</span> Data preparation</h3>
<p class="p"> For the experiments on the bigram shift task, we used 119,998 English sentences from the established bigram shift dataset.<a class="xref fn" href="#fn3"><span class="show-for-sr">Footnote </span>
<sup class="sup">c</sup>
</a> The bigram shift dataset labels original sentences as ‘Original’ and inverted sentence as ‘Inverted’. The dataset contains 59,999 ‘Inverted’ sentences and 59,999 ‘Original’ sentences.</p>
<p class="p"> The experiments on the general idiom token identification task are based on the VNIC dataset (Cook <em class="italic">et al.</em> <a class="xref bibr" href="#ref10"><span class="show-for-sr">Reference Cook, Fazly and Stevenson</span>2008</a>). The VNIC dataset is a set of 2,979 English sentences with each sentence containing an instance of one of 53 idiomatic expressions. An idiomatic expression can be used either in an idiomatic or literal sense. The VNIC dataset contains manually annotated labels, where every sentence is marked as ‘Idiomatic usage’, ‘Literal usage’ or ‘Unknown’. If the idiomatic expression in the sentence is used idiomatically then the sentence will be labelled as ‘Idiomatic usage’, if the idiomatic expression is used literally then the sentence will be labelled as ‘Literal usage’, and if the usage is ambiguous for human annotator then the sentence will be marked as ‘Unknown’. The sentences with the ‘Unknown’ label are either idiomatic or literal samples but the human annotator was unable to decide whether it is idiomatic or not. Unfortunately, from a practical point of view, the ‘Unknown’ sentences without a manually annotated label (idiom or literal) are impossible to use for training or for evaluation in a supervised learning setup. Therefore we removed all such sentences from our dataset for our experimentation. The removal of these sentences from the dataset does simplify the task of general idiom token identification with respect to the task that humans processing language face. However, the focus of our analysis is on understanding what linguistic information (topic <em class="italic">vs.</em> non-topic) Transformer-based pre-trained language models like BERT and RoBERTa use when they are processing language, and so removing these ‘Unknown’ sentences may lead to a cleaner signal within the analysis of the performance of these models on the task. This filtering of the VNIC dataset left 2,566 samples of which 2,016 were idiomatic usages and 550 were literal usages across the 53 expressions. When preparing the training and test sets, we split the data by sentence rather than by expression, and so an idiomatic expression may appear in sentences in the training set and the test set.<a class="xref fn" href="#fn4"><span class="show-for-sr">Footnote </span>
<sup class="sup">d</sup>
</a>
</p>
</div>
<div class="sec" data-magellan-destination="s4-2" id="s4-2">

<h3 class="B"><span class="label">4.2</span> Representations</h3>
<p class="p"> For our probing experiments, we generated a distributed representation of each sample sentence by using a pre-trained BERT model (<em class="italic">bert-base-uncased</em>) and a pre-trained RoBERTa model (<em class="italic">roberta-base</em>). Both these models are based on the Transformer encoder architecture with 12 layers, 768 hidden dimensions, and 12 attention heads. Jawahar <em class="italic">et al.</em> (<a class="xref bibr" href="#ref29"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019b</a>) observed that different layers of the BERT architecture capture different kinds of information. So we generated different embeddings from each of the 12 different layers of both BERT and RoBERTa architectures. Our models have 12 layers and therefore we generated 12 different BERT and RoBERTa embeddings for each sample sentence. In each layer, we generated an aggregate distributed representation of the sentence by averaging the distributed representations of each token in the sentence. There are a number of ways that we could have generated a sentence embedding from BERT and RoBERTa, for example, we could have used the CLS token. However, Mosbach <em class="italic">et al.</em> (<a class="xref bibr" href="#ref43"><span class="show-for-sr">Reference Mosbach, Khokhlova, Hedderich and Klakow</span>2020</a>) suggest that, for probing tasks, the average of the embeddings of the token in a sentence is a better sentence-level representation than the embedding of the CLS token. Also, as we explain below, in our experiments we use GloVe as a baseline topic-based distributed representation, and using the average token embedding for BERT and RoBERTa makes the process we use to generate BERT and RoBERTa sentence representations more consistent with the process we use to generate GloVe sentence representations, which is the average of the GloVe representation of all words in the sentence.</p>
<p class="p"> GloVe is a distributed representation based on a word-to-word co-occurrence matrix Pennington <em class="italic">et al.</em> (<a class="xref bibr" href="#ref49"><span class="show-for-sr">Reference Pennington, Socher and Manning</span>2014</a>). This approach is very similar to the topic modelling approaches, particularly to Latent Semantic Indexing, the approach we are using for our probing experiment. So we assume that GloVe embeddings primarily capture topic information of words from a large corpus. To generate the GloVe representation of a sample sentence we averaged the GloVe representations of each word in it. Therefore the GloVe embeddings used in our experiment likely neglect the syntactic structure of the sentence, and this is a deliberate choice as part of our methodology so that GloVe can be used as a metric for the expected performance of an embedding that primarily encodes topic on a probing task. We consider GloVe distributed representations as one of the baseline representations for topic-aware probing. The comparison against the GloVe baseline enables us to estimate the amount of topic and non-topic signal BERT and RoBERTa encode.</p>
<p class="p"> Hewitt and Liang (<a class="xref bibr" href="#ref26"><span class="show-for-sr">Reference Hewitt and Liang</span>2019</a>) warned that the probe itself can learn the task without using the information in the sentence representation especially when the probing model is powerful enough to capture the task objective. To control for this possible confounding factor we also train a baseline probe model for each task on random vector representations to measure the performance due to the power of the probing model. The random vector representations are created by randomly generating a 768-dimensional vector for each of the input sentences (we use 768-dimensional vectors so that the random vectors have the same dimension as the BERT and RoBERTa embeddings).</p>
</div>
<div class="sec" data-magellan-destination="s4-3" id="s4-3">

<h3 class="B"><span class="label">4.3</span> Experimental design</h3>
<p class="p"> In the topic-aware probing with LSI topic modelling, we have to specify the number of topics. If the number of topics is kept small then each topic will be more generic and this may reduce the power of the topic-aware probing, although this may be mitigated by the fact that a small number of topics will also result in larger training and test sets. Conversely, if the number of topics is too large then each topic will be very specific but the training and testing sample size will be reduced and this may result in the underperformance of the probe. This will be reflected in the seen scores and unseen scores and therefore in our analysis.</p>
<p class="p"> In order to control for the confounding effects of the number and size of topics on our analysis, for both probing tasks (bigram shift and general idiom token identification), we repeat our topic-aware probing experiment 10 times while varying the number of topics from 5 to 50 in increments of 5. Consequently, each iteration of an experiment uses a different topic model in the topic-aware probing as the basis for the experiment. Note that as the number of topics approaches 50, the chance that the initial topic model in the topic-aware probing will contain topics with <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline10.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="23" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline10.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$\lt 5$
</span></span>
</span>
</span> samples with some label (i.e., tail topics) increases. The actual number of tail topics from each of the 10 topic models on both the Bigram shift and VNIC datasets is shown in Table <a class="xref table" href="#tbl2">2</a>. In such cases, the actual number of topics used for probing will be less than the specified number of topics after the tail reduction (as discussed in Section <a class="xref sec" href="#s3">3</a>).</p>
<p class="p"> To apply our topic-aware probing we first divide the dataset into different partitions with different topics. The topic model can divide samples with the same class label into the same partition (or same set of partitions) and in such cases, we can see that the topic model itself will internally do the classification task. Similarly, in the case of the general idiom token identification dataset, the topic model can divide samples with the same idiomatic expression into the same partition (or the same set of partitions). We initially checked for such an interaction between topics, labels, and expressions by investigating how evenly the labels and expressions are distributed across different topics. For that, we calculated the <em class="italic">mean normalised entropy</em> of the distributions of each class label and each expression. To calculate the <em class="italic">mean normalised entropy</em> of a distribution, we first calculated the normalised entropy of the distribution across different topics in each of the 10 topic models and then averaged it. The normalised entropy of a probability distribution is the entropy of the distribution normalised with the maximum possible entropy.<a class="xref fn" href="#fn5"><span class="show-for-sr">Footnote </span>
<sup class="sup">e</sup>
</a> An interesting property of normalised entropy is that the value will be in the range of 0 to 1 and it will be independent of the base of the logarithm. If the distribution is uniformly distributed then we will get the maximum normalised entropy 1 and if the distribution is highly skewed (i.e., samples with the same class label are in the same partition or samples with the same expressions are in the same partition) then we will get the minimum entropy 0. In the VNIC dataset, the <em class="italic">mean normalised entropy</em> across topics for the ‘Idiomatic usage’ label was <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline20.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="28" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline20.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.85$
</span></span>
</span>
</span> bits and for the ‘Literal usage’ label was <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline21.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="27" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline21.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.81$
</span></span>
</span>
</span> bits. In the bigram shift dataset the <em class="italic">mean normalised entropy</em> across topics for the ‘Original’ label and the ‘Inverted’ label was <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline22.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="28" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline22.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.86$
</span></span>
</span>
</span> bits. These large entropy values indicate that on average both labels in both datasets are relatively evenly distributed across topics (i.e., the topic model process was neither doing general idiom token identification nor bigram shift classification). The average <em class="italic">mean normalised entropy</em> of distributions from different idiomatic expressions in the VNIC dataset is <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline23.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="28" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline23.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.39$
</span></span>
</span>
</span> bits. This entropy is lower than that of the label distributions. Also, we note that there are a few expressions that have a <em class="italic">mean normalised entropy</em> lower than <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline24.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="20" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline24.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.2$
</span></span>
</span>
</span> bits and for one expression the <em class="italic">mean normalised entropy</em> is less than <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline25.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="19" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline25.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.1$
</span></span>
</span>
</span>. This suggests that for these expressions with low entropy, most of the samples are grouped into the same topic. One intuitive explanation for this is that for each expression the literal instances tend to cluster within a topic and the idiomatic instances tend to be distributed across topics. To test this intuition we calculated the <em class="italic">mean normalised entropy</em> of the distribution of the expressions across topics when we consider only the literal instances of the expression, this entropy was found to be <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline26.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="44" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline26.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.3583$
</span></span>
</span>
</span>, and when we only consider the idiomatic instances the entropy was <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline27.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="44" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline27.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.4098$
</span></span>
</span>
</span>. The fact that in general, the distribution of an expression’s idiomatic instances across topics has a higher entropy than the literal instances suggests that our intuition is correct.</p>
<p class="p"> For each topic-aware probing (i.e., for each combination of task plus embedding) with <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline28.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline28.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> topics, we get <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline29.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline29.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> seen and <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline30.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline30.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> unseen scores (each being an average across 5 folds). We have averaged (a micro average) these seen scores and unseen scores from all topic models to calculate an aggregate seen score and unseen score. We then calculate the average difference by taking the difference between these two averages.<a class="xref fn" href="#fn6"><span class="show-for-sr">Footnote </span>
<sup class="sup">f</sup>
</a> If the average difference is positive and not a negligibly small value, this is an indication that the topic signal contributes to the performance of the probing model on the task. Our experimental design is illustrated in Figure <a class="xref fig" href="#f2">2</a>.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f2" id="f2">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig2.png?pub-status=live" class="aop-lazy-load-image" width="3501" height="2306" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig2.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 2.</span> Experimental design.</p>
</div></div></section>

<p class="p"> The label distribution in the VNIC dataset for general idiom token identification is somewhat imbalanced. Most imbalanced datasets contain more negative samples and fewer positive samples. However, somewhat unusually in this case, there are more positive samples (idiomatic usage) than negative samples (2566 idiomatic and 550 literal usages). Savary <em class="italic">et al.</em> (<a class="xref bibr" href="#ref60"><span class="show-for-sr">Reference Savary, Cordeiro, Lichte, Ramisch, nurrieta and Giouli</span>2019</a>) proposed heuristics to automatically identify sample sentences that might contain literal occurrences of MWE and this approach can be considered for balancing the general idiom token identification dataset. However, using this approach would still require manual verification and therefore in our experimentation we used the VNIC dataset as it is and selected an evaluation metric that accounts for imbalanced distributions. Accuracy is a popular and intuitive evaluation metric, but it is not suitable for an imbalanced dataset: if 99 samples out of 100 are idiomatic, a model could report high accuracy by blindly labelling every sample as positive. The F1 score and Area Under Precision Recall curve (AUC-PR) are both suitable for the standard imbalanced scenario where the positive class is the minority, this is because they both focus on the identification of positive samples. However, because they both exclusively focus on the performance of the positive class they are not suitable when the positive class is the majority. In this context, although we could use the F1 scores as our evaluation metric by treating the literal class as the positive class, doing this would essentially change the task to literal token identification and so would add an extra layer of complexity to the interpretation of the results in terms of idiom identification. An alternative is to use other metrics that are suitable for imbalanced datasets that consider both the positive and the negative classes. The most suitable evaluation metrics for an imbalanced dataset with a majority of positive instances are the AUC ROC and Mathew Correlation Coefficient (MCC). An empirical comparative study by Halimu <em class="italic">et al.</em> (<a class="xref bibr" href="#ref22"><span class="show-for-sr">Reference Halimu, Kasem and Newaz</span>2019</a>) showed that both AUC ROC and MCC are statistically consistent with each other, however, AUC ROC is more discriminating than MCC. Therefore we selected the AUC ROC as the most suitable metric for these experiments.</p>
<p class="p"> For this experiment, we used the gensim<a class="xref fn" href="#fn7"><span class="show-for-sr">Footnote </span>
<sup class="sup">g</sup>
</a> library implementation of Latent Semantic Indexing with default parameters for training the topic model. We used the bigram phrase model, tf-idf model, and data lemmatisation from gensim library to prepare the corpus for training the LSI model. For training the MLP probing model we used the scikit-learn Pedregosa <em class="italic">et al.</em> (<a class="xref bibr" href="#ref47"><span class="show-for-sr">Reference Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot and Duchesnay</span>2011</a>) implementation of MLP with default parameters.</p>
</div>
</div>
<div class="sec results" data-magellan-destination="s5" id="s5">

<h2 class="A"><span class="label">5.</span> Results and discussion</h2>
<p class="p"> We report results obtained from our probing experiments: micro-averaged seen and unseen AUC ROC scores, and the differences between seen and unseen scores for each representation in Table <a class="xref table" href="#tbl3">3</a>.</p>
<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl3" id="tbl3">

<div class="caption">
<p class="p"><span class="label">Table 3.</span> Average seen and unseen AUC ROC scores and their differences along with standard deviations for different embeddings on the Bigram Shift Probing task and the General Idiom Token Identification task</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png?pub-status=live" class="aop-lazy-load-image" width="666" height="820" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png" data-zoomable="true"></div>
</span>
</div>

<p class="p"> Focusing first on the bigram shift task, we observe very small differences in performance between seen AUC scores and unseen AUC scores using any of the representations (GloVe, BERT, or RoBERTa) with a maximum 2.55% difference, and in most of the cases less than 1% difference. This is evident in Figure <a class="xref fig" href="#f3">3</a> where the GloVe seen and unseen scores are plotted on top of each other as are the BERT and RoBERTa seen and unseen scores across all the layers. This is an expected result (see Section <a class="xref sec" href="#s4">4</a>) and one that we take to indicate that topic-aware probing works as expected. Furthermore, this result also indicates that the bigram shift task is not sensitive to a topic signal.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f3" id="f3">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig3.png?pub-status=live" class="aop-lazy-load-image" width="4062" height="1300" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig3.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 3.</span> Seen and Unseen AUC ROC scores from GloVe and different layers of BERT and RoBERTa on the Bigram Shift Task.</p>
</div></div></section>

<p class="p"> Given that the bigram shift task does not appear to be sensitive to the topic signal it is interesting to observe that GloVe embeddings and random embeddings give the same performance on this task. If GloVe encodes non-topic signals along with topic signals then we would expect that this information would help the GloVe-based probing model to achieve better performance (as compared to a random embedding) on a non-topic sensitive task such as bigram shift. But in this case, GloVe has the same performance as the random baseline, which suggests that GloVe embeddings do not encode non-topic signals. This is in line with our assumption that GloVe primarily captures the topic signal in a text (Section <a class="xref sec" href="#s4-2">4.2</a>).</p>
<p class="p"> From the bigram shift results on BERT representations, we can observe that the initial layer of BERT (BERT0) and GloVe have similar performance (same as that of random embedding). Similar to our previous argument, if the initial layer of BERT encodes non-topic signals along with topic signals then we would expect that the probing model achieves a better performance (as compared to a random embedding) on a non-topic sensitive task such as bigram shift. So similar to GloVe representation, we can argue that the initial layer of BERT also primarily encodes the topic signal. But, when we look at the RoBERTa results on the bigram shift task, the initial layer achieves better performance than the GloVe baseline. This suggests that, unlike BERT, RoBERTa encodes some non-topic signals even in the initial layer. But, both BERT and RoBERTa improve their performance by using their later layers and we hypothesise that this is because both the models encode more non-topic signals in their later layers. In that case, the difference in performance between GloVe and the later layers of BERT and RoBERTa can be attributed to the encoded non-topic information that is useful to this non-topic sensitive task, for example, syntactic information. When we compare the later layer performances of BERT and RoBERTa, BERT’s performance converges with that of RoBERTa and BERT’s best seen score performance of 0.9447 AUC surpasses the best RoBERTa seen score of 0.9390 AUC. This suggests that even though BERT encodes less (or no) non-topic signal in its initial layers, compared to RoBERTa it encodes more non-topic signal in its later layers. Note that the drop in performance in the final layers of BERT and RoBERTa for the bigram shift probing task is likely due to the fine-tuning of the embeddings in these layers to the specific tasks that BERT and RoBERTa was trained on, namely: masked language modelling (in the case of both BERT and RoBERTa) and next-sentence prediction (in the case of BERT). This pattern of performance drop is also reported in other layer-wise studies of BERT (see e.g., Jawahar <em class="italic">et al.</em> (<a class="xref bibr" href="#ref29"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019b</a>)). We also observe a similar drop in performance in the last layers of BERT and RoBERTa for the general idiom token identification task and attribute the same root cause to it for that task.</p>
<p class="p"> The results from the bigram shift task also suggest two methods for assessing the sensitivity of a probing task to topic information. The first method is to consider the difference between a probe’s performance on a task when it is trained on random embeddings versus GloVe embeddings. Tasks which have the same performance on random embeddings and GloVe embeddings are likely insensitive to the topic information (such as is the case with the bigram shift task). A corollary to this is that tasks for which there is a large difference in performance between GloVe and random embeddings are likely to be sensitive to information relating to topic. The second method to measure a task’s sensitivity to the topic information is to use the difference between seen and unseen topic scores. For example, on the bigram shift task, the difference between seen and unseen topic scores is negligible for GloVe, and also for each of the layers of BERT and RoBERTa. Later in the paper, we will use both of these methods (GloVe <em class="italic">vs.</em> random, and seen <em class="italic">vs.</em> unseen topic scores) to assess task sensitivity to the topic information, and we will show across a range of probing tasks that these two methods are highly correlated.</p>
<p class="p"> Switching focus to the results for the general idiom token identification task listed in Table <a class="xref table" href="#tbl3">3</a> there are very large (as compared with the bigram shift task) performance differences between average seen and unseen topic scores on GloVe embeddings with 11.77% difference (as compared to <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline31.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="37" height="13" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline31.png" data-zoomable="false">
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$\lt 1\%$
</span></span>
</span>
</span> difference on GloVe for bigram shift) and all 12 layers of BERT and RoBERTa embeddings with all differences in the range of 8.81% to 13.34% (as compared with a maximum difference of 2.55% for the bigram shift task). This difference in performance between seen and unseen topics is also apparent in Figure <a class="xref fig" href="#f4">4</a>. This difference indicates the importance of the topic signal to the task of general idiom token identification. We also observed a large standard deviation for seen and unseen scores including for random embedding and we believe that this is an artefact of the relatively small dataset (2,566 sentences) used for the general idiom token identification task experiments.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f4" id="f4">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig4.png?pub-status=live" class="aop-lazy-load-image" width="4064" height="1302" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig4.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 4.</span> Seen and Unseen AUC ROC scores from different layers of BERT and RoBERTa with GloVe baseline on General Idiom Token Identification Task.</p>
</div></div></section>

<p class="p"> The fact that the task of general idiom token identification is sensitive to topic signal is also evidenced by the fact that GloVe performs much better than random on both seen topic and unseen topic samples. Furthermore, the difference in performance for GloVe embeddings between seen topic and unseen topic samples also reflects the ability of GloVe to encode the topic information. Also, from the results for the bigram shift task, we noted that GloVe had a similar performance to BERT0. We see a similar pattern of results here, with GloVe and BERT0 embeddings resulting in a similar performance for both the seen and unseen probing conditions. This reinforces our earlier observation that BERT0 may be primarily encoding the topic information but also suggests that GloVe and BERT generally have a similar capacity to encode the topic signal (with BERT being slightly better at capturing this signal). In the case of RoBERTa, we observed that the initial layer encodes some non-topic signal in the bigram shift task results. But, from the results for general idiom identification on RoBERTa, we can see that the initial layer performance is similar to BERT’s initial layer and GloVe. This means that, even though the first layer of RoBERTa encodes some non-topic signal, that non-topical information is not that useful for the idiom identification task. Also, as with the bigram shift task, we can observe on the general idiom token identification task an improvement in the performance of BERT and RoBERTa embeddings as we move into deeper layers, with the best BERT performance being BERT7 and the best RoBERTa performance being RoBERTa9 (on both seen and unseen conditions). This suggests that similar to our observation on the bigram shift task, the improvement in performance observed in BERT’s and RoBERTa’s deeper layers is attributable to non-topic-based information encoded in BERT and RoBERTa. This hypothesis is reinforced by the fact that the improvement in performance across BERT and RoBERTa layers is similar across both the seen and unseen conditions (i.e., an improvement in one layer for the seen is matched by a similar improvement for the same layer for the unseen condition). Indeed, Figure <a class="xref fig" href="#f5">5</a> plots the difference in BERT’s and RoBERTa’a performance between seen and unseen conditions across the different layers and highlights that this difference is relatively stable and similar to the difference between GloVe for seen and unseen. It is also important to note that the difference between seen score and unseen score of all RoBERTa layers is consistently greater than that of the corresponding layers of BERT. This suggests that even though RoBERTa achieves a better performance than BERT on general idiom token identification it is more sensitive to the topic information.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f5" id="f5">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig5.png?pub-status=live" class="aop-lazy-load-image" width="3126" height="2181" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig5.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 5.</span> Difference between seen scores and unseen scores from different layers of BERT and RoBERTa on General Idiom Token Identification Task.</p>
</div></div></section>

<p class="p"> To summarise, our results suggest that (a) the bigram shift task is not sensitive to the topic information whereas the general idiom token identification task is; (b) the GloVe embeddings primarily encode topic information; (c) that initial layer embeddings of BERT (not RoBERTa) behave similarly to GloVe embeddings; (d) that later layers of BERT and RoBERTa encode non-topic information that is useful for both bigram shift and general idiom token identification (we hypothesise that this information may be syntactic in nature); (e) that both BERT and RoBERTa are sensitive to the topic information on general idiom identification.</p>
</div>
<div class="sec other" data-magellan-destination="s6" id="s6">

<h2 class="A"><span class="label">6.</span> Other probing tasks</h2>
<p class="p"> Broadening the focus beyond predicting idiomatic usage, we have analysed the relationship between topic, task, and the performance of probing models trained on transformer-generated embeddings across a range of standard probing tasks. We selected 8 probing tasks introduced by Conneau <em class="italic">et al.</em> (<a class="xref bibr" href="#ref8"><span class="show-for-sr">Reference Conneau, Kruszewski, Lample, Barrault and Baroni</span>2018</a>) and the explanation and summary statistics of the dataset of each of these are shown in Table <a class="xref table" href="#tbl4">4</a>. We did a topic-aware probing on each of these 8 probing tasks and the number of tail topics obtained from different topic models on different datasets are shown in Table <a class="xref table" href="#tbl5">5</a>. Figures <a class="xref fig" href="#f6">6</a> and <a class="xref fig" href="#f7">7</a> show for the 8 probing tasks the performance of GloVe and the different layers of BERT and RoBERTa on each of the tasks. Looking at Figures <a class="xref fig" href="#f6">6</a> and <a class="xref fig" href="#f7">7</a>, a number of general observations can be made. First for nearly all the tasks—with the exception of Sentence Length, and Object Number (to a lesser degree)—the performance of the initial layer of BERT and GloVe is very similar in both the unseen and seen conditions. It is worth noting that in this set of probing tasks, the Sentence Length dataset is distinctive because the sentences for this task are not controlled for sentence length whereas sentences used for all the other tasks have a similar length. As pointed out by Adi <em class="italic">et al.</em> (<a class="xref bibr" href="#ref2"><span class="show-for-sr">Reference Adi, Kermany, Belinkov, Lavi and Goldberg</span>2017</a>), sentence length can have a significant impact on the norm of the sentence embedding. Norms of sentence embeddings from Sentence Length datasets generated by averaging the embeddings of each word in the sentence can have a higher variance compared to other datasets, and this variance may be the reason for the different behaviour observed for the Sentence Length probing task. Deviating from this pattern, the initial layer of RoBERTa achieves better performance than GloVe and the initial layer of BERT on a number of probing tasks. But for some tasks like SOMO, Coordination Inversion, and Past-Present up to some extent (and for general idiom identification task in Section <a class="xref sec" href="#s5">5</a>), the initial layer of RoBERTa shows similar performance to that of GloVe and the initial layer of BERT. Second, in all tasks for GloVe, BERT, and RoBERTa, the performance in the seen condition is better than the unseen condition. Third, the difference in performance between the seen and unseen conditions remains relatively stable across all the layers of BERT and RoBERTa.</p>
<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl4" id="tbl4">

<div class="caption">
<p class="p"><span class="label">Table 4.</span> Descriptions and summary statistics of the datasets for the VNIC, Bigram shift, and 8 other probing tasks</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png?pub-status=live" class="aop-lazy-load-image" width="666" height="451" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png" data-zoomable="true"></div>
</span>
</div>

<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl5" id="tbl5">

<div class="caption">
<p class="p"><span class="label">Table 5.</span> Number of tail topics from 10 topic models on datasets of other 8 probing tasks</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png?pub-status=live" class="aop-lazy-load-image" width="666" height="260" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png" data-zoomable="true"></div>
</span>
</div>

<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f6" id="f6">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig6.png?pub-status=live" class="aop-lazy-load-image" width="3367" height="5625" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig6.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 6.</span> Seen and Unseen AUC ROC scores from different layers of BERT with GloVe baseline on Probing Tasks.</p>
</div></div></section>

<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f7" id="f7">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig7.png?pub-status=live" class="aop-lazy-load-image" width="3373" height="5625" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig7.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 7.</span> Seen and Unseen AUC ROC scores from different layers of RoBERTa with GloVe baseline on Probing Tasks.</p>
</div></div></section>

<p class="p"> For each task, we list the average seen score obtained from random embeddings, the average GloVe seen and unseen scores along with their difference, and the average seen and unseen scores and their difference from the best BERT layer and the best RoBERTa layer in Table <a class="xref table" href="#tbl6">6</a>. The best BERT layer and RoBERTa layer are the layers which obtain the best average seen AUC ROC scores, and in most of the tasks (except a very small difference in Subject Number and Tree Depth for BERT and Past-Present, Tree Depth, and Object Number for RoBERTa), the best BERT layer and RoBERTa layer also gives the best average unseen AUC ROC score. For all these exceptional cases, the difference between the best unseen score and the unseen score from the best layer is negligibly small (0.0003 for Subject Number and 0.0024 for Tree Depth on BERT best layers; 0.0019 for Tree Depth, 0.0014 for Past-Present, and 0.0041 for Object Number on RoBERTa best layers).</p>
<p class="p"> 
</p><div class="table-wrap" data-magellan-destination="tbl6" id="tbl6">

<div class="caption">
<p class="p"><span class="label">Table 6.</span> Average seen and unseen AUC ROC scores and their differences for GloVe and best BERT and RoBERTa layer embeddings on different probing tasks—tasks are ranked in the descending order of the difference between GloVe Seen score and GloVe Unseen score</p>
</div>
<span>

<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png?pub-status=live" class="aop-lazy-load-image" width="666" height="372" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png" data-zoomable="true"></div>
</span>
</div>

<p class="p"> Earlier we proposed two metrics derived from the topic-aware probing methodology for measuring the sensitivity of a probing task to the topic signal. The first metric is the difference between seen and unseen scores from the topic-aware probe, the larger the difference the more sensitive the task to the topic signal. The second method is based on the premise that GloVe embeddings primarily encode topic information, and so the difference between the seen scores on a task obtained from GloVe embeddings and from random embeddings can be considered as another measure of the sensitivity of a task to topic information. If both these measures are true indications of the topic sensitivity of tasks, then there should be a high correlation between them across tasks. We used the set of selected probing tasks to verify whether these measures are correlated or not. From the scores reported in Table <a class="xref table" href="#tbl6">6</a>, we calculated the correlation between these two measures of topic sensitivity from all probing tasks, that is (a) the difference between GloVe seen scores and Random seen scores (b) the difference between GloVe seen scores and GloVe unseen scores. We found a very high correlation coefficient, 0.80, between these measures which indicates that both these measures of task sensitivity to the topic information are consistent with each other.</p>
<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f8" id="f8">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig8.png?pub-status=live" class="aop-lazy-load-image" width="3251" height="2142" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig8.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 8.</span> GloVe Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task (Note that scores of SOMO and CI are very similar and therefore both of them are overlapping in the plot).</p>
</div></div></section>

<p class="p"> 
</p><section><div class="fig" data-magellan-destination="f9" id="f9">


<div class="figure-thumb"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig9.png?pub-status=live" class="aop-lazy-load-image" width="3249" height="2589" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig9.png" data-zoomable="true"></div>
<div class="caption"><p class="p"> 
</p><p class="p"><span class="label">Figure 9.</span> BERT and RoBERTa Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task.</p>
</div></div></section>

<p class="p"> By taking the difference between GloVe seen performance and GloVe unseen performance as the measure of topic sensitivity, we generated two scatter plots to check how the performance of GloVe and Transformer-based neural language models (BERT and RoBERTa) varies across different tasks with different topic sensitivity. Figure <a class="xref fig" href="#f8">8</a> plots for each task the GloVe seen performance versus the difference between GloVe seen and unseen and Figure <a class="xref fig" href="#f9">9</a> plots for each task the best performance by BERT and RoBERTa (for any layer) versus the difference between GloVe seen and unseen. In these scatter plots, we can see a general trend that as sensitivity to topic increases (i.e., the difference between GloVe seen and unseen gets larger and we move to the right on the x-axis) there is a tendency for the performance of GloVe and Transformer-based neural language models (BERT and RoBERTa) to increase. This suggests that the less sensitive a probing task is to topic information the more difficult the task is for Transformer-based neural language models (BERT and RoBERTa). We also calculated the correlation coefficients between performances of neural language models (seen scores) and topic sensitivity scores (difference between GloVe seen and unseen score), and we found a coefficient of 0.2829 for BERT and 0.3345 for RoBERTa. This suggests that neural language models rely on topical information to solve different tasks.</p>
<p class="p"> When we compare the BERT and RoBERTa neural language models, we observe slightly more topic reliance for RoBERTa than BERT. This difference is evident in Figure <a class="xref fig" href="#f9">9</a> (steeper trend line for RoBERTa) and in a slightly higher value of correlation coefficient for RoBERTa than BERT (0.3345 <em class="italic">vs.</em> 0.2829). Our results indicate that newer and generally better neural language models, such as RoBERTa, are more reliant on topic information as compared with BERT. We will return to this point in our conclusions.</p>
</div>
<div class="sec conclusions" data-magellan-destination="s7" id="s7">

<h2 class="A"><span class="label">7.</span> Conclusions</h2>
<p class="p"> We proposed a topic-aware probing method to measure the role of the topic signal in distributed representations and validated this method using baseline representations (GloVe and random) and a baseline task (bigram shift). The results of our analysis on the bigram shift probing task supported our hypothesis that GloVe embeddings primarily encode topic information and furthermore suggested that the initial layer of BERT also primarily encodes topic information and that later layers of BERT, and all layers of RoBERTa encode non-topic information (the observation that this information was non-topic related is based on the fact that this information was useful for the non-topic sensitive probing task of bigram shift).</p>
<p class="p"> With respect to the task of general idiom token identification, previous research has pointed to the importance of topic information for identifying idiomatic usage (Feldman <em class="italic">et al.</em> <a class="xref bibr" href="#ref16"><span class="show-for-sr">Reference Feldman and Peng</span>2013</a>; Peng <em class="italic">et al.</em> <a class="xref bibr" href="#ref48"><span class="show-for-sr">Reference Peng, Feldman and Vylomova</span>2014</a>). However, work such as by Fazly <em class="italic">et al.</em> (<a class="xref bibr" href="#ref15"><span class="show-for-sr">Reference Fazly, Cook and Stevenson</span>2009</a>) has highlighted the importance of non-topic information for idiom token identification and the results of Salton <em class="italic">et al.</em> (<a class="xref bibr" href="#ref59"><span class="show-for-sr">Reference Salton, Ross and Kelleher</span>2016</a>) suggested that by using distributed embeddings it is possible to create a general idiom token identification model (i.e., a model that works across multiple expressions within a category) without requiring large amounts of topic information. The results of our topic-aware probing experiments confirm the importance of the topic signal to the task of general idiom token identification. One potential reason for the topic signal being important for general idiomatic token identification systems is that distinctions between topics may align with seen versus unseen expressions. Our analysis of the distribution of expressions across topics (see Section <a class="xref sec" href="#s4-3">4.3</a>) suggested that for many expressions, the sample sentences containing the expression tend to cluster within a topic, particularly the literal uses of an expression. Consequently, training a probe on one topic and testing it on other topics is similar to training on one set of expressions and testing on another set of expressions. Indeed, Nedumpozhimana <em class="italic">et al.</em> (<a class="xref bibr" href="#ref45"><span class="show-for-sr">Reference Nedumpozhimana, Klubička and Kelleher</span>2022</a>) report an expression-based analysis of idiom token identification that includes a seen versus unseen experiment. The results of that experiment also reported a drop in performance, in this case from seen expressions to unseen expressions rather than seen versus unseen topics. The fact that the distinction between seen and unseen topics and seen and unseen expressions overlaps is not surprising. However, the overlap does not mean that the phenomena are identical. For example, whereas the expression-based distinction is solely based on the words within an expression, the definition of topic in our experimentation considers all words in a sentence. The importance of considering both the information within an expression and the surrounding context for idiom token identification has been demonstrated by Nedumpozhimana and Kelleher (<a class="xref bibr" href="#ref44"><span class="show-for-sr">Reference Nedumpozhimana and Kelleher</span>2021</a>) who show that BERT not only relies on information within the expression but also in the surrounding context.</p>
<p class="p"> Switching to our primary question of the extent to which Transformer-based language models rely on word order/syntactic information versus word co-occurrence/topic information, the results of our topic-aware probing experiments suggest that BERT and RoBERTa encode both topic and non-topic information. This is indicated by the fact that across all the probing tasks the embeddings generated by BERT and RoBERTa’s middle layers result in a higher performance than GloVe, see Figure <a class="xref fig" href="#f6">6</a>. These results are in line with various layer-wise studies on BERT in the literature, such as by Jawahar <em class="italic">et al.</em> (<a class="xref bibr" href="#ref29"><span class="show-for-sr">Reference Jawahar, Sagot and Seddah</span>2019b</a>), which suggest that the syntactic features of a sentence are encoded in the middle layers of BERT. However, despite the fact that BERT can capture useful non-topic information our analysis suggests that in general BERT (and RoBERTa) primarily rely on the topic information. Furthermore, our analysis of BERT and RoBERTa’s performance across a set of standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for BERT and RoBERTa. These observations agree with the findings of Pham <em class="italic">et al.</em> (<a class="xref bibr" href="#ref50"><span class="show-for-sr">Reference Pham, Bui, Mai and Nguyen</span>2021</a>), in which they observe that most of the BERT-based models behave similarly to bag-of-word models on GLUE tasks. They also agree with Sinha <em class="italic">et al.</em> (<a class="xref bibr" href="#ref63"><span class="show-for-sr">Reference Sinha, Jia, Hupkes, Pineau, Williams and Kiela</span>2021a</a>) who argue that the success of pre-trained language models on many tasks is primarily based on their ability to encode distributional information.</p>
<p class="p"> From our experiments, we observed that the RoBERTa model is more reliant on the topic information than the BERT model. One of the most notable differences between BERT and RoBERTa is in the pretraining objectives used for the two models. BERT is trained on the masked language model and the next-sentence prediction objectives, whereas the RoBERTa model excludes the next-sentence prediction objective. Mickus <em class="italic">et al.</em> (<a class="xref bibr" href="#ref42"><span class="show-for-sr">Reference Mickus, Paperno, Constant and van Deemter</span>2020</a>) argue that the use of a next-sentence prediction objective adulterates the distributional nature of the semantics learned by BERT. Building on this argument, the removal of the next-sentence prediction from the pretraining objective for RoBERTa may result in the RoBERTa model being more focused towards distributional semantics, and this may explain the relatively stronger reliance of RoBERTa on topic compared to BERT.</p>
<p class="p"> The broader implications of our findings for NLP are that the performance of Transformer-based systems on NLP tasks can be improved by incorporating more word order or syntactic information into these language models. Wang <em class="italic">et al.</em> (<a class="xref bibr" href="#ref71"><span class="show-for-sr">Reference Wang, Bi, Yan, Wu, Xia, Bao, Peng and Si</span>2020</a>) is an example of recent work that attempts to do this. They show that adding word-order and sentence-order learning objectives into BERT pretraining could lead to improved performance on language processing tasks. Also, Pham <em class="italic">et al.</em> (<a class="xref bibr" href="#ref50"><span class="show-for-sr">Reference Pham, Bui, Mai and Nguyen</span>2021</a>) show that fine-tuning on a word-order-sensitive task prior to fine-tuning on a downstream task increases a language model’s sensitivity to word order on the downstream task. Another potential way forward in this direction is to explicitly integrate syntactic information into the language modelling architecture, for example by using methods like Recursive Neural Network in which the representation of a sentence is composed of representations of words by applying compositions recursively through the parsed tree (Socher <em class="italic">et al.</em> <a class="xref bibr" href="#ref65"><span class="show-for-sr">Reference Socher, Perelygin, Wu, Chuang, Manning, Ng and Potts</span>2013</a>). Such approaches utilise compositional semantics which can be helpful to capture more non-distributional semantics in language models.</p>
<p class="p"> There are a number of limitations of our analysis that should be noted. The first is that the analysis is based on two neural language models BERT and RoBERTa, both are based on Transformer encoder architecture. Recent neural language models like GPT, Llama, etc. are based on Transformer decoder architecture. It is natural to ask whether we can extend our study to neural language models with Transformer decoder architecture and whether the observations from our experiment can be applicable to those models. Also, our analyses are based only on English datasets which have relatively fixed word order. It would be interesting to examine whether the observations we have drawn from our experiments are limited to the English language or whether the reliance of Transformer-based language models on topic as strong for other languages that have a relatively more flexible word order and richer morphology. In future work, we will extend our experiments by using different neural language models based on the Transformer decoder architecture and different datasets like PARSEME (Savary <em class="italic">et al.</em> <a class="xref bibr" href="#ref61"><span class="show-for-sr">Reference Savary, Ramisch, Cordeiro, Sangati, Vincze, QasemiZadeh, Candito, Cap, Giouli, Stoyanova and Doucet</span>2017</a>; Ramisch <em class="italic">et al.</em> <a class="xref bibr" href="#ref53"><span class="show-for-sr">Reference Ramisch, Cordeiro, Savary, Vincze, Barbu Mititelu, Bhatia, Buljan, Candito, Gantar, Giouli, Güngör, Hawwari, Iñurrieta, Kovalevskaitė, Krek, Lichte, Liebeskind, Monti, Parra Escartín, QasemiZadeh, Ramisch, Schneider, Stoyanova, Vaidya and Walsh</span>2018</a>, <a class="xref bibr" href="#ref54"><span class="show-for-sr">Reference Ramisch, Savary, Guillaume, Waszczuk, Candito, Vaidya, Barbu Mititelu, Bhatia, Iñurrieta, Giouli, Güngör, Jiang, Lichte, Liebeskind, Monti, Ramisch, Stymne, Walsh and Xu</span>2020</a>), MAGPIE (Haagsma, Bos, and Nissim <a class="xref bibr" href="#ref21"><span class="show-for-sr">Reference Haagsma, Bos and Nissim</span>2020</a>), etc., which contains both English and Non-English data to address these questions.</p>
<p class="p"> Our findings regarding the topic reliance of neural language models have implications for NLP tasks such as machine translation and question answering. For example, the findings of Amponsah-Kaakyire <em class="italic">et al.</em> (<a class="xref bibr" href="#ref3"><span class="show-for-sr">Reference Amponsah-Kaakyire, Pylypenko, Genabith and España-Bonet</span>2022</a>) indicate that part of BERT’s performance on the task of identifying translationese is due to topic differences learned by BERT. So, another direction for future work is to use topic-aware probing to investigate the topic reliance of neural language models on identifying translationese.</p>
</div>
</div>
<div class="back">
<div class="sec coi-statement" data-magellan-destination="s50" id="s50">
<h2 class="A"> Competing interests</h2>
<p class="p"> The authors declare none</p>
</div>


</div>
</div></div> <hr aria-hidden="true" class="list-divider separator default" data-v-7036083a> <div id="footnotes-list" class="circle-list"><h2>Footnotes</h2>  <div data-type="fulltextNote" id="fn1" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">a</span> Note that in previous work, Nedumpozhimana <em class="italic">et al.</em> (<a class="xref bibr" href="#ref45"><span class="show-for-sr">Reference Nedumpozhimana, Klubička and Kelleher</span>2022</a>) have demonstrated that it is possible to train a model to generalise from a set of known (trained on) idiomatic expressions within a category to unknown (unseen during training) idiomatic expressions from the same category.</p>
</div></div></div><div data-type="fulltextNote" id="fn2" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">b</span> Our code is available at <a class="uri" href="https://github.com/vasudev2020/BERTAnalysis">https://github.com/vasudev2020/BERTAnalysis</a>
</p>
</div></div></div><div data-type="fulltextNote" id="fn3" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">c</span> 
<a class="uri" href="https://github.com/facebookresearch/SentEval/tree/master/data/probing">https://github.com/facebookresearch/SentEval/tree/master/data/probing</a>
</p>
</div></div></div><div data-type="fulltextNote" id="fn4" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">d</span> There are multiple samples for each of the 53 idiomatic expressions in the general idiom token identification dataset—some marked as ‘Idiomatic usage’, others as ‘Literal usage’—and these samples may be distributed across multiple topics. Consequently, some topics may include both ‘Idiomatic usage’ and ‘Literal usage’ samples for some expressions, or only samples of one type for an expression, or no samples for a given expression. Consequently, the seen topic versus unseen topic distinction is different to the seen expression versus unseen expression distinction examined by Salton <em class="italic">et al.</em> (<a class="xref bibr" href="#ref59"><span class="show-for-sr">Reference Salton, Ross and Kelleher</span>2016</a>) (which is not our focus in this paper).</p>
</div></div></div><div data-type="fulltextNote" id="fn5" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">e</span> The normalised entropy of a label (or an expression) distribution <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline11.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="10" height="14" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline11.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$p$
</span></span>
</span>
</span> across a set of <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline12.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="9" height="8" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline12.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$n$
</span></span>
</span>
</span> topics is <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline13.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="139" height="29" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline13.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$ -\sum _{i=1}^{n} \frac{p(x_i)log_b(p(x_i))}{log_b(n)}$
</span></span>
</span>
</span>. For example the normalised entropy of distribution: <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline14.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="48" height="17" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline14.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$(1,0,0)$
</span></span>
</span>
</span> will be <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline15.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="8" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline15.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0$
</span></span>
</span>
</span>, <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline16.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="56" height="22" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline16.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$\big(\frac{1}{3},\frac{1}{3},\frac{1}{3}\big)$
</span></span>
</span>
</span> will be <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline17.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="6" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline17.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$1$
</span></span>
</span>
</span>, and <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline18.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="53" height="22" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline18.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$\big(\frac{1}{2},\frac{1}{2},0\big)$
</span></span>
</span>
</span> will be <span data-mathjax-status="alt-graphic" class="inline-formula">
<span class="alternatives">
<img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline19.png?pub-status=live" class="aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off" width="28" height="12" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline19.png" data-zoomable="false" />
<span class="mathjax-tex-wrapper" data-mathjax-type="texmath"><span class="tex-math mathjax-tex-math mathjax-on">
$0.63$
</span></span>
</span>
</span>
</p>
</div></div></div><div data-type="fulltextNote" id="fn6" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">f</span> Note that the difference between the averages is the same as the average of the differences.</p>
</div></div></div><div data-type="fulltextNote" id="fn7" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><!----></div> <div class="circle-list__item__grouped"><div class="circle-list__item__grouped__content">

<p class="p"><span class="label">g</span> 
<a class="uri" href="https://radimrehurek.com/gensim/">https://radimrehurek.com/gensim/</a>
</p>
</div></div></div></div> <hr aria-hidden="true" class="list-divider separator default" data-v-7036083a> <div id="references-list" class="circle-list"><h2>References</h2> <div id="ref1" aria-flowto="reference-1-content reference-1-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 1 in the content" id="reference-1-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-1-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Abdou</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Ravishankar</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Kulmizev</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Søgaard</span>, <span class="given-names">A.</span></span> (<span class="year">2022</span>). Word order does matter and shuffled language models know it. In <em class="italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, <span class="publisher-loc">Dublin, Ireland</span>: Association for Computational Linguistics, pp. <span class="fpage">6907</span>–<span class="lpage">6919</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.' href=https://dx.doi.org/10.18653/v1/2022.acl-long.476>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.' href=https://scholar.google.com/scholar?q=Abdou,+M.,+Ravishankar,+V.,+Kulmizev,+A.+and+Søgaard,+A.+(2022).+Word+order+does+matter+and+shuffled+language+models+know+it.+In+Proceedings+of+the+60th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Dublin,+Ireland:+Association+for+Computational+Linguistics,+pp.+6907–6919.>Google Scholar</a></div></div></div><div id="ref2" aria-flowto="reference-2-content reference-2-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 2 in the content" id="reference-2-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-2-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Adi</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Kermany</span>, <span class="given-names">E.</span></span>, <span class="string-name"><span class="surname">Belinkov</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Lavi</span>, <span class="given-names">O.</span></span> and <span class="string-name"><span class="surname">Goldberg</span>, <span class="given-names">Y.</span></span> (<span class="year">2017</span>). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, <em class="italic">5th International Conference on Learning Representations, ICLR. 2017</em>, <span class="publisher-loc">Toulon, France</span>: Conference Track Proceedings, OpenReview.net.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Adi, Y., Kermany, E., Belinkov, Y., Lavi, O. and Goldberg, Y. (2017). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 5th International Conference on Learning Representations, ICLR. 2017, Toulon, France: Conference Track Proceedings, OpenReview.net.' href=https://scholar.google.com/scholar?q=Adi,+Y.,+Kermany,+E.,+Belinkov,+Y.,+Lavi,+O.+and+Goldberg,+Y.+(2017).+Fine-grained+analysis+of+sentence+embeddings+using+auxiliary+prediction+tasks,+5th+International+Conference+on+Learning+Representations,+ICLR.+2017,+Toulon,+France:+Conference+Track+Proceedings,+OpenReview.net.>Google Scholar</a></div></div></div><div id="ref3" aria-flowto="reference-3-content reference-3-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 3 in the content" id="reference-3-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-3-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Amponsah-Kaakyire</span>, <span class="given-names">K.</span></span>, <span class="string-name"><span class="surname">Pylypenko</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Genabith</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">España-Bonet</span>, <span class="given-names">C.</span></span> (<span class="year">2022</span>). Explaining translationese: why are neural classifiers better and what do they learn? In <em class="italic">Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>, <span class="publisher-loc">Abu Dhabi, United Arab Emirates (Hybrid)</span>: Association for Computational Linguistics, <span class="fpage">281</span>–<span class="lpage">296</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.' href=https://dx.doi.org/10.18653/v1/2022.blackboxnlp-1.23>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.' href=https://scholar.google.com/scholar?q=Amponsah-Kaakyire,+K.,+Pylypenko,+D.,+Genabith,+J.+and+España-Bonet,+C.+(2022).+Explaining+translationese:+why+are+neural+classifiers+better+and+what+do+they+learn?+In+Proceedings+of+the+Fifth+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Abu+Dhabi,+United+Arab+Emirates+(Hybrid):+Association+for+Computational+Linguistics,+281–296.>Google Scholar</a></div></div></div><div id="ref4" aria-flowto="reference-4-content reference-4-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 4 in the content" id="reference-4-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-4-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Arps</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Samih</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Kallmeyer</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Sajjad</span>, <span class="given-names">H.</span></span> (<span class="year">2022</span>). <span class="chapter-title">Probing for constituency structure in neural language models</span>. In Goldberg Y., Kozareva Z. and Zhang Y., (eds), <span class="source">Findings of the Association for Computational Linguistics: EMNLP.</span> Abu Dhabi, United Arab Emirates. Association for Computational Linguistics, pp. <span class="fpage">6738</span>–<span class="lpage">6757</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Findings of the Association for Computational Linguistics: EMNLP.' href=https://dx.doi.org/10.18653/v1/2022.findings-emnlp.502>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Findings of the Association for Computational Linguistics: EMNLP.' href=https://scholar.google.com/scholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP.&author=Arps+D.&author=Samih+Y.&author=Kallmeyer+L.&author=Sajjad+H.&publication+year=2022&pages=6738-6757>Google Scholar</a></div></div></div><div id="ref5" aria-flowto="reference-5-content reference-5-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 5 in the content" id="reference-5-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-5-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Balasubramanian</span>, <span class="given-names">S.</span></span>, <span class="string-name"><span class="surname">Jain</span>, <span class="given-names">N.</span></span>, <span class="string-name"><span class="surname">Jindal</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Awasthi</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Sarawagi</span>, <span class="given-names">S.</span></span> (<span class="year">2020</span>). What’s in a name? are BERT named entity representations just as good for any other name? In <em class="italic">Proceedings of the 5th Workshop on Representation Learning for NLP</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">205</span>–<span class="lpage">214</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Balasubramanian, S., Jain, N., Jindal, G., Awasthi, A. and Sarawagi, S. (2020). What’s in a name? are BERT named entity representations just as good for any other name? In Proceedings of the 5th Workshop on Representation Learning for NLP, Online. Association for Computational Linguistics, pp. 205–214.' href=https://scholar.google.com/scholar?q=Balasubramanian,+S.,+Jain,+N.,+Jindal,+G.,+Awasthi,+A.+and+Sarawagi,+S.+(2020).+What’s+in+a+name?+are+BERT+named+entity+representations+just+as+good+for+any+other+name?+In+Proceedings+of+the+5th+Workshop+on+Representation+Learning+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+205–214.>Google Scholar</a></div></div></div><div id="ref6" aria-flowto="reference-6-content reference-6-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 6 in the content" id="reference-6-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-6-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Chen</span>, <span class="given-names">Z.</span></span> and <span class="string-name"><span class="surname">Gao</span>, <span class="given-names">Q.</span></span> (<span class="year">2022</span>). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, <span class="fpage">10509</span>–<span class="lpage">10517</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.' href=https://dx.doi.org/10.1609/aaai.v36i10.21294>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.' href=https://scholar.google.com/scholar?q=Chen,+Z.+and+Gao,+Q.+(2022).+Probing+linguistic+information+for+logical+inference+in+pre-trained+language+models.+Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+36,+10509–10517.>Google Scholar</a></div></div></div><div id="ref7" aria-flowto="reference-7-content reference-7-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 7 in the content" id="reference-7-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-7-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Clark</span>, <span class="given-names">K.</span></span>, <span class="string-name"><span class="surname">Khandelwal</span>, <span class="given-names">U.</span></span>, <span class="string-name"><span class="surname">Levy</span>, <span class="given-names">O.</span></span> and <span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C. D.</span></span> (<span class="year">2019</span>). What does BERT look at? an analysis of BERT’s attention. In <em class="italic">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, <span class="publisher-loc">Florence, Italy</span>: Association for Computational Linguistics, pp. <span class="fpage">276</span>–<span class="lpage">286</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.' href=https://dx.doi.org/10.18653/v1/W19-4828>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.' href=https://scholar.google.com/scholar?q=Clark,+K.,+Khandelwal,+U.,+Levy,+O.+and+Manning,+C.+D.+(2019).+What+does+BERT+look+at?+an+analysis+of+BERT’s+attention.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+276–286.>Google Scholar</a></div></div></div><div id="ref8" aria-flowto="reference-8-content reference-8-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 8 in the content" id="reference-8-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-8-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Conneau</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Kruszewski</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Lample</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Barrault</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Baroni</span>, <span class="given-names">M.</span></span> (<span class="year">2018</span>). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In <em class="italic">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, <span class="publisher-loc">Melbourne, Australia</span>: Association for Computational Linguistics, pp. <span class="fpage">2126</span>–<span class="lpage">2136</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.' href=https://dx.doi.org/10.18653/v1/P18-1198>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.' href=https://scholar.google.com/scholar?q=Conneau,+A.,+Kruszewski,+G.,+Lample,+G.,+Barrault,+L.+and+Baroni,+M.+(2018).+What+you+can+cram+into+a+single+vector:+Probing+sentence+embeddings+for+linguistic+properties.+In+Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Melbourne,+Australia:+Association+for+Computational+Linguistics,+pp.+2126–2136.>Google Scholar</a></div></div></div><div id="ref9" aria-flowto="reference-9-content reference-9-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 9 in the content" id="reference-9-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-9-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Constant</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Eryiğit</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Monti</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">van der Plas</span>, <span class="given-names">L.</span></span>, <span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Rosner</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">Todirascu</span>, <span class="given-names">A.</span></span> (<span class="year">2017</span>). <span class="article-title">Survey: multiword expression processing: a survey</span>. <span class="source">Computational Linguistics</span> <span class="volume">43</span>, <span class="fpage">837</span>–<span class="lpage">892</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Survey: multiword expression processing: a survey' href=https://dx.doi.org/10.1162/COLI_a_00302>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Survey: multiword expression processing: a survey' href=https://scholar.google.com/scholar_lookup?title=Survey%3A+multiword+expression+processing%3A+a+survey&author=Constant+M.&author=Eryi%C4%9Fit+G.&author=Monti+J.&author=van+der+Plas+L.&author=Ramisch+C.&author=Rosner+M.&author=Todirascu+A.&publication+year=2017&journal=Computational+Linguistics&volume=43&doi=10.1162%2FCOLI_a_00302&pages=837-892>Google Scholar</a></div></div></div><div id="ref10" aria-flowto="reference-10-content reference-10-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 10 in the content" id="reference-10-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-10-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Cook</span>, <span class="given-names">P.</span></span>, <span class="string-name"><span class="surname">Fazly</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Stevenson</span>, <span class="given-names">S.</span></span> (<span class="year">2008</span>). The VNC-tokens dataset. In <em class="italic">Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008)</em>, pp. <span class="fpage">19</span>–<span class="lpage">22</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Cook, P., Fazly, A. and Stevenson, S. (2008). The VNC-tokens dataset. In Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), pp. 19–22.' href=https://scholar.google.com/scholar?q=Cook,+P.,+Fazly,+A.+and+Stevenson,+S.+(2008).+The+VNC-tokens+dataset.+In+Proceedings+of+the+LREC+Workshop+Towards+a+Shared+Task+for+Multiword+Expressions+(MWE+2008),+pp.+19–22.>Google Scholar</a></div></div></div><div id="ref11" aria-flowto="reference-11-content reference-11-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 11 in the content" id="reference-11-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-11-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Deerwester</span>, <span class="given-names">S. C.</span></span>, <span class="string-name"><span class="surname">Dumais</span>, <span class="given-names">S. T.</span></span>, <span class="string-name"><span class="surname">Landauer</span>, <span class="given-names">T. K.</span></span>, <span class="string-name"><span class="surname">Furnas</span>, <span class="given-names">G. W.</span></span> and <span class="string-name"><span class="surname">Harshman</span>, <span class="given-names">R. A.</span></span> (<span class="year">1990</span>). <span class="article-title">Indexing by latent semantic analysis</span>. <span class="source">Journal of the American Society of Information Science</span> <span class="volume">41</span>, <span class="fpage">391</span>–<span class="lpage">407</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Indexing by latent semantic analysis' href=https://dx.doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Indexing by latent semantic analysis' href=https://scholar.google.com/scholar_lookup?title=Indexing+by+latent+semantic+analysis&author=Deerwester+S.+C.&author=Dumais+S.+T.&author=Landauer+T.+K.&author=Furnas+G.+W.&author=Harshman+R.+A.&publication+year=1990&journal=Journal+of+the+American+Society+of+Information+Science&volume=41&doi=10.1002%2F(SICI)1097-4571(199009)41%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9&pages=391-407>Google Scholar</a></div></div></div><div id="ref12" aria-flowto="reference-12-content reference-12-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 12 in the content" id="reference-12-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-12-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Devlin</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Chang</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Lee</span>, <span class="given-names">K.</span></span> and <span class="string-name"><span class="surname">Toutanova</span>, <span class="given-names">K.</span></span> (<span class="year">2018</span>). BERT: pre-training of deep bidirectional transformers for language understanding. <em class="italic">NAACL-HLT</em>, pp. 4171–4186.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Devlin, J., Chang, M., Lee, K. and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. NAACL-HLT, pp. 4171–4186.' href=https://scholar.google.com/scholar?q=Devlin,+J.,+Chang,+M.,+Lee,+K.+and+Toutanova,+K.+(2018).+BERT:+pre-training+of+deep+bidirectional+transformers+for+language+understanding.+NAACL-HLT,+pp.+4171–4186.>Google Scholar</a></div></div></div><div id="ref13" aria-flowto="reference-13-content reference-13-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 13 in the content" id="reference-13-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-13-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Eisenstein</span>, <span class="given-names">J.</span></span> (<span class="year">2019</span>). <span class="source">Introduction to Natural Language Processing</span>. <span class="publisher-loc">Cambridge, MA</span>: MIT Press.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Introduction to Natural Language Processing' href=https://scholar.google.com/scholar_lookup?title=Introduction+to+Natural+Language+Processing&author=Eisenstein+J.&publication+year=2019>Google Scholar</a></div></div></div><div id="ref14" aria-flowto="reference-14-content reference-14-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 14 in the content" id="reference-14-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-14-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Ettinger</span>, <span class="given-names">A.</span></span> (<span class="year">2020</span>). <span class="article-title">What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models</span>. <span class="source">Transactions of the Association for Computational Linguistics</span> <span class="volume">8</span>, <span class="fpage">34</span>–<span class="lpage">48</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models' href=https://dx.doi.org/10.1162/tacl_a_00298>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models' href=https://scholar.google.com/scholar_lookup?title=What+BERT+is+not%3A+lessons+from+a+new+suite+of+psycholinguistic+diagnostics+for+language+models&author=Ettinger+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00298&pages=34-48>Google Scholar</a></div></div></div><div id="ref15" aria-flowto="reference-15-content reference-15-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 15 in the content" id="reference-15-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-15-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Fazly</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Cook</span>, <span class="given-names">P.</span></span> and <span class="string-name"><span class="surname">Stevenson</span>, <span class="given-names">S.</span></span> (<span class="year">2009</span>). <span class="article-title">Unsupervised type and token identification of idiomatic expressions</span>. <span class="source">Computational Linguistics</span> <span class="volume">35</span>, <span class="fpage">61</span>–<span class="lpage">103</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Unsupervised type and token identification of idiomatic expressions' href=https://dx.doi.org/10.1162/coli.08-010-R1-07-048>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Unsupervised type and token identification of idiomatic expressions' href=https://scholar.google.com/scholar_lookup?title=Unsupervised+type+and+token+identification+of+idiomatic+expressions&author=Fazly+A.&author=Cook+P.&author=Stevenson+S.&publication+year=2009&journal=Computational+Linguistics&volume=35&doi=10.1162%2Fcoli.08-010-R1-07-048&pages=61-103>Google Scholar</a></div></div></div><div id="ref16" aria-flowto="reference-16-content reference-16-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 16 in the content" id="reference-16-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-16-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Feldman</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Peng</span>, <span class="given-names">J.</span></span> (<span class="year">2013</span>). <span class="chapter-title">Automatic detection of idiomatic clauses</span>. In Gelbukh A., (ed), <span class="source">Computational Linguistics and Intelligent Text Processing</span>. <span class="publisher-loc">Berlin, Heidelberg</span>: Springer, pp. <span class="fpage">435</span>–<span class="lpage">446</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Computational Linguistics and Intelligent Text Processing' href=https://dx.doi.org/10.1007/978-3-642-37247-6_35>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Computational Linguistics and Intelligent Text Processing' href=https://scholar.google.com/scholar_lookup?title=Computational+Linguistics+and+Intelligent+Text+Processing&author=Feldman+A.&author=Peng+J.&publication+year=2013&pages=435-446>Google Scholar</a></div></div></div><div id="ref17" aria-flowto="reference-17-content reference-17-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 17 in the content" id="reference-17-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-17-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Garcia</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Kramer Vieira</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Scarton</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Idiart</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">Villavicencio</span>, <span class="given-names">A.</span></span> (<span class="year">2021</span>). Probing for idiomaticity in vector space models. In <em class="italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">3551</span>–<span class="lpage">3564</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.' href=https://dx.doi.org/10.18653/v1/2021.eacl-main.310>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.' href=https://scholar.google.com/scholar?q=Garcia,+M.,+Kramer+Vieira,+T.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2021).+Probing+for+idiomaticity+in+vector+space+models.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3551–3564.>Google Scholar</a></div></div></div><div id="ref18" aria-flowto="reference-18-content reference-18-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 18 in the content" id="reference-18-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-18-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Glavaš</span>, <span class="given-names">G.</span></span> and <span class="string-name"><span class="surname">Vulić</span>, <span class="given-names">I.</span></span> (<span class="year">2021</span>). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In <em class="italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">3090</span>–<span class="lpage">3104</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.' href=https://dx.doi.org/10.18653/v1/2021.eacl-main.270>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.' href=https://scholar.google.com/scholar?q=Glavaš,+G.+and+Vulić,+I.+(2021).+Is+supervised+syntactic+parsing+beneficial+for+language+understanding+tasks?+an+empirical+investigation.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3090–3104.>Google Scholar</a></div></div></div><div id="ref19" aria-flowto="reference-19-content reference-19-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 19 in the content" id="reference-19-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-19-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Goldberg</span>, <span class="given-names">Y.</span></span> (<span class="year">2019</span>). <span class="source">Assessing BERT’s Syntactic Abilities</span>. CoRR, abs/1901.05287.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Goldberg, Y. (2019). Assessing BERT’s Syntactic Abilities. CoRR, abs/1901.05287.' href=https://scholar.google.com/scholar?q=Goldberg,+Y.+(2019).+Assessing+BERT’s+Syntactic+Abilities.+CoRR,+abs/1901.05287.>Google Scholar</a></div></div></div><div id="ref20" aria-flowto="reference-20-content reference-20-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 20 in the content" id="reference-20-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-20-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Gupta</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Kvernadze</span>, <span class="given-names">G.</span></span> and <span class="string-name"><span class="surname">Srikumar</span>, <span class="given-names">V.</span></span> (<span class="year">2021</span>). <span class="article-title">Bert &amp; family eat word salad: experiments with text understanding</span>. <span class="source">Proceedings of the AAAI Conference on Artificial Intelligence</span> <span class="volume">35</span>, <span class="fpage">12946</span>–<span class="lpage">12954</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Bert & family eat word salad: experiments with text understanding' href=https://dx.doi.org/10.1609/aaai.v35i14.17531>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Bert & family eat word salad: experiments with text understanding' href=https://scholar.google.com/scholar_lookup?title=Bert+%26+family+eat+word+salad%3A+experiments+with+text+understanding&author=Gupta+A.&author=Kvernadze+G.&author=Srikumar+V.&publication+year=2021&journal=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence&volume=35&doi=10.1609%2Faaai.v35i14.17531&pages=12946-12954>Google Scholar</a></div></div></div><div id="ref21" aria-flowto="reference-21-content reference-21-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 21 in the content" id="reference-21-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-21-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Haagsma</span>, <span class="given-names">H.</span></span>, <span class="string-name"><span class="surname">Bos</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">Nissim</span>, <span class="given-names">M.</span></span> (<span class="year">2020</span>). MAGPIE: A large corpus of potentially idiomatic expressions. In <em class="italic">Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, <span class="publisher-loc">Marseille, France</span>: European Language Resources Association, pp. <span class="fpage">279</span>–<span class="lpage">287</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Haagsma, H., Bos, J. and Nissim, M. (2020). MAGPIE: A large corpus of potentially idiomatic expressions. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France: European Language Resources Association, pp. 279–287.' href=https://scholar.google.com/scholar?q=Haagsma,+H.,+Bos,+J.+and+Nissim,+M.+(2020).+MAGPIE:+A+large+corpus+of+potentially+idiomatic+expressions.+In+Proceedings+of+the+Twelfth+Language+Resources+and+Evaluation+Conference,+Marseille,+France:+European+Language+Resources+Association,+pp.+279–287.>Google Scholar</a></div></div></div><div id="ref22" aria-flowto="reference-22-content reference-22-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 22 in the content" id="reference-22-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-22-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Halimu</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Kasem</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Newaz</span>, <span class="given-names">S. H. S.</span></span> (<span class="year">2019</span>). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In <em class="italic">Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC</em>, <span class="publisher-loc">New York, NY, USA</span>: Association for Computing Machinery, pp. <span class="fpage">1</span>–<span class="lpage">6</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.' href=https://dx.doi.org/10.1145/3310986.3311023>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.' href=https://scholar.google.com/scholar?q=Halimu,+C.,+Kasem,+A.+and+Newaz,+S.+H.+S.+(2019).+Empirical+comparison+of+area+under+ROC+curve+(AUC)+and+mathew+correlation+coefficient+(MCC)+for+evaluating+machine+learning+algorithms+on+imbalanced+datasets+for+binary+classification.+In+Proceedings+of+the+3rd+International+Conference+on+Machine+Learning+and+Soft+Computing,+ICMLSC,+New+York,+NY,+USA:+Association+for+Computing+Machinery,+pp.+1–6.>Google Scholar</a></div></div></div><div id="ref23" aria-flowto="reference-23-content reference-23-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 23 in the content" id="reference-23-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-23-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Hashempour</span>, <span class="given-names">R.</span></span> and <span class="string-name"><span class="surname">Villavicencio</span>, <span class="given-names">A.</span></span> (<span class="year">2020</span>). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In <em class="italic">Proceedings of the Workshop on the Cognitive Aspects of the Lexicon</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">72</span>–<span class="lpage">80</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Hashempour, R. and Villavicencio, A. (2020). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In Proceedings of the Workshop on the Cognitive Aspects of the Lexicon, Online. Association for Computational Linguistics, pp. 72–80.' href=https://scholar.google.com/scholar?q=Hashempour,+R.+and+Villavicencio,+A.+(2020).+Leveraging+contextual+embeddings+and+idiom+principle+for+detecting+idiomaticity+in+potentially+idiomatic+expressions.+In+Proceedings+of+the+Workshop+on+the+Cognitive+Aspects+of+the+Lexicon,+Online.+Association+for+Computational+Linguistics,+pp.+72–80.>Google Scholar</a></div></div></div><div id="ref24" aria-flowto="reference-24-content reference-24-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 24 in the content" id="reference-24-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-24-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Hashimoto</span>, <span class="given-names">C.</span></span> and <span class="string-name"><span class="surname">Kawahara</span>, <span class="given-names">D.</span></span> (<span class="year">2008</span>). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In <em class="italic">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08</em>, <span class="publisher-loc">Stroudsburg, PA, USA</span>: Association for Computational Linguistics, pp. <span class="fpage">992</span>–<span class="lpage">1001</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.' href=https://dx.doi.org/10.3115/1613715.1613844>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.' href=https://scholar.google.com/scholar?q=Hashimoto,+C.+and+Kawahara,+D.+(2008).+Construction+of+an+idiom+corpus+and+its+application+to+idiom+identification+based+on+wsd+incorporating+idiom-specific+features.+In+Proceedings+of+the+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+EMNLP+’08,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+992–1001.>Google Scholar</a></div></div></div><div id="ref25" aria-flowto="reference-25-content reference-25-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 25 in the content" id="reference-25-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-25-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Hessel</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">Schofield</span>, <span class="given-names">A.</span></span> (<span class="year">2021</span>). How effective is BERT without word ordering? implications for language understanding and data privacy. In <em class="italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">204</span>–<span class="lpage">211</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.' href=https://dx.doi.org/10.18653/v1/2021.acl-short.27>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.' href=https://scholar.google.com/scholar?q=Hessel,+J.+and+Schofield,+A.+(2021).+How+effective+is+BERT+without+word+ordering?+implications+for+language+understanding+and+data+privacy.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+2:+Short+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+204–211.>Google Scholar</a></div></div></div><div id="ref26" aria-flowto="reference-26-content reference-26-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 26 in the content" id="reference-26-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-26-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Hewitt</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">Liang</span>, <span class="given-names">P.</span></span> (<span class="year">2019</span>). Designing and interpreting probes with control tasks. In <em class="italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019</em>, <span class="publisher-loc">Hong Kong, China</span>: Association for Computational Linguistics, pp. <span class="fpage">2733</span>–<span class="lpage">2743</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.' href=https://dx.doi.org/10.18653/v1/D19-1275>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.' href=https://scholar.google.com/scholar?q=Hewitt,+J.+and+Liang,+P.+(2019).+Designing+and+interpreting+probes+with+control+tasks.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing,+EMNLP-IJCNLP.+2019,+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+2733–2743.>Google Scholar</a></div></div></div><div id="ref27" aria-flowto="reference-27-content reference-27-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 27 in the content" id="reference-27-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-27-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Hewitt</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C. D.</span></span> (<span class="year">2019</span>). A structural probe for finding syntax in word representations. In <em class="italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, <span class="publisher-loc">Minneapolis, Minnesota</span>: Association for Computational Linguistics, vol <span class="volume">1</span>, pp. <span class="fpage">4129</span>–<span class="lpage">4138</span>, <span class="series">(Long and Short Papers)</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Minnesota: Association for Computational Linguistics, vol 1, pp. 4129–4138, (Long and Short Papers).' href=https://scholar.google.com/scholar?q=Hewitt,+J.+and+Manning,+C.+D.+(2019).+A+structural+probe+for+finding+syntax+in+word+representations.+In+Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics:+Human+Language+Technologies,+Minneapolis,+Minnesota:+Association+for+Computational+Linguistics,+vol+1,+pp.+4129–4138,+(Long+and+Short+Papers).>Google Scholar</a></div></div></div><div id="ref28" aria-flowto="reference-28-content reference-28-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 28 in the content" id="reference-28-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-28-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Jawahar</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Sagot</span>, <span class="given-names">B.</span></span> and <span class="string-name"><span class="surname">Seddah</span>, <span class="given-names">D.</span></span> (<span class="year">2019</span>a). What does BERT learn about the structure of language? In <em class="italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, <span class="publisher-loc">Florence, Italy</span>: Association for Computational Linguistics, pp. <span class="fpage">3651</span>–<span class="lpage">3657</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.' href=https://dx.doi.org/10.18653/v1/P19-1356>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.' href=https://scholar.google.com/scholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019a).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+3651–3657.>Google Scholar</a></div></div></div><div id="ref29" aria-flowto="reference-29-content reference-29-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 29 in the content" id="reference-29-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-29-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Jawahar</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Sagot</span>, <span class="given-names">B.</span></span> and <span class="string-name"><span class="surname">Seddah</span>, <span class="given-names">D.</span></span> (<span class="year">2019</span>b). What does BERT learn about the structure of language? In <em class="italic">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019</em>, <span class="publisher-loc">Florence, Italy</span>: Association for Computational Linguistics, vol <span class="volume">1</span>, pp. <span class="fpage">3651</span>–<span class="lpage">3657</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Jawahar, G., Sagot, B. and Seddah, D. (2019b). What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, Florence, Italy: Association for Computational Linguistics, vol 1, pp. 3651–3657.' href=https://scholar.google.com/scholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019b).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Conference+of+the+Association+for+Computational+Linguistics,+ACL.+2019,+Florence,+Italy:+Association+for+Computational+Linguistics,+vol+1,+pp.+3651–3657.>Google Scholar</a></div></div></div><div id="ref30" aria-flowto="reference-30-content reference-30-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 30 in the content" id="reference-30-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-30-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Kacmajor</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">Kelleher</span>, <span class="given-names">J. D.</span></span> (<span class="year">2020</span>). <span class="article-title">Capturing and measuring thematic relatedness</span>. <span class="source">Language Resources and Evaluation</span> <span class="volume">54</span>, <span class="fpage">645</span>–<span class="lpage">682</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Capturing and measuring thematic relatedness' href=https://dx.doi.org/10.1007/s10579-019-09452-w>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Capturing and measuring thematic relatedness' href=https://scholar.google.com/scholar_lookup?title=Capturing+and+measuring+thematic+relatedness&author=Kacmajor+M.&author=Kelleher+J.+D.&publication+year=2020&journal=Language+Resources+and+Evaluation&volume=54&doi=10.1007%2Fs10579-019-09452-w&pages=645-682>Google Scholar</a></div></div></div><div id="ref31" aria-flowto="reference-31-content reference-31-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 31 in the content" id="reference-31-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-31-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Kim</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Choi</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Edmiston</span>, <span class="given-names">D.</span></span> and <span class="string-name"><span class="surname">goo Lee</span>, <span class="given-names">S.</span></span> (<span class="year">2020</span>). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. <span class="source">ArXiv</span>, abs/2002.00737.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Kim, T., Choi, J., Edmiston, D. and goo Lee, S. (2020). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. ArXiv, abs/2002.00737.' href=https://scholar.google.com/scholar?q=Kim,+T.,+Choi,+J.,+Edmiston,+D.+and+goo+Lee,+S.+(2020).+Are+pre-trained+language+models+aware+of+phrases?+simple+but+strong+baselines+for+grammar+induction.+ArXiv,+abs/2002.00737.>Google Scholar</a></div></div></div><div id="ref32" aria-flowto="reference-32-content reference-32-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 32 in the content" id="reference-32-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-32-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Kiros</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Zhu</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Salakhutdinov</span>, <span class="given-names">R. R.</span></span>, <span class="string-name"><span class="surname">Zemel</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Urtasun</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Torralba</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Fidler</span>, <span class="given-names">S.</span></span> (<span class="year">2015</span>). <span class="chapter-title">Skip-thought vectors</span>. In Cortes C., Lawrence N., Lee D., Sugiyama M. and Garnett R., (eds), <span class="source">Advances in Neural Information Processing Systems</span>, <span class="volume">28</span>, Curran Associates, Inc.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Advances in Neural Information Processing Systems' href=https://scholar.google.com/scholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Kiros+R.&author=Zhu+Y.&author=Salakhutdinov+R.+R.&author=Zemel+R.&author=Urtasun+R.&author=Torralba+A.&author=Fidler+S.&publication+year=2015>Google Scholar</a></div></div></div><div id="ref33" aria-flowto="reference-33-content reference-33-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 33 in the content" id="reference-33-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-33-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Klubička</span>, <span class="given-names">F.</span></span> and <span class="string-name"><span class="surname">Kelleher</span>, <span class="given-names">J. D.</span></span> (<span class="year">2022</span>). Probing with noise: Unpicking the warp and weft of embeddings. In <em class="italic">Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP</em>, Association for Computational Linguistics.<a class='ref-link' target='_blank' aria-label='CrossRef link for Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.' href=https://dx.doi.org/10.18653/v1/2022.blackboxnlp-1.34>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.' href=https://scholar.google.com/scholar?q=Klubička,+F.+and+Kelleher,+J.+D.+(2022).+Probing+with+noise:+Unpicking+the+warp+and+weft+of+embeddings.+In+Proceedings+of+the+Fifth+BlackBoxNLP+Workshop+on+analyzing+and+interpreting+neural+networks+for+NLP,+Association+for+Computational+Linguistics.>Google Scholar</a></div></div></div><div id="ref34" aria-flowto="reference-34-content reference-34-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 34 in the content" id="reference-34-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-34-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Kovaleva</span>, <span class="given-names">O.</span></span>, <span class="string-name"><span class="surname">Romanov</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Rogers</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Rumshisky</span>, <span class="given-names">A.</span></span> (<span class="year">2019</span>). Revealing the dark secrets of BERT. In <em class="italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, <span class="publisher-loc">Hong Kong, China</span>: Association for Computational Linguistics, pp. <span class="fpage">4365</span>–<span class="lpage">4374</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.' href=https://dx.doi.org/10.18653/v1/D19-1445>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.' href=https://scholar.google.com/scholar?q=Kovaleva,+O.,+Romanov,+A.,+Rogers,+A.+and+Rumshisky,+A.+(2019).+Revealing+the+dark+secrets+of+BERT.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing+(EMNLP-IJCNLP),+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+4365–4374.>Google Scholar</a></div></div></div><div id="ref35" aria-flowto="reference-35-content reference-35-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 35 in the content" id="reference-35-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-35-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Li</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Sporleder</span>, <span class="given-names">C.</span></span> (<span class="year">2010</span>a). Linguistic cues for distinguishing literal and non-literal usages. In <em class="italic">Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10</em>, <span class="publisher-loc">Stroudsburg, PA, USA</span>: Association for Computational Linguistics, pp. <span class="fpage">683</span>–<span class="lpage">691</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Li, L. and Sporleder, C. (2010a). Linguistic cues for distinguishing literal and non-literal usages. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 683–691.' href=https://scholar.google.com/scholar?q=Li,+L.+and+Sporleder,+C.+(2010a).+Linguistic+cues+for+distinguishing+literal+and+non-literal+usages.+In+Proceedings+of+the+23rd+International+Conference+on+Computational+Linguistics:+Posters,+COLING+’10,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+683–691.>Google Scholar</a></div></div></div><div id="ref36" aria-flowto="reference-36-content reference-36-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 36 in the content" id="reference-36-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-36-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Li</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Sporleder</span>, <span class="given-names">C.</span></span> (<span class="year">2010</span>b). Using Gaussian mixture models to detect figurative language in context. In <em class="italic">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</em>, <span class="publisher-loc">Los Angeles, California</span>: Association for Computational Linguistics, pp. <span class="fpage">297</span>–<span class="lpage">300</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Li, L. and Sporleder, C. (2010b). Using Gaussian mixture models to detect figurative language in context. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California: Association for Computational Linguistics, pp. 297–300.' href=https://scholar.google.com/scholar?q=Li,+L.+and+Sporleder,+C.+(2010b).+Using+Gaussian+mixture+models+to+detect+figurative+language+in+context.+In+Human+Language+Technologies:+The+2010+Annual+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics,+Los+Angeles,+California:+Association+for+Computational+Linguistics,+pp.+297–300.>Google Scholar</a></div></div></div><div id="ref37" aria-flowto="reference-37-content reference-37-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 37 in the content" id="reference-37-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-37-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Lin</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Tan</span>, <span class="given-names">Y. C.</span></span> and <span class="string-name"><span class="surname">Frank</span>, <span class="given-names">R.</span></span> (<span class="year">2019</span>). Open sesame: Getting inside BERT’s linguistic knowledge. In <em class="italic">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, <span class="publisher-loc">Florence, Italy</span>: Association for Computational Linguistics, pp. <span class="fpage">241</span>–<span class="lpage">253</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.' href=https://dx.doi.org/10.18653/v1/W19-4825>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.' href=https://scholar.google.com/scholar?q=Lin,+Y.,+Tan,+Y.+C.+and+Frank,+R.+(2019).+Open+sesame:+Getting+inside+BERT’s+linguistic+knowledge.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+241–253.>Google Scholar</a></div></div></div><div id="ref38" aria-flowto="reference-38-content reference-38-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 38 in the content" id="reference-38-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-38-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Liu</span>, <span class="given-names">N. F.</span></span>, <span class="string-name"><span class="surname">Gardner</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Belinkov</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Peters</span>, <span class="given-names">M. E.</span></span> and <span class="string-name"><span class="surname">Smith</span>, <span class="given-names">N. A.</span></span> (<span class="year">2019</span>a). Linguistic knowledge and transferability of contextual representations. <span class="source">CoRR</span>, abs/1903.08855.<a class='ref-link' target='_blank' aria-label='CrossRef link for Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855.' href=https://dx.doi.org/10.18653/v1/N19-1112>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs/1903.08855.' href=https://scholar.google.com/scholar?q=Liu,+N.+F.,+Gardner,+M.,+Belinkov,+Y.,+Peters,+M.+E.+and+Smith,+N.+A.+(2019a).+Linguistic+knowledge+and+transferability+of+contextual+representations.+CoRR,+abs/1903.08855.>Google Scholar</a></div></div></div><div id="ref39" aria-flowto="reference-39-content reference-39-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 39 in the content" id="reference-39-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-39-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Liu</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Ott</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Goyal</span>, <span class="given-names">N.</span></span>, <span class="string-name"><span class="surname">Du</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Joshi</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Chen</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Levy</span>, <span class="given-names">O.</span></span>, <span class="string-name"><span class="surname">Lewis</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Zettlemoyer</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Stoyanov</span>, <span class="given-names">V.</span></span> (<span class="year">2019</span>b). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. (2019b). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.' href=https://scholar.google.com/scholar?q=Liu,+Y.,+Ott,+M.,+Goyal,+N.,+Du,+J.,+Joshi,+M.,+Chen,+D.,+Levy,+O.,+Lewis,+M.,+Zettlemoyer,+L.+and+Stoyanov,+V.+(2019b).+RoBERTa:+ARobustly+Optimized+BERT+Pretraining+Approach,+arXiv+preprint+arXiv:1907.11692.>Google Scholar</a></div></div></div><div id="ref40" aria-flowto="reference-40-content reference-40-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 40 in the content" id="reference-40-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-40-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C.</span></span> and <span class="string-name"><span class="surname">Schutze</span>, <span class="given-names">H.</span></span> (<span class="year">1999</span>). <span class="source">Foundations of Statistical Natural Language Processing</span>. <span class="publisher-loc">Cambridge, MA</span>: MIT Press.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Foundations of Statistical Natural Language Processing' href=https://scholar.google.com/scholar_lookup?title=Foundations+of+Statistical+Natural+Language+Processing&author=Manning+C.&author=Schutze+H.&publication+year=1999>Google Scholar</a></div></div></div><div id="ref41" aria-flowto="reference-41-content reference-41-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 41 in the content" id="reference-41-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-41-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C. D.</span></span>, <span class="string-name"><span class="surname">Clark</span>, <span class="given-names">K.</span></span>, <span class="string-name"><span class="surname">Hewitt</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Khandelwal</span>, <span class="given-names">U.</span></span> and <span class="string-name"><span class="surname">Levy</span>, <span class="given-names">O.</span></span> (<span class="year">2020</span>). <span class="article-title">Emergent linguistic structure in artificial neural networks trained by self-supervision</span>. <span class="source">Proceedings of the National Academy of Sciences of the United States of America</span> <span class="volume">117</span>, <span class="fpage">30046</span>–<span class="lpage">30054</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https://dx.doi.org/10.1073/pnas.1907367117>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https://scholar.google.com/scholar_lookup?title=Emergent+linguistic+structure+in+artificial+neural+networks+trained+by+self-supervision&author=Manning+C.+D.&author=Clark+K.&author=Hewitt+J.&author=Khandelwal+U.&author=Levy+O.&publication+year=2020&journal=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&volume=117&doi=10.1073%2Fpnas.1907367117&pages=30046-30054>Google Scholar</a><a class='ref-link' target='_blank' aria-label='PubMed link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https://www.ncbi.nlm.nih.gov/pubmed/32493748>PubMed</a></div></div></div><div id="ref42" aria-flowto="reference-42-content reference-42-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 42 in the content" id="reference-42-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-42-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Mickus</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Paperno</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Constant</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">van Deemter</span>, <span class="given-names">K.</span></span> (<span class="year">2020</span>). What do you mean, BERT? In <em class="italic">Proceedings of the Society for Computation in Linguistics.</em>, <span class="publisher-loc">New York</span>: Association for Computational Linguistics, pp. <span class="fpage">279</span>–<span class="lpage">290</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Mickus, T., Paperno, D., Constant, M. and van Deemter, K. (2020). What do you mean, BERT? In Proceedings of the Society for Computation in Linguistics., New York: Association for Computational Linguistics, pp. 279–290.' href=https://scholar.google.com/scholar?q=Mickus,+T.,+Paperno,+D.,+Constant,+M.+and+van+Deemter,+K.+(2020).+What+do+you+mean,+BERT?+In+Proceedings+of+the+Society+for+Computation+in+Linguistics.,+New+York:+Association+for+Computational+Linguistics,+pp.+279–290.>Google Scholar</a></div></div></div><div id="ref43" aria-flowto="reference-43-content reference-43-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 43 in the content" id="reference-43-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-43-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Mosbach</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Khokhlova</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Hedderich</span>, <span class="given-names">M. A.</span></span> and <span class="string-name"><span class="surname">Klakow</span>, <span class="given-names">D.</span></span> (<span class="year">2020</span>). 
<em class="italic">On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers</em>
. In <em class="italic">Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">68</span>–<span class="lpage">82</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.' href=https://dx.doi.org/10.18653/v1/2020.blackboxnlp-1.7>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.' href=https://scholar.google.com/scholar?q=Mosbach,+M.,+Khokhlova,+A.,+Hedderich,+M.+A.+and+Klakow,+D.+(2020).+On+the+interplay+between+fine-tuning+and+sentence-level+probing+for+linguistic+knowledge+in+pre-trained+transformers+.+In+Proceedings+of+the+Third+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+68–82.>Google Scholar</a></div></div></div><div id="ref44" aria-flowto="reference-44-content reference-44-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 44 in the content" id="reference-44-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-44-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Nedumpozhimana</span>, <span class="given-names">V.</span></span> and <span class="string-name"><span class="surname">Kelleher</span>, <span class="given-names">J.</span></span> (<span class="year">2021</span>). Finding BERT’s idiomatic key. In <em class="italic">Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)</em>, pp. <span class="fpage">57</span>–<span class="lpage">62</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.' href=https://dx.doi.org/10.18653/v1/2021.mwe-1.7>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.' href=https://scholar.google.com/scholar?q=Nedumpozhimana,+V.+and+Kelleher,+J.+(2021).+Finding+BERT’s+idiomatic+key.+In+Proceedings+of+the+17th+Workshop+on+Multiword+Expressions+(MWE+2021),+pp.+57–62.>Google Scholar</a></div></div></div><div id="ref45" aria-flowto="reference-45-content reference-45-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 45 in the content" id="reference-45-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-45-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Nedumpozhimana</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Klubička</span>, <span class="given-names">F.</span></span> and <span class="string-name"><span class="surname">Kelleher</span>, <span class="given-names">J. D.</span></span> (<span class="year">2022</span>). <span class="article-title">Shapley idioms: analysing BERT sentence embeddings for general idiom token identification</span>. <span class="source">Frontiers in Artificial Intelligence</span> <span class="volume">5</span>, 813967.<a class='ref-link' target='_blank' aria-label='CrossRef link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https://dx.doi.org/10.3389/frai.2022.813967>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https://scholar.google.com/scholar_lookup?title=Shapley+idioms%3A+analysing+BERT+sentence+embeddings+for+general+idiom+token+identification&author=Nedumpozhimana+V.&author=Klubi%C4%8Dka+F.&author=Kelleher+J.+D.&publication+year=2022&journal=Frontiers+in+Artificial+Intelligence&volume=5&doi=10.3389%2Ffrai.2022.813967>Google Scholar</a><a class='ref-link' target='_blank' aria-label='PubMed link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https://www.ncbi.nlm.nih.gov/pubmed/35360661>PubMed</a></div></div></div><div id="ref46" aria-flowto="reference-46-content reference-46-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 46 in the content" id="reference-46-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-46-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Niu</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Lu</span>, <span class="given-names">W.</span></span> and <span class="string-name"><span class="surname">Penn</span>, <span class="given-names">G.</span></span> (<span class="year">2022</span>). Does BERT rediscover a classical NLP pipeline? In <em class="italic">Proceedings of the 29th International Conference on Computational Linguistics</em>, <span class="publisher-loc">Gyeongju, Republic of Korea</span>: International Committee on Computational Linguistics, <span class="fpage">3143</span>–<span class="lpage">3153</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Niu, J., Lu, W. and Penn, G. (2022). Does BERT rediscover a classical NLP pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea: International Committee on Computational Linguistics, 3143–3153.' href=https://scholar.google.com/scholar?q=Niu,+J.,+Lu,+W.+and+Penn,+G.+(2022).+Does+BERT+rediscover+a+classical+NLP+pipeline?+In+Proceedings+of+the+29th+International+Conference+on+Computational+Linguistics,+Gyeongju,+Republic+of+Korea:+International+Committee+on+Computational+Linguistics,+3143–3153.>Google Scholar</a></div></div></div><div id="ref47" aria-flowto="reference-47-content reference-47-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 47 in the content" id="reference-47-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-47-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Pedregosa</span>, <span class="given-names">F.</span></span>, <span class="string-name"><span class="surname">Varoquaux</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Gramfort</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Michel</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Thirion</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Grisel</span>, <span class="given-names">O.</span></span>, <span class="string-name"><span class="surname">Blondel</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Prettenhofer</span>, <span class="given-names">P.</span></span>, <span class="string-name"><span class="surname">Weiss</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Dubourg</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Vanderplas</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Passos</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Cournapeau</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Brucher</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Perrot</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">Duchesnay</span>, <span class="given-names">E.</span></span> (<span class="year">2011</span>). <span class="article-title">Scikit-learn: machine learning in python</span>. <span class="source">Journal of Machine Learning Research</span> <span class="volume">12</span>, <span class="fpage">2825</span>–<span class="lpage">2830</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Scikit-learn: machine learning in python' href=https://scholar.google.com/scholar_lookup?title=Scikit-learn%3A+machine+learning+in+python&author=Pedregosa+F.&author=Varoquaux+G.&author=Gramfort+A.&author=Michel+V.&author=Thirion+B.&author=Grisel+O.&author=Blondel+M.&author=Prettenhofer+P.&author=Weiss+R.&author=Dubourg+V.&author=Vanderplas+J.&author=Passos+A.&author=Cournapeau+D.&author=Brucher+M.&author=Perrot+M.&author=Duchesnay+E.&publication+year=2011&journal=Journal+of+Machine+Learning+Research&volume=12&pages=2825-2830>Google Scholar</a></div></div></div><div id="ref48" aria-flowto="reference-48-content reference-48-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 48 in the content" id="reference-48-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-48-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Peng</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Feldman</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Vylomova</span>, <span class="given-names">E.</span></span> (<span class="year">2014</span>). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In <em class="italic">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, <span class="publisher-loc">Doha, Qatar</span>: Association for Computational Linguistics, pp. <span class="fpage">2019</span>–<span class="lpage">2027</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.' href=https://dx.doi.org/10.3115/v1/D14-1216>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.' href=https://scholar.google.com/scholar?q=Peng,+J.,+Feldman,+A.+and+Vylomova,+E.+(2014).+Classifying+idiomatic+and+literal+expressions+using+topic+models+and+intensity+of+emotions.+In+Proceedings+of+the+2014+Conference+on+Empirical+Methods+in+Natural+Language+Processing+(EMNLP),+Doha,+Qatar:+Association+for+Computational+Linguistics,+pp.+2019–2027.>Google Scholar</a></div></div></div><div id="ref49" aria-flowto="reference-49-content reference-49-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 49 in the content" id="reference-49-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-49-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Pennington</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Socher</span>, <span class="given-names">R.</span></span> and <span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C.</span></span> (<span class="year">2014</span>). Glove: Global vectors for word representation. In <em class="italic">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, pp. <span class="fpage">1532</span>–<span class="lpage">1543</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.' href=https://dx.doi.org/10.3115/v1/D14-1162>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.' href=https://scholar.google.com/scholar?q=Pennington,+J.,+Socher,+R.+and+Manning,+C.+(2014).+Glove:+Global+vectors+for+word+representation.+In+Proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(EMNLP),+pp.+1532–1543.>Google Scholar</a></div></div></div><div id="ref50" aria-flowto="reference-50-content reference-50-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 50 in the content" id="reference-50-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-50-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Pham</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Bui</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Mai</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Nguyen</span>, <span class="given-names">A.</span></span> (<span class="year">2021</span>). <span class="chapter-title">Out of order: how important is the sequential order of words in a sentence in natural language understanding tasks?</span>. In <span class="source">Findings of the Association for Computational Linguistics</span>. Association for Computational Linguistics, pp. <span class="fpage">1145</span>–<span class="lpage">1160</span>,<a class='ref-link' target='_blank' aria-label='Google Scholar link for Findings of the Association for Computational Linguistics' href=https://scholar.google.com/scholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics&author=Pham+T.&author=Bui+T.&author=Mai+L.&author=Nguyen+A.&publication+year=2021&pages=1145-1160>Google Scholar</a></div></div></div><div id="ref51" aria-flowto="reference-51-content reference-51-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 51 in the content" id="reference-51-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-51-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Pimentel</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Valvoda</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Stoehr</span>, <span class="given-names">N.</span></span> and <span class="string-name"><span class="surname">Cotterell</span>, <span class="given-names">R.</span></span> (<span class="year">2022</span>). The architectural bottleneck principle.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Pimentel, T., Valvoda, J., Stoehr, N. and Cotterell, R. (2022). The architectural bottleneck principle.' href=https://scholar.google.com/scholar?q=Pimentel,+T.,+Valvoda,+J.,+Stoehr,+N.+and+Cotterell,+R.+(2022).+The+architectural+bottleneck+principle.>Google Scholar</a></div></div></div><div id="ref52" aria-flowto="reference-52-content reference-52-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 52 in the content" id="reference-52-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-52-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Raganato</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Tiedemann</span>, <span class="given-names">J.</span></span> (<span class="year">2018</span>). An analysis of encoder representations in transformer-based machine translation. In <em class="italic">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, <span class="publisher-loc">Brussels, Belgium</span>: Association for Computational Linguistics, pp. <span class="fpage">287</span>–<span class="lpage">297</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.' href=https://dx.doi.org/10.18653/v1/W18-5431>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.' href=https://scholar.google.com/scholar?q=Raganato,+A.+and+Tiedemann,+J.+(2018).+An+analysis+of+encoder+representations+in+transformer-based+machine+translation.+In+Proceedings+of+the+2018+EMNLP+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Brussels,+Belgium:+Association+for+Computational+Linguistics,+pp.+287–297.>Google Scholar</a></div></div></div><div id="ref53" aria-flowto="reference-53-content reference-53-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 53 in the content" id="reference-53-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-53-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Cordeiro</span>, <span class="given-names">S. R.</span></span>, <span class="string-name"><span class="surname">Savary</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Vincze</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Barbu Mititelu</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Bhatia</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Buljan</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Candito</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Gantar</span>, <span class="given-names">P.</span></span>, <span class="string-name"><span class="surname">Giouli</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Güngör</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Hawwari</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Iñurrieta</span>, <span class="given-names">U.</span></span>, <span class="string-name"><span class="surname">Kovalevskaitė</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Krek</span>, <span class="given-names">S.</span></span>, <span class="string-name"><span class="surname">Lichte</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Liebeskind</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Monti</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Parra Escartín</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">QasemiZadeh</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Schneider</span>, <span class="given-names">N.</span></span>, <span class="string-name"><span class="surname">Stoyanova</span>, <span class="given-names">I.</span></span>, <span class="string-name"><span class="surname">Vaidya</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Walsh</span>, <span class="given-names">A.</span></span> (<span class="year">2018</span>). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In <em class="italic">Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</em>, <span class="publisher-loc">Santa Fe, New Mexico, USA</span>: Association for Computational Linguistics, pp. <span class="fpage">222</span>–<span class="lpage">240</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Ramisch, C., Cordeiro, S. R., Savary, A., Vincze, V., Barbu Mititelu, V., Bhatia, A., Buljan, M., Candito, M., Gantar, P., Giouli, V., Güngör, T., Hawwari, A., Iñurrieta, U., Kovalevskaitė, J., Krek, S., Lichte, T., Liebeskind, C., Monti, J., Parra Escartín, C., QasemiZadeh, B., Ramisch, R., Schneider, N., Stoyanova, I., Vaidya, A. and Walsh, A. (2018). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Santa Fe, New Mexico, USA: Association for Computational Linguistics, pp. 222–240.' href=https://scholar.google.com/scholar?q=Ramisch,+C.,+Cordeiro,+S.+R.,+Savary,+A.,+Vincze,+V.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Buljan,+M.,+Candito,+M.,+Gantar,+P.,+Giouli,+V.,+Güngör,+T.,+Hawwari,+A.,+Iñurrieta,+U.,+Kovalevskaitė,+J.,+Krek,+S.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Parra+Escartín,+C.,+QasemiZadeh,+B.,+Ramisch,+R.,+Schneider,+N.,+Stoyanova,+I.,+Vaidya,+A.+and+Walsh,+A.+(2018).+Edition+1.1+of+the+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Linguistic+Annotation,+Multiword+Expressions+and+Constructions+(LAW-MWE-CxG-2018),+Santa+Fe,+New+Mexico,+USA:+Association+for+Computational+Linguistics,+pp.+222–240.>Google Scholar</a></div></div></div><div id="ref54" aria-flowto="reference-54-content reference-54-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 54 in the content" id="reference-54-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-54-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Savary</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Guillaume</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Waszczuk</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Candito</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Vaidya</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Barbu Mititelu</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Bhatia</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Iñurrieta</span>, <span class="given-names">U.</span></span>, <span class="string-name"><span class="surname">Giouli</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Güngör</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Jiang</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Lichte</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Liebeskind</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Monti</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Stymne</span>, <span class="given-names">S.</span></span>, <span class="string-name"><span class="surname">Walsh</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Xu</span>, <span class="given-names">H.</span></span> (<span class="year">2020</span>). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In <em class="italic">Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons</em>, Association for Computational Linguistics, pp. <span class="fpage">107</span>–<span class="lpage">118</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Ramisch, C., Savary, A., Guillaume, B., Waszczuk, J., Candito, M., Vaidya, A., Barbu Mititelu, V., Bhatia, A., Iñurrieta, U., Giouli, V., Güngör, T., Jiang, M., Lichte, T., Liebeskind, C., Monti, J., Ramisch, R., Stymne, S., Walsh, A. and Xu, H. (2020). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons, Association for Computational Linguistics, pp. 107–118.' href=https://scholar.google.com/scholar?q=Ramisch,+C.,+Savary,+A.,+Guillaume,+B.,+Waszczuk,+J.,+Candito,+M.,+Vaidya,+A.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Iñurrieta,+U.,+Giouli,+V.,+Güngör,+T.,+Jiang,+M.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Ramisch,+R.,+Stymne,+S.,+Walsh,+A.+and+Xu,+H.+(2020).+Edition+1.2+of+the+PARSEME+shared+task+on+semi-supervised+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Multiword+Expressions+and+Electronic+Lexicons,+Association+for+Computational+Linguistics,+pp.+107–118.>Google Scholar</a></div></div></div><div id="ref55" aria-flowto="reference-55-content reference-55-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 55 in the content" id="reference-55-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-55-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Reif</span>, <span class="given-names">E.</span></span>, <span class="string-name"><span class="surname">Yuan</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Wattenberg</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Viegas</span>, <span class="given-names">F. B.</span></span>, <span class="string-name"><span class="surname">Coenen</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Pearce</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Kim</span>, <span class="given-names">B.</span></span> (<span class="year">2019</span>). <span class="chapter-title">Visualizing and measuring the geometry of bert</span>. In Wallach H., Larochelle H., Beygelzimer A., d’Alché-Buc F., Fox E. and Garnett R., (eds), <span class="source">Advances in Neural Information Processing Systems</span>, <span class="volume">32</span>, Curran Associates, Inc.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Advances in Neural Information Processing Systems' href=https://scholar.google.com/scholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Reif+E.&author=Yuan+A.&author=Wattenberg+M.&author=Viegas+F.+B.&author=Coenen+A.&author=Pearce+A.&author=Kim+B.&publication+year=2019>Google Scholar</a></div></div></div><div id="ref56" aria-flowto="reference-56-content reference-56-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 56 in the content" id="reference-56-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-56-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Rogers</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Kovaleva</span>, <span class="given-names">O.</span></span> and <span class="string-name"><span class="surname">Rumshisky</span>, <span class="given-names">A.</span></span> (<span class="year">2020</span>). <span class="article-title">A primer in BERTology: what we know about how BERT works</span>. <span class="source">Transactions of the Association for Computational Linguistics</span> <span class="volume">8</span>, <span class="fpage">842</span>–<span class="lpage">866</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for A primer in BERTology: what we know about how BERT works' href=https://dx.doi.org/10.1162/tacl_a_00349>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for A primer in BERTology: what we know about how BERT works' href=https://scholar.google.com/scholar_lookup?title=A+primer+in+BERTology%3A+what+we+know+about+how+BERT+works&author=Rogers+A.&author=Kovaleva+O.&author=Rumshisky+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00349&pages=842-866>Google Scholar</a></div></div></div><div id="ref57" aria-flowto="reference-57-content reference-57-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 57 in the content" id="reference-57-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-57-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Rosa</span>, <span class="given-names">R.</span></span> and <span class="string-name"><span class="surname">Mareček</span>, <span class="given-names">D.</span></span> (<span class="year">2019</span>). Inducing syntactic trees from BERT representations, arXiv, abs/1906.11511.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Rosa, R. and Mareček, D. (2019). Inducing syntactic trees from BERT representations, arXiv, abs/1906.11511.' href=https://scholar.google.com/scholar?q=Rosa,+R.+and+Mareček,+D.+(2019).+Inducing+syntactic+trees+from+BERT+representations,+arXiv,+abs/1906.11511.>Google Scholar</a></div></div></div><div id="ref58" aria-flowto="reference-58-content reference-58-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 58 in the content" id="reference-58-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-58-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Sag</span>, <span class="given-names">I. A.</span></span>, <span class="string-name"><span class="surname">Baldwin</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Bond</span>, <span class="given-names">F.</span></span>, <span class="string-name"><span class="surname">Copestake</span>, <span class="given-names">A. A.</span></span> and <span class="string-name"><span class="surname">Flickinger</span>, <span class="given-names">D.</span></span> (<span class="year">2002</span>). Multiword expressions: A pain in the neck for nlp. In <em class="italic">Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing</em>, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. <span class="fpage">1</span>–<span class="lpage">15</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.' href=https://dx.doi.org/10.1007/3-540-45715-1_1>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.' href=https://scholar.google.com/scholar?q=Sag,+I.+A.,+Baldwin,+T.,+Bond,+F.,+Copestake,+A.+A.+and+Flickinger,+D.+(2002).+Multiword+expressions:+A+pain+in+the+neck+for+nlp.+In+Proceedings+of+the+Third+International+Conference+on+Computational+Linguistics+and+Intelligent+Text+Processing,+Berlin,+Heidelberg:+Springer-Verlag,+vol+CICLing+’02,+pp.+1–15.>Google Scholar</a></div></div></div><div id="ref59" aria-flowto="reference-59-content reference-59-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 59 in the content" id="reference-59-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-59-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Salton</span>, <span class="given-names">G.</span></span>, <span class="string-name"><span class="surname">Ross</span>, <span class="given-names">R.</span></span> and <span class="string-name"><span class="surname">Kelleher</span>, <span class="given-names">J.</span></span> (<span class="year">2016</span>). Idiom token classification using sentential distributed semantics. In <em class="italic">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, <span class="publisher-loc">Berlin, Germany</span>: Association for Computational Linguistics, pp. <span class="fpage">194</span>–<span class="lpage">204</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.' href=https://dx.doi.org/10.18653/v1/P16-1019>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.' href=https://scholar.google.com/scholar?q=Salton,+G.,+Ross,+R.+and+Kelleher,+J.+(2016).+Idiom+token+classification+using+sentential+distributed+semantics.+In+Proceedings+of+the+54th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Berlin,+Germany:+Association+for+Computational+Linguistics,+pp.+194–204.>Google Scholar</a></div></div></div><div id="ref60" aria-flowto="reference-60-content reference-60-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 60 in the content" id="reference-60-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-60-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Savary</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Cordeiro</span>, <span class="given-names">S. R.</span></span>, <span class="string-name"><span class="surname">Lichte</span>, <span class="given-names">T.</span></span>, <span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">nurrieta</span>, <span class="given-names">U. I.</span></span> and <span class="string-name"><span class="surname">Giouli</span>, <span class="given-names">V.</span></span> (<span class="year">2019</span>). <span class="article-title">Literal occurrences of multiword expressions: rare birds that cause a stir</span>. <span class="source">The Prague Bulletin of Mathematical Linguistics</span> <span class="volume">112</span>, <span class="fpage">5</span>–<span class="lpage">54</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Literal occurrences of multiword expressions: rare birds that cause a stir' href=https://dx.doi.org/10.2478/pralin-2019-0001>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Literal occurrences of multiword expressions: rare birds that cause a stir' href=https://scholar.google.com/scholar_lookup?title=Literal+occurrences+of+multiword+expressions%3A+rare+birds+that+cause+a+stir&author=Savary+A.&author=Cordeiro+S.+R.&author=Lichte+T.&author=Ramisch+C.&author=nurrieta+U.+I.&author=Giouli+V.&publication+year=2019&journal=The+Prague+Bulletin+of+Mathematical+Linguistics&volume=112&doi=10.2478%2Fpralin-2019-0001&pages=5-54>Google Scholar</a></div></div></div><div id="ref61" aria-flowto="reference-61-content reference-61-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 61 in the content" id="reference-61-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-61-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Savary</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Ramisch</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Cordeiro</span>, <span class="given-names">S.</span></span>, <span class="string-name"><span class="surname">Sangati</span>, <span class="given-names">F.</span></span>, <span class="string-name"><span class="surname">Vincze</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">QasemiZadeh</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Candito</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Cap</span>, <span class="given-names">F.</span></span>, <span class="string-name"><span class="surname">Giouli</span>, <span class="given-names">V.</span></span>, <span class="string-name"><span class="surname">Stoyanova</span>, <span class="given-names">I.</span></span> and <span class="string-name"><span class="surname">Doucet</span>, <span class="given-names">A.</span></span> (<span class="year">2017</span>). The PARSEME shared task on automatic identification of verbal multiword expressions. In <em class="italic">Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</em>, <span class="publisher-loc">Valencia, Spain</span>: Association for Computational Linguistics, pp. <span class="fpage">31</span>–<span class="lpage">47</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.' href=https://dx.doi.org/10.18653/v1/W17-1704>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.' href=https://scholar.google.com/scholar?q=Savary,+A.,+Ramisch,+C.,+Cordeiro,+S.,+Sangati,+F.,+Vincze,+V.,+QasemiZadeh,+B.,+Candito,+M.,+Cap,+F.,+Giouli,+V.,+Stoyanova,+I.+and+Doucet,+A.+(2017).+The+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+13th+Workshop+on+Multiword+Expressions+(MWE+2017),+Valencia,+Spain:+Association+for+Computational+Linguistics,+pp.+31–47.>Google Scholar</a></div></div></div><div id="ref62" aria-flowto="reference-62-content reference-62-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 62 in the content" id="reference-62-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-62-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Schneider</span>, <span class="given-names">N.</span></span>, <span class="string-name"><span class="surname">Hovy</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Johannsen</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Carpuat</span>, <span class="given-names">M.</span></span> (<span class="year">2016</span>). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In <em class="italic">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</em>, <span class="publisher-loc">San Diego, California</span>: Association for Computational Linguistics, pp. <span class="fpage">546</span>–<span class="lpage">559</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Schneider, N., Hovy, D., Johannsen, A. and Carpuat, M. (2016). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California: Association for Computational Linguistics, pp. 546–559.' href=https://scholar.google.com/scholar?q=Schneider,+N.,+Hovy,+D.,+Johannsen,+A.+and+Carpuat,+M.+(2016).+SemEval-.+2016+task+10:+Detecting+minimal+semantic+units+and+their+meanings+(DiMSUM).+In+Proceedings+of+the+10th+International+Workshop+on+Semantic+Evaluation+(SemEval-2016),+San+Diego,+California:+Association+for+Computational+Linguistics,+pp.+546–559.>Google Scholar</a></div></div></div><div id="ref63" aria-flowto="reference-63-content reference-63-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 63 in the content" id="reference-63-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-63-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Sinha</span>, <span class="given-names">K.</span></span>, <span class="string-name"><span class="surname">Jia</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Hupkes</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Pineau</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Williams</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Kiela</span>, <span class="given-names">D.</span></span> (<span class="year">2021</span>a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In <em class="italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, <span class="publisher-loc">Online and Punta Cana, Dominican Republic</span>: Association for Computational Linguistics, pp. <span class="fpage">2888</span>–<span class="lpage">2913</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.' href=https://dx.doi.org/10.18653/v1/2021.emnlp-main.230>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.' href=https://scholar.google.com/scholar?q=Sinha,+K.,+Jia,+R.,+Hupkes,+D.,+Pineau,+J.,+Williams,+A.+and+Kiela,+D.+(2021a).+Masked+language+modeling+and+the+distributional+hypothesis:+Order+word+matters+pre-training+for+little.+In+Proceedings+of+the+2021+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Online+and+Punta+Cana,+Dominican+Republic:+Association+for+Computational+Linguistics,+pp.+2888–2913.>Google Scholar</a></div></div></div><div id="ref64" aria-flowto="reference-64-content reference-64-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 64 in the content" id="reference-64-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-64-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Sinha</span>, <span class="given-names">K.</span></span>, <span class="string-name"><span class="surname">Parthasarathi</span>, <span class="given-names">P.</span></span>, <span class="string-name"><span class="surname">Pineau</span>, <span class="given-names">J.</span></span> and <span class="string-name"><span class="surname">Williams</span>, <span class="given-names">A.</span></span> (<span class="year">2021</span>b). UnNatural Language Inference. In <em class="italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">7329</span>–<span class="lpage">7346</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.' href=https://dx.doi.org/10.18653/v1/2021.acl-long.569>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.' href=https://scholar.google.com/scholar?q=Sinha,+K.,+Parthasarathi,+P.,+Pineau,+J.+and+Williams,+A.+(2021b).+UnNatural+Language+Inference.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+1:+Long+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+7329–7346.>Google Scholar</a></div></div></div><div id="ref65" aria-flowto="reference-65-content reference-65-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 65 in the content" id="reference-65-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-65-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Socher</span>, <span class="given-names">R.</span></span>, <span class="string-name"><span class="surname">Perelygin</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Wu</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Chuang</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Manning</span>, <span class="given-names">C. D.</span></span>, <span class="string-name"><span class="surname">Ng</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">Potts</span>, <span class="given-names">C.</span></span> (<span class="year">2013</span>). Recursive deep models for semantic compositionality over a sentiment treebank. In <em class="italic">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</em>, <span class="publisher-loc">Seattle, Washington, USA</span>: Association for Computational Linguistics, pp. <span class="fpage">1631</span>–<span class="lpage">1642</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA: Association for Computational Linguistics, pp. 1631–1642.' href=https://scholar.google.com/scholar?q=Socher,+R.,+Perelygin,+A.,+Wu,+J.,+Chuang,+J.,+Manning,+C.+D.,+Ng,+A.+and+Potts,+C.+(2013).+Recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+In+Proceedings+of+the+2013+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Seattle,+Washington,+USA:+Association+for+Computational+Linguistics,+pp.+1631–1642.>Google Scholar</a></div></div></div><div id="ref66" aria-flowto="reference-66-content reference-66-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 66 in the content" id="reference-66-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-66-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Sporleder</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Li</span>, <span class="given-names">L.</span></span>, <span class="string-name"><span class="surname">Gorinski</span>, <span class="given-names">P.</span></span> and <span class="string-name"><span class="surname">Koch</span>, <span class="given-names">X.</span></span> (<span class="year">2010</span>). <span class="chapter-title">Idioms in context: the idix corpus</span>. In <span class="source">LREC</span>
<a class='ref-link' target='_blank' aria-label='Google Scholar link for LREC' href=https://scholar.google.com/scholar_lookup?title=LREC&author=Sporleder+C.&author=Li+L.&author=Gorinski+P.&author=Koch+X.&publication+year=2010>Google Scholar</a></div></div></div><div id="ref67" aria-flowto="reference-67-content reference-67-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 67 in the content" id="reference-67-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-67-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Tayyar Madabushi</span>, <span class="given-names">H.</span></span>, <span class="string-name"><span class="surname">Gow-Smith</span>, <span class="given-names">E.</span></span>, <span class="string-name"><span class="surname">Garcia</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Scarton</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Idiart</span>, <span class="given-names">M.</span></span> and <span class="string-name"><span class="surname">Villavicencio</span>, <span class="given-names">A.</span></span> (<span class="year">2022</span>). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In <em class="italic">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</em>, Seattle, United States. Association for Computational Linguistics, pp. <span class="fpage">107</span>–<span class="lpage">121</span>.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Tayyar Madabushi, H., Gow-Smith, E., Garcia, M., Scarton, C., Idiart, M. and Villavicencio, A. (2022). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Seattle, United States. Association for Computational Linguistics, pp. 107–121.' href=https://scholar.google.com/scholar?q=Tayyar+Madabushi,+H.,+Gow-Smith,+E.,+Garcia,+M.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2022).+SemEval-.+2022+task+2:+Multilingual+idiomaticity+detection+and+sentence+embedding.+In+Proceedings+of+the+16th+International+Workshop+on+Semantic+Evaluation+(SemEval-2022),+Seattle,+United+States.+Association+for+Computational+Linguistics,+pp.+107–121.>Google Scholar</a></div></div></div><div id="ref68" aria-flowto="reference-68-content reference-68-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 68 in the content" id="reference-68-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-68-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Tenney</span>, <span class="given-names">I.</span></span>, <span class="string-name"><span class="surname">Das</span>, <span class="given-names">D.</span></span> and <span class="string-name"><span class="surname">Pavlick</span>, <span class="given-names">E.</span></span> (<span class="year">2019</span>a). BERT rediscovers the classical NLP pipeline. In <em class="italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, <span class="publisher-loc">Florence, Italy</span>: Association for Computational Linguistics, pp. <span class="fpage">4593</span>–<span class="lpage">4601</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.' href=https://dx.doi.org/10.18653/v1/P19-1452>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.' href=https://scholar.google.com/scholar?q=Tenney,+I.,+Das,+D.+and+Pavlick,+E.+(2019a).+BERT+rediscovers+the+classical+NLP+pipeline.+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+4593–4601.>Google Scholar</a></div></div></div><div id="ref69" aria-flowto="reference-69-content reference-69-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 69 in the content" id="reference-69-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-69-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Tenney</span>, <span class="given-names">I.</span></span>, <span class="string-name"><span class="surname">Xia</span>, <span class="given-names">P.</span></span>, <span class="string-name"><span class="surname">Chen</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Wang</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">Poliak</span>, <span class="given-names">A.</span></span>, <span class="string-name"><span class="surname">McCoy</span>, <span class="given-names">R. T.</span></span>, <span class="string-name"><span class="surname">Kim</span>, <span class="given-names">N.</span></span>, <span class="string-name"><span class="surname">Durme</span>, <span class="given-names">B. V.</span></span>, <span class="string-name"><span class="surname">Bowman</span>, <span class="given-names">S. R.</span></span>, <span class="string-name"><span class="surname">Das</span>, <span class="given-names">D.</span></span> and <span class="string-name"><span class="surname">Pavlick</span>, <span class="given-names">E.</span></span> (<span class="year">2019</span>b). What do you learn from context? probing for sentence structure in contextualized word representations. <span class="source">CoRR</span>, abs/1905.06316.<a class='ref-link' target='_blank' aria-label='Google Scholar link for Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D. and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs/1905.06316.' href=https://scholar.google.com/scholar?q=Tenney,+I.,+Xia,+P.,+Chen,+B.,+Wang,+A.,+Poliak,+A.,+McCoy,+R.+T.,+Kim,+N.,+Durme,+B.+V.,+Bowman,+S.+R.,+Das,+D.+and+Pavlick,+E.+(2019b).+What+do+you+learn+from+context?+probing+for+sentence+structure+in+contextualized+word+representations.+CoRR,+abs/1905.06316.>Google Scholar</a></div></div></div><div id="ref70" aria-flowto="reference-70-content reference-70-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 70 in the content" id="reference-70-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-70-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Vilares</span>, <span class="given-names">D.</span></span>, <span class="string-name"><span class="surname">Strzyz</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Søgaard</span>, <span class="given-names">A.</span></span> and <span class="string-name"><span class="surname">G’omez-Rodr’iguez</span>, <span class="given-names">C.</span></span> (<span class="year">2020</span>). Parsing as pretraining. In <em class="italic">AAAI Conference on Artificial Intelligence</em>
<a class='ref-link' target='_blank' aria-label='CrossRef link for Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence' href=https://dx.doi.org/10.1609/aaai.v34i05.6446>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence' href=https://scholar.google.com/scholar?q=Vilares,+D.,+Strzyz,+M.,+Søgaard,+A.+and+G’omez-Rodr’iguez,+C.+(2020).+Parsing+as+pretraining.+In+AAAI+Conference+on+Artificial+Intelligence>Google Scholar</a></div></div></div><div id="ref71" aria-flowto="reference-71-content reference-71-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 71 in the content" id="reference-71-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-71-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Wang</span>, <span class="given-names">W.</span></span>, <span class="string-name"><span class="surname">Bi</span>, <span class="given-names">B.</span></span>, <span class="string-name"><span class="surname">Yan</span>, <span class="given-names">M.</span></span>, <span class="string-name"><span class="surname">Wu</span>, <span class="given-names">C.</span></span>, <span class="string-name"><span class="surname">Xia</span>, <span class="given-names">J.</span></span>, <span class="string-name"><span class="surname">Bao</span>, <span class="given-names">Z.</span></span>, <span class="string-name"><span class="surname">Peng</span>, <span class="given-names">L.</span></span> and <span class="string-name"><span class="surname">Si</span>, <span class="given-names">L.</span></span> (<span class="year">2020</span>). Structbert: Incorporating language structures into pre-training for deep language understanding. In <em class="italic">8th International Conference on Learning Representations, ICLR</em>, <span class="publisher-loc">Ethiopia</span>: Addis Ababa, OpenReview.net,<a class='ref-link' target='_blank' aria-label='Google Scholar link for Wang, W., Bi, B., Yan, M., Wu, C., Xia, J., Bao, Z., Peng, L. and Si, L. (2020). Structbert: Incorporating language structures into pre-training for deep language understanding. In 8th International Conference on Learning Representations, ICLR, Ethiopia: Addis Ababa, OpenReview.net,' href=https://scholar.google.com/scholar?q=Wang,+W.,+Bi,+B.,+Yan,+M.,+Wu,+C.,+Xia,+J.,+Bao,+Z.,+Peng,+L.+and+Si,+L.+(2020).+Structbert:+Incorporating+language+structures+into+pre-training+for+deep+language+understanding.+In+8th+International+Conference+on+Learning+Representations,+ICLR,+Ethiopia:+Addis+Ababa,+OpenReview.net,>Google Scholar</a></div></div></div><div id="ref72" aria-flowto="reference-72-content reference-72-button" class="circle-list__item"><!----> <div class="circle-list__item__indicator"><AppButton icon="up-circle" aria-label="Return to the reference 72 in the content" id="reference-72-button" class="circle-list__item__indicator__up"></AppButton></div> <div aria-hidden="true" data-test-hidden="true" class="circle-list__item__number">
        
      </div> <div class="circle-list__item__grouped"><div id="reference-72-content" class="circle-list__item__grouped__content"><span class="string-name"><span class="surname">Wu</span>, <span class="given-names">Z.</span></span>, <span class="string-name"><span class="surname">Chen</span>, <span class="given-names">Y.</span></span>, <span class="string-name"><span class="surname">Kao</span>, <span class="given-names">B.</span></span> and <span class="string-name"><span class="surname">Liu</span>, <span class="given-names">Q.</span></span> (<span class="year">2020</span>). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In <em class="italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, Online. Association for Computational Linguistics, pp. <span class="fpage">4166</span>–<span class="lpage">4176</span>.<a class='ref-link' target='_blank' aria-label='CrossRef link for Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.' href=https://dx.doi.org/10.18653/v1/2020.acl-main.383>CrossRef</a><a class='ref-link' target='_blank' aria-label='Google Scholar link for Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.' href=https://scholar.google.com/scholar?q=Wu,+Z.,+Chen,+Y.,+Kao,+B.+and+Liu,+Q.+(2020).+Perturbed+masking:+Parameter-free+probing+for+analyzing+and+interpreting+BERT.+In+Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Online.+Association+for+Computational+Linguistics,+pp.+4166–4176.>Google Scholar</a></div></div></div></div></div> <!----></div></div> <!----> <div id="figures-tab" class="figures tab-pane" data-v-241a4b23><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 0" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 1.</span> <span data-v-241a4b23><span class="p">Sample input and corresponding target output for the general idiom token identification task</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 1" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-96676-mediumThumb-png-S2977042424000438_fig1.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-86121-optimisedImage-png-S2977042424000438_fig1.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 1.</span> <span data-v-241a4b23><span class="p">Topic-Aware probing method.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 2" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 2.</span> <span data-v-241a4b23><span class="p">Number of tail topics from 10 topic models on Bigram shift and VNIC dataset</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 3" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-52050-mediumThumb-png-S2977042424000438_fig2.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-84769-optimisedImage-png-S2977042424000438_fig2.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 2.</span> <span data-v-241a4b23><span class="p">Experimental design.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 4" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 3.</span> <span data-v-241a4b23><span class="p">Average seen and unseen AUC ROC scores and their differences along with standard deviations for different embeddings on the Bigram Shift Probing task and the General Idiom Token Identification task</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 5" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-68511-mediumThumb-png-S2977042424000438_fig3.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-99766-optimisedImage-png-S2977042424000438_fig3.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 3.</span> <span data-v-241a4b23><span class="p">Seen and Unseen AUC ROC scores from GloVe and different layers of BERT and RoBERTa on the Bigram Shift Task.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 6" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-83266-mediumThumb-png-S2977042424000438_fig4.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-49180-optimisedImage-png-S2977042424000438_fig4.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 4.</span> <span data-v-241a4b23><span class="p">Seen and Unseen AUC ROC scores from different layers of BERT and RoBERTa with GloVe baseline on General Idiom Token Identification Task.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 7" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-77274-mediumThumb-png-S2977042424000438_fig5.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-13883-optimisedImage-png-S2977042424000438_fig5.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 5.</span> <span data-v-241a4b23><span class="p">Difference between seen scores and unseen scores from different layers of BERT and RoBERTa on General Idiom Token Identification Task.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 8" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 4.</span> <span data-v-241a4b23><span class="p">Descriptions and summary statistics of the datasets for the VNIC, Bigram shift, and 8 other probing tasks</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 9" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 5.</span> <span data-v-241a4b23><span class="p">Number of tail topics from 10 topic models on datasets of other 8 probing tasks</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 10" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-93684-mediumThumb-png-S2977042424000438_fig6.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-02891-optimisedImage-png-S2977042424000438_fig6.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 6.</span> <span data-v-241a4b23><span class="p">Seen and Unseen AUC ROC scores from different layers of BERT with GloVe baseline on Probing Tasks.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 11" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-75777-mediumThumb-png-S2977042424000438_fig7.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-97774-optimisedImage-png-S2977042424000438_fig7.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 7.</span> <span data-v-241a4b23><span class="p">Seen and Unseen AUC ROC scores from different layers of RoBERTa with GloVe baseline on Probing Tasks.</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 12" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Table 6.</span> <span data-v-241a4b23><span class="p">Average seen and unseen AUC ROC scores and their differences for GloVe and best BERT and RoBERTa layer embeddings on different probing tasks—tasks are ranked in the descending order of the difference between GloVe Seen score and GloVe Unseen score</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 13" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-21219-mediumThumb-png-S2977042424000438_fig8.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-02417-optimisedImage-png-S2977042424000438_fig8.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 8.</span> <span data-v-241a4b23><span class="p">GloVe Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task (Note that scores of SOMO and CI are very similar and therefore both of them are overlapping in the plot).</span></span></p></div> </div></div> <hr aria-hidden="true" class="separator dashed" data-v-7036083a data-v-241a4b23></div><div data-v-241a4b23><div class="figures__item" data-v-241a4b23><div class="figures__item__image-box" data-v-241a4b23><button class="figures__ref" data-v-241a4b23>
          View in content
        </button> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==" alt="Figure 14" data-zoomable="true" data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-06673-mediumThumb-png-S2977042424000438_fig9.jpg" data-enlarged-image="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary-alt:20241219095033-25832-optimisedImage-png-S2977042424000438_fig9.jpg" class="graphic" data-v-241a4b23></div> <div data-v-241a4b23><div class="caption" data-v-241a4b23><p data-v-241a4b23><span class="label" data-v-241a4b23>Figure 9.</span> <span data-v-241a4b23><span class="p">BERT and RoBERTa Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task.</span></span></p></div> </div></div> <!----></div></div> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div id="metrics-tab" publication-date="25 October 2024" class="metrics tab-pane" data-v-c41a0c86><div class="app-loader" data-v-c41a0c86></div></div></div></div> <!----></div> <div role="complementary" aria-label="related contents" class="column__main__right" data-v-01274b1d><div class="access-block row access-status" data-v-5fad35b8 data-v-01274b1d><span class="has-access" data-v-5fad35b8><img src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTQiIGhlaWdodD0iMTQiIHZpZXdCb3g9IjAgMCAxNCAxNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0xNCA3QzE0IDEwLjg2NjUgMTAuODY2NSAxNCA3IDE0QzMuMTMzNDUgMTQgMCAxMC44NjY1IDAgN0MwIDMuMTMzNDUgMy4xMzM0NSAwIDcgMEMxMC44NjY1IDAgMTQgMy4xMzM0NSAxNCA3WiIgZmlsbD0iIzFGOTYzOCIvPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTUuOTA4MTUgMTAuNjczTDIuNTQ2ODggNy4zMTE3Nkw0LjM0NjUxIDUuNTEyMTNMNS45MDgxNSA3LjA3Mzc2TDkuNjUyNTEgMy4zMjgxMkwxMS40NTIxIDUuMTI5MDNMNS45MDgxNSAxMC42NzNaIiBmaWxsPSIjRkVGRUZFIi8+Cjwvc3ZnPgo=" alt="" class="app-icon access" data-v-d2c09870 data-v-5fad35b8> <span class="sr-only" data-v-5fad35b8>You have </span>
    Access
  </span> <span class="open-access" data-v-5fad35b8><img src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTIiIGhlaWdodD0iMjAiIHZpZXdCb3g9IjAgMCAxMiAyMCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik0wLjE5Nzk5OCA0LjQ5ODA5QzAuODY3NDA1IDEuOTE1NTIgMy4yMTEzNCAwIDYuMDAwMzEgMEM5LjMwODcxIDAgMTEuOTk5OSAyLjY5MTczIDExLjk5OTkgNi4wMDAxNFYxMy4xOTk1QzExLjk5OTkgMTYuNTA3OSA5LjMwODcxIDE5LjE5OTcgNi4wMDAzMSAxOS4xOTk3QzIuNjkxOSAxOS4xOTk3IDAgMTYuNTA3OSAwIDEzLjE5OTVDMCA5Ljg5MDk2IDIuNjkxOSA3LjE5OTczIDYuMDAwMzEgNy4xOTk3M0M3LjQ1ODY5IDcuMTk5NzMgOC43OTYzMyA3LjcyNDA0IDkuODM3MjIgOC41OTIyOFY2LjAwMDE0QzkuODM3MjIgMy44ODM5MyA4LjExNjAxIDIuMTYyNzIgNi4wMDAzMSAyLjE2MjcyQzQuMjExMjUgMi4xNjI3MiAyLjcxNzI2IDMuMzk4MDkgMi4yOTQwNSA1LjA1Njk5TDAuMTk3OTk4IDQuNDk4MDlaTTIuMTYyNzIgMTMuMTk5NUMyLjE2MjcyIDE1LjMxNTcgMy44ODM5NCAxNy4wMzY5IDYuMDAwMzEgMTcuMDM2OUM4LjExNjAxIDE3LjAzNjkgOS44MzcyMiAxNS4zMTU3IDkuODM3MjIgMTMuMTk5NUM5LjgzNzIyIDExLjA4MzggOC4xMTYwMSA5LjM2MjQ1IDYuMDAwMzEgOS4zNjI0NUMzLjg4Mzk0IDkuMzYyNDUgMi4xNjI3MiAxMS4wODM4IDIuMTYyNzIgMTMuMTk5NVpNNC41MDczMyAxMy4xOTk1QzQuNTA3MzMgMTIuMzc0OCA1LjE3NTczIDExLjcwNjUgNi4wMDAzMSAxMS43MDY1QzYuODI0ODkgMTEuNzA2NSA3LjQ5MzI5IDEyLjM3NDggNy40OTMyOSAxMy4xOTk1QzcuNDkzMjkgMTQuMDI0MyA2LjgyNDg5IDE0LjY5MjUgNi4wMDAzMSAxNC42OTI1QzUuMTc1NzMgMTQuNjkyNSA0LjUwNzMzIDE0LjAyNDMgNC41MDczMyAxMy4xOTk1WiIgZmlsbD0iI0NCNEYwMCIvPgo8L3N2Zz4K" alt="" class="app-icon open-access" data-v-d2c09870 data-v-5fad35b8>
    Open access
  </span></div> <!----> <!----> <!----> <!----></div></div></div></div></div> <div id="cited-by-modal" role="dialog" aria-labelledby="Cited by modal" aria-hidden="true" class="modal fade" data-v-34d47c8d data-v-01274b1d><div class="modal-dialog modal-xl cited-by-modal" data-v-34d47c8d><div tabindex="-1" class="modal-content" data-v-34d47c8d><div class="modal-header" data-v-34d47c8d><h1 class="modal-header__heading" data-v-34d47c8d>Cited by</h1></div> <div class="modal-body" data-v-34d47c8d><div class="modal-body__loader" data-v-34d47c8d><div class="modal-body__loader__spinner" data-v-34d47c8d></div> <p class="modal-body__loader__message" data-v-34d47c8d>Loading...</p></div></div> <button aria-label="Close Cited by" aria-expanded="false" data-dismiss="modal" class="app-button cited-by-modal__button--close app-button__icon app-button--" data-v-2a038744 data-v-34d47c8d><img src="data:image/svg+xml;base64,CjxzdmcgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB3aWR0aD0iMzciIGhlaWdodD0iMzciIHZpZXdCb3g9IjAgMCAzNyAzNyI+PHBhdGggZmlsbD0iI0ZGRiIgZD0iTTIyLjMgMjAuNWwtMi0yLS4xLS4xLjEtLjEgMi0yYy41LS41LjYtMS40LjEtMS44LS41LS41LTEuMy0uNC0xLjguMWwtMiAyLS4xLjEtLjEtLjEtMi0yYy0uNS0uNS0xLjQtLjYtMS44LS4xLS41LjUtLjQgMS4zLjEgMS44bDIgMiAuMS4xLS4xLjEtMiAyYy0uNS41LS42IDEuNC0uMSAxLjguNS41IDEuMy40IDEuOC0uMWwyLTIgLjEtLjEuMS4xIDIgMmMuNS41IDEuNC42IDEuOC4xLjUtLjQuNC0xLjItLjEtMS44eiIvPjxwYXRoIGZpbGw9IiNGRkYiIGQ9Ik0xOC41IDM2QzguOSAzNiAxIDI4LjEgMSAxOC41UzguOSAxIDE4LjUgMSAzNiA4LjkgMzYgMTguNSAyOC4xIDM2IDE4LjUgMzZ6bTAtMzRDOS40IDIgMiA5LjQgMiAxOC41UzkuNCAzNSAxOC41IDM1IDM1IDI3LjYgMzUgMTguNSAyNy42IDIgMTguNSAyeiIvPjwvc3ZnPg==" alt="" class="app-icon icon close-modal" data-v-d2c09870 data-v-2a038744> <!----></button></div></div></div></div></div></div></div><script>window.__NUXT__=(function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az){return {layout:"default",data:[{article:{id:"0FFF33B18E284DAB8FE8DCF69A963A30",metadata:{title:t,htmlTitle:t,subtitle:a,authorsGroup:{authors:{contributors:[{givenNames:"Vasudevan",surname:"Nedumpozhimana",nameStyle:d,affiliations:[{text:"ADAPT Research Centre, Technological University Dublin, Dublin, Ireland"}],isCorresponding:c,notes:"\u003Cdiv class=\"corresp\"\u003E\u003Cspan class=\"bold\"\u003ECorresponding author:\u003C\u002Fspan\u003E Vasudevan Nedumpozhimana; Email: \u003Ca href=\"mailto:vasudevan.nedumpozhimana@tudublin.ie\"\u003Evasudevan.nedumpozhimana@tudublin.ie\u003C\u002Fa\u003E\u003C\u002Fdiv\u003E",isAnonymous:b,fullName:"Vasudevan Nedumpozhimana",searchUrl:"\u002Fcore\u002Fsearch?filters%5BauthorTerms%5D=Vasudevan%20Nedumpozhimana&eventCode=SE-AU",orcidUrl:"https:\u002F\u002Forcid.org\u002F0000-0001-5161-8925"},{givenNames:"John D.",surname:"Kelleher",nameStyle:d,affiliations:[{text:"ADAPT Research Centre, School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland"}],isCorresponding:b,notes:a,isAnonymous:b,fullName:"John D. Kelleher",searchUrl:"\u002Fcore\u002Fsearch?filters%5BauthorTerms%5D=John%20D.%20Kelleher&eventCode=SE-AU",orcidUrl:d}]},translators:{contributors:[],label:"Translated by"}},collections:[],publishedDate:"25 October 2024",keywords:[{url:"\u002Fcore\u002Fsearch?filters[keywords]=Semantics",name:"Semantics"},{url:"\u002Fcore\u002Fsearch?filters[keywords]=topic modelling",name:"topic modelling"},{url:"\u002Fcore\u002Fsearch?filters[keywords]=idiom token identification",name:"idiom token identification"},{url:"\u002Fcore\u002Fsearch?filters[keywords]=neural language model",name:"neural language model"},{url:"\u002Fcore\u002Fsearch?filters[keywords]=transformer",name:"transformer"}],openPracticeBadges:[],doi:{url:"https:\u002F\u002Fdoi.org\u002F10.1017\u002Fnlp.2024.43",value:"10.1017\u002Fnlp.2024.43"},copyright:{statement:["© The Author(s), 2024. Published by Cambridge University Press"],holder:["The Author(s)"],year:[2024]},creativeCommons:{descriptions:["This is an Open Access article, distributed under the terms of the Creative Commons Attribution licence (\u003Ca href=\"http:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F\" target=\"_blank\"\u003Ehttp:\u002F\u002Fcreativecommons.org\u002Flicenses\u002Fby\u002F4.0\u002F\u003C\u002Fa\u003E), which permits unrestricted re-use, distribution and reproduction, provided the original article is properly cited."],oaLicenseType:"CC-BY",licenses:["CC","BY"]},acceptedManuscript:b,type:"research-article",pageRange:{range:"1 - 29",firstPage:E,lastPage:"29"},typeDescription:"Article",resultTypes:[],commentsCount:u,topicsAndSubtopics:d},journal:{id:F,title:p,titleSlug:G,mnemonic:"NLP",titleHistory:[],isFirstView:c,journalSlug:G,isCompanion:b,parentCompanionJournalName:p,associatedParentCollection:d,url:H,firstViewUrl:I,coverUrl:"https:\u002F\u002Fstatic.cambridge.org\u002Fcovers\u002FNLP_0_0_0\u002Fnatural-language-processing.jpg",submitMaterialsUrl:"\u002Fcore\u002Fjournals\u002Fnatural-language-processing\u002Finformation\u002Fauthor-instructions\u002Fsubmitting-your-materials",hasHistory:b,latestTitle:p,latestId:F,hasPastTitle:b},abstract:{textAbstracts:[{title:"Abstract",content:"\u003Cdiv class=\"abstract\" data-abstract-type=\"normal\"\u003E\u003Cp\u003ETransformer-based neural language models achieve state-of-the-art performance on various natural language processing tasks. However, an open question is the extent to which these models rely on word-order\u002Fsyntactic or word co-occurrence\u002Ftopic-based information when processing natural language. This work contributes to this debate by addressing the question of whether these models primarily use topic as a signal, by exploring the relationship between Transformer-based models’ (BERT and RoBERTa’s) performance on a range of probing tasks in English, from simple lexical tasks such as sentence length prediction to complex semantic tasks such as idiom token identification, and the sensitivity of these tasks to the topic information. To this end, we propose a novel probing method which we call \u003Cspan class=\"italic\"\u003Etopic-aware probing\u003C\u002Fspan\u003E. Our initial results indicate that Transformer-based models encode both topic and non-topic information in their intermediate layers, but also that the facility of these models to distinguish idiomatic usage is primarily based on their ability to identify and encode topic. Furthermore, our analysis of these models’ performance on other standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for these models.\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E",lang:J}]},content:{html:"\u003Cdiv class=\"article research-article NLM\"\u003E\n\n\u003Cdiv class=\"body\"\u003E\n\u003Cdiv class=\"sec intro\" data-magellan-destination=\"s1\" id=\"s1\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E1.\u003C\u002Fspan\u003E Introduction\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E Pre-trained deep neural language models such as BERT (Devlin \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref12\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Devlin, Chang, Lee and Toutanova\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E) and RoBERTa (Liu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref39\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer and Stoyanov\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) are used to generate contextualised distributed representations (vector embeddings) of natural language text. Models based on these contextualised embeddings have achieved excellent performance across a range of NLP tasks. Consequently, what type of information is encoded in the embeddings generated by these deep neural language models is an interesting research question.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Conneau \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref8\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Conneau, Kruszewski, Lample, Barrault and Baroni\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E) proposed the probing methodology as a way to understand what types of information are present in an embedding. A probing task is a classification problem where a model is trained on embeddings of sentences with the goal of categorising sentences based on a linguistic property. Examples of the types of properties that might act as the basis for the classification include the tense of the sentence, the length of the sentence, the depth of a parse tree, or the presence of particular pre-selected terms within a sentence. The probing method assumes that the success of a trained classification model (i.e., a probe) on a task indicates whether the embeddings the probe is trained on encode information relevant to the linguistic property the probe is attempting to identify.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E A number of probing studies of Transformer-based language models have suggested that these models encode word-order and syntactic information in their embeddings (Raganato and Tiedemann \u003Ca class=\"xref bibr\" href=\"#ref52\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Raganato and Tiedemann\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E; Hewitt and Manning \u003Ca class=\"xref bibr\" href=\"#ref27\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hewitt and Manning\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E; Clark \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref7\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Clark, Khandelwal, Levy and Manning\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E; Reif \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref55\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Reif, Yuan, Wattenberg, Viegas, Coenen, Pearce and Kim\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E; Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref28\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019a\u003C\u002Fa\u003E; Lin \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref37\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Lin, Tan and Frank\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E; Manning \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref41\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Manning, Clark, Hewitt, Khandelwal and Levy\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E; Arps \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref4\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Arps, Samih, Kallmeyer and Sajjad\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E; Pimentel \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref51\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pimentel, Valvoda, Stoehr and Cotterell\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E). Indeed, it has been claimed that these models rediscover the classical NLP pipeline, with the earlier layers encoding syntactic information and later layers semantic (Tenney, Das, and Pavlick, \u003Ca class=\"xref bibr\" href=\"#ref68\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Tenney, Das and Pavlick\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003Ea) (although this claim has been questioned (Niu, Lu, and Penn, \u003Ca class=\"xref bibr\" href=\"#ref46\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Niu, Lu and Penn\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E)). Recently, a number of studies (Sinha \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref64\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sinha, Parthasarathi, Pineau and Williams\u003C\u002Fspan\u003E2021b\u003C\u002Fa\u003E; Pham \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref50\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pham, Bui, Mai and Nguyen\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E; Gupta \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref20\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Gupta, Kvernadze and Srikumar\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E; Hessel and Schofield \u003Ca class=\"xref bibr\" href=\"#ref25\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hessel and Schofield\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E; Sinha \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref63\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sinha, Jia, Hupkes, Pineau, Williams and Kiela\u003C\u002Fspan\u003E2021a\u003C\u002Fa\u003E) have examined the sensitivity of neural language models to word-order perturbations during pretraining, fine-tuning, and\u002For inference across standard benchmarks such as GLUE and PAWS and found the performance of these models is relatively insensitive to word order (although the results reported by Abdou \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref1\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Abdou, Ravishankar, Kulmizev and Søgaard\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) indicate that even when trained on shuffled text neural language models do encode some word order information). One interpretation of these word-permutation results is that we need to develop more challenging benchmarks in order to really assess the linguistic abilities of modern NLP models (Sinha \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref63\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sinha, Jia, Hupkes, Pineau, Williams and Kiela\u003C\u002Fspan\u003E2021a\u003C\u002Fa\u003E). A parallel interpretation is that much of the performance of these models on current benchmarks is based on shallow surface-level information such as word co-occurrence\u002Ftopic, rather than syntactic information.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Given that these language models encode multiple forms of linguistic information and yet their performance on a range of tasks seems to be insensitive to word order perturbations, we are interested in examining the relative contribution of these different types of information to the improvement in NLP that Transformer-based language models have achieved in the last number of years. In particular, is this improvement primarily based on simply more effective topic modelling or is it that Transformer language models rely more on combining a variety of other linguistics signals? The concept of a topic is directly related to the idea of a coherent group of concepts, or entities in the world, that are likely to co-occur (Manning and Schutze \u003Ca class=\"xref bibr\" href=\"#ref40\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Manning and Schutze\u003C\u002Fspan\u003E1999\u003C\u002Fa\u003E) and so share a non-taxonomic semantic association (Kacmajor and Kelleher \u003Ca class=\"xref bibr\" href=\"#ref30\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Kacmajor and Kelleher\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E). Consequently, words that refer to entities\u002Fconcepts that belong to the same topic are more likely to co-occur than words that refer to entities\u002Fconcepts from different topics. We focus on the relative contribution of topic versus non-topic information because the topic information, understood in terms of word co-occurrence, is directly related to the masked language modelling and next-word prediction objective functions used to train many language models like BERT, RoBERTa, and GPT. However, as Mickus \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref42\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Mickus, Paperno, Constant and van Deemter\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) pointed out, models like BERT which are also trained on next-sentence prediction objective are not purely based on distributional semantics (i.e., word co-occurrence). On the other hand, models like RoBERTa which are trained only on masked language model objective may be more focused on encoding distributional semantics. Consequently, our study of the relative contribution of topic versus non-topic information to both BERT’s and RoBERTa’s performance is relevant both to the current debate on the extent to which neural language models encode and use syntactic information, and also sheds light on the ongoing theoretical debate about whether these models are based on distributional semantics or not.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E We approach the research question of the extent to which Transformer-based pre-trained language models rely on the topic information from two complementary directions. First, we propose a new methodology for probing which we call topic-aware probing. We also experiment using a variety of probing tasks some of which we expect to be less sensitive to topic information and others to be more sensitive. Combining the novel topic-aware probing methodology with an analysis across a range of probing tasks enables us to explore the extent to which Transformer-based models are reliant on topic versus non-topic information. We selected BERT (Devlin \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref12\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Devlin, Chang, Lee and Toutanova\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E) and RoBERTa (Liu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref39\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer and Stoyanov\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) base models to conduct our experiments.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Within the set of probing tasks that we examine, we foreground the task of idiom token identification because the encoding of idiomatic information in neural models is relatively understudied (e.g., it is not one of the standard probing tasks proposed by Conneau \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref8\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Conneau, Kruszewski, Lample, Barrault and Baroni\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E)), and prior research suggests that identifying idiomatic usage requires the encoding of lexical, syntactic and topic information (Nedumpozhimana, Klubička, and Kelleher \u003Ca class=\"xref bibr\" href=\"#ref45\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana, Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E). An idiom is a multiword expression with a meaning that cannot be composed of its parts (Sporleder \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref66\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sporleder, Li, Gorinski and Koch\u003C\u002Fspan\u003E2010\u003C\u002Fa\u003E). It is hard to find a single agreed-upon definition for idioms in the literature, but they are often defined as sequences of words involving some degree of semantic idiosyncrasy or non-compositionality (Fazly, Cook, and Stevenson, \u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E). Idioms appear in all languages and text genres, prototypical examples from English include expressions such as \u003Cem class=\"italic\"\u003Eby and large\u003C\u002Fem\u003E and \u003Cem class=\"italic\"\u003Ekick the bucket\u003C\u002Fem\u003E (Sag \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref58\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sag, Baldwin, Bond, Copestake and Flickinger\u003C\u002Fspan\u003E2002\u003C\u002Fa\u003E). An idiomatic expression can have both idiomatic and literal meanings associated with it. Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E) highlight this aspect of idiomatic expressions and illustrate it with the expression \u003Cem class=\"italic\"\u003Emake a face\u003C\u002Fem\u003E which has an idiomatic sense in the sentence \u003Cem class=\"italic\"\u003EThe little girl made a funny face at her mother\u003C\u002Fem\u003E and has a literal sense in the sentence \u003Cem class=\"italic\"\u003Eshe made a face on the snowman using a carrot and two buttons\u003C\u002Fem\u003E. Building on this distinction, Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E) define the task of idiom token identification as deciding whether a particular usage of a given idiomatic expression is an idiomatic usage or a literal usage. While this idiom token identification task by Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E) identifies the idiomatic usage of a specific idiomatic expression in a sentence, we generalise the problem to identify the idiomatic usage within a sentence of any expression from a target category of multiword expressions. In this case, we considered the category of verb-noun idiomatic expressions from VNIC dataset (Cook, Fazly, and Stevenson \u003Ca class=\"xref bibr\" href=\"#ref10\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Cook, Fazly and Stevenson\u003C\u002Fspan\u003E2008\u003C\u002Fa\u003E) as our target set, but in principle, it could be any set of multiword expressions that the model is trained on.\u003Ca class=\"xref fn\" href=\"#fn1\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Ea\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E As shown in Table \u003Ca class=\"xref table\" href=\"#tbl1\"\u003E1\u003C\u002Fa\u003E, the input of this task is a sentence that contains a usage of one of the target expressions in it (in our case, a VNIC expression) and we do not explicitly provide any information to the model regarding which idiomatic expression is present in the sentence. The same model is used to process all sentences irrespective of which expression is present in the sentence. This task of general idiom token identification is a sentence-level binary classification task and the model is required to label the input sentence as ‘Idiomatic’ if it contains an expression from the target category that is being used idiomatically, and ‘Literal’ if the expression is being used literally. We pay particular attention to the task of idiom token identification because a review of the literature on idiom token identification (see Section \u003Ca class=\"xref sec\" href=\"#s2-2\"\u003E2.2\u003C\u002Fa\u003E) reveals that this task is sensitive not only to the topic information, but also to a variety of other types of information, such as lexical and syntactic fixedness, or fluency based information. Consequently, this task provides an ideal case study to explore the relative contribution of different types of linguistics information to the performance of Transformer-based neural language models on a task.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl1\" id=\"tbl1\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 1.\u003C\u002Fspan\u003E Sample input and corresponding target output for the general idiom token identification task\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"666\" height=\"135\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E To summarise, the main contributions of this work are: (a) we present an extension to the probing method called topic-aware probing, (b) we assess the contribution of topic-based information to the performance of a Transformer-based probing on the task of general idiom token identification, and (c) more generally we explore the relationship between topic and the performance of Transformer-based neural language models across a range of probing tasks and find that tasks that are relatively insensitive to topic are also tasks that Transformer-based neural language models find relatively difficult.\u003Ca class=\"xref fn\" href=\"#fn2\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Eb\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E\n\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec other\" data-magellan-destination=\"s2\" id=\"s2\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E2.\u003C\u002Fspan\u003E Related work\u003C\u002Fh2\u003E\n\u003Cdiv class=\"sec\" data-magellan-destination=\"s2-1\" id=\"s2-1\"\u003E\n\n\u003Ch3 class=\"B\"\u003E\u003Cspan class=\"label\"\u003E2.1\u003C\u002Fspan\u003E Transformer-based neural language models\u003C\u002Fh3\u003E\n\u003Cp class=\"p\"\u003E Most of the recent neural language models are Transformer-based and many pre-trained Transformer-based language models achieve very good performance on most of the downstream natural language processing tasks. However, due to the distributed nature of the representations used by these models and the opacity of their processing of information arising from the internal complexity of Transformer neural architecture, the specific types of information these models extract from language are unclear. In response to this, there is a growing body of work focused on understanding the basis for the state-of-the-art performance of these models. For example, Rogers \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref56\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Rogers, Kovaleva and Rumshisky\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) surveyed over 150 papers related to the BERT model, one of the foundational Transformer-based neural language models and reviewed various kinds of information learned by BERT.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Focusing first on BERT’s ability to encode syntactic information, a number of studies in the literature investigate whether BERT learns, or internally represents, any syntactic information about the input sentence. These investigations are interesting because BERT is pre-trained on a sequence of words and the pretraining objectives are not syntactic tasks. Lin \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref37\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Lin, Tan and Frank\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) showed that BERT internally represents the syntactic tree structure or an input sentence, and Hewitt and Manning (\u003Ca class=\"xref bibr\" href=\"#ref27\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hewitt and Manning\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) showed that it is possible to learn transformation matrices from BERT representations which can be used to recover syntactic dependency relations within the sentences from the PennTreebank. Other studies have also shown that enough syntactic information is encoded in BERT sentence representations to allow the recovery of the parse tree structure of an input sentence (Vilares \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref70\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Vilares, Strzyz, Søgaard and G’omez-Rodr’iguez\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E; Kim \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref31\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Kim, Choi, Edmiston and goo Lee\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E; Rosa and Mareček, \u003Ca class=\"xref bibr\" href=\"#ref57\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Rosa and Mareček\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E). However, Ettinger (\u003Ca class=\"xref bibr\" href=\"#ref14\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ettinger\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) shows that BERT is insensitive to malformed input and argues that therefore the syntactic knowledge in BERT is either incomplete, or else BERT doesn’t rely on it for solving tasks. Furthermore, Glavaš and Vulić (\u003Ca class=\"xref bibr\" href=\"#ref18\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Glavaš and Vulić\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E) show that intermediate fine-tuning of BERT for a supervised parsing task does not improve BERT’s performance, suggesting that BERT does not rely on traditional syntactic knowledge for solving tasks. The divergence between studies demonstrating BERT’s ability to encode syntactic information and those that question this ability may be explained through the work of Wu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref72\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Wu, Chen, Kao and Liu\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E). Wu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref72\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Wu, Chen, Kao and Liu\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) conducted a probing experiment to assess the impact of each word on predicting other words in a masked language model. Their study found that words in the same syntactic sub-tree have a larger impact on each other. Interestingly their results also show that although BERT learns some syntactic information it is not very similar to linguistically annotated resources and that the impact of performance on downstream NLP tasks achieved by using the syntactic structural information encoded by BERT is comparable, and even superior, to the human-designed syntactic structural information.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E These studies on BERT have been extended to other BERT-like neural language models. Arps \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref4\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Arps, Samih, Kallmeyer and Sajjad\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) investigated the extent to which neural language models (namely BERT, XLNet, RoBERTa, and DistilBERT) implicitly learn syntactic structure. They found that constituency parse trees of sentences can be extracted from distributed representations generated by these language models. Their results show that if the syntactic structure of data is correct then tree structures are extractable even if the data is semantically ill-formed. This suggests that without the help of semantic information, syntactic information can be extracted from these language models which indicates that these language models do encode syntactic information. By using a novel probing method based on the architectural bottleneck principle, Pimentel \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref51\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pimentel, Valvoda, Stoehr and Cotterell\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) also showed that the syntactic structure of a sentence is mostly extractable from BERT, ALBERT, and RoBERTa language models. They also point out that even though syntactic information is extractable from language models it is not clear whether this information is actually used by these models.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E There are also several studies in the literature which investigate the presence of semantic information in Transformer-based neural language models. Studies conducted by Ettinger (\u003Ca class=\"xref bibr\" href=\"#ref14\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ettinger\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) and Tenney \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref69\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) show that BERT encodes some knowledge about semantic roles and entity relations. However, Balasubramanian \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref5\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Balasubramanian, Jain, Jindal, Awasthi and Sarawagi\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) showed that although a BERT-based model achieves a good performance in Named Entity Recognition, there is a huge performance drop after replacing the names in the dataset, which indicates that BERT does not actually form a generic idea about named entities. By using a novel methodology to probe linguistic information for logical inference Chen and Gao (\u003Ca class=\"xref bibr\" href=\"#ref6\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Chen and Gao\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) observed that RoBERTa and BERT language models encode information on simple semantic phenomena rather than complex semantic phenomena.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Given that Transformer-based neural language models appear to partially encode both syntactic and semantic information about natural language text a natural question to ask is where in the Transformer architecture this information is encoded? Moreover, is the encoding of different types of information localised to specific layers in a Transformer or is it spread across multiple layers? A number of studies have investigated where the encoding of information occurs within the Transformer architecture of BERT: Lin \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref37\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Lin, Tan and Frank\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) report that word order information decreases after the 4th layer of the base-BERT model; Hewitt and Manning (\u003Ca class=\"xref bibr\" href=\"#ref27\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hewitt and Manning\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) report that the reconstruction of tree depth is most successful using the middle-layer embeddings of BERT (6th to 9th layers of base-BERT); Goldberg (\u003Ca class=\"xref bibr\" href=\"#ref19\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Goldberg\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) show that the best subject-verb agreement is obtained by using 8th and 9th layer of the base-BERT model, and Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref29\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) observed that the best performances of various high-level syntactic probing tasks are achieved with middle-layer embeddings of BERT. This set of results suggests that the initial layers of BERT encode information about linear word order and the later layers encode more hierarchical and syntactic information.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E There are, however, conflicting results about the presence of syntactic information in various layers of BERT. For example, Tenney \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref69\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) and Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref29\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) observed that the best performance on basic syntactic tasks such as POS tagging and chunking is achieved using the initial layer embeddings of BERT and good performance on high-level tasks like parsing and other semantic tasks can be achieved using embeddings from the middle layers of BERT. On the contrary, Liu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref38\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Liu, Gardner, Belinkov, Peters and Smith\u003C\u002Fspan\u003E2019a\u003C\u002Fa\u003E) observed that the best performance for both POS Tagging and chunking is obtained using middle-layer embeddings of BERT, and Tenney \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref69\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Tenney, Xia, Chen, Wang, Poliak, McCoy, Kim, Durme, Bowman, Das and Pavlick\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) observed that syntactic information is located in early and middle layers but semantic information is spread across all layers of the model. While the initial and middle layers of BERT encode syntactic and semantic information, the final layers of BERT are more task-specific, and Kovaleva \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref34\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Kovaleva, Romanov, Rogers and Rumshisky\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) observed that these layers change the most while fine-tuning. This is in agreement with the observation of Liu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref38\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Liu, Gardner, Belinkov, Peters and Smith\u003C\u002Fspan\u003E2019a\u003C\u002Fa\u003E) that overall the best performance is generally obtained using middle-layer embeddings and that embeddings from these layers are the most transferable across different tasks.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Extending this body of work, we investigate the relationship between topic and the performance of Transformer-based language models (BERT and RoBERTa) across a number of tasks. We also examine in which layers of these Transformer models the encoding of topic and non-topic information is located.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec\" data-magellan-destination=\"s2-2\" id=\"s2-2\"\u003E\n\n\u003Ch3 class=\"B\"\u003E\u003Cspan class=\"label\"\u003E2.2\u003C\u002Fspan\u003E Idiom token identification\u003C\u002Fh3\u003E\n\u003Cp class=\"p\"\u003E Idioms are a sub-type of multiword expression (MWE). Other types of MWEs include compound nouns and verb particle constructions. Consequently, research on MWE is also relevant to the aspects of this research that is focused on idiom token identification. The MWE identification problem has been widely addressed within the NLP research community via the development and release of multiple shared tasks. To support research on understanding, modelling, and processing of MWEs, Savary \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref61\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Savary, Ramisch, Cordeiro, Sangati, Vincze, QasemiZadeh, Candito, Cap, Giouli, Stoyanova and Doucet\u003C\u002Fspan\u003E2017\u003C\u002Fa\u003E) introduced a shared task called PARSEME. This shared task released annotated datasets for 18 languages. Ramisch \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref53\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ramisch, Cordeiro, Savary, Vincze, Barbu Mititelu, Bhatia, Buljan, Candito, Gantar, Giouli, Güngör, Hawwari, Iñurrieta, Kovalevskaitė, Krek, Lichte, Liebeskind, Monti, Parra Escartín, QasemiZadeh, Ramisch, Schneider, Stoyanova, Vaidya and Walsh\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E) extended this shared task to the PARSEME 1.1 task by updating the annotation methodology and releasing annotated data for 20 languages. The PARSEME shared task was further extended to the PARSEME 1.2 edition by Ramisch \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref54\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ramisch, Savary, Guillaume, Waszczuk, Candito, Vaidya, Barbu Mititelu, Bhatia, Iñurrieta, Giouli, Güngör, Jiang, Lichte, Liebeskind, Monti, Ramisch, Stymne, Walsh and Xu\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) in which the task involved identifying unseen MWEs and they released annotated data for 14 languages for this new task.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Schneider \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref62\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Schneider, Hovy, Johannsen and Carpuat\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) proposed a task (SemEval-2016 Task 10) which combines the labelling of multiword expressions and supersenses by the assumption that MWE and supersenses are tightly coupled. Recently Tayyar Madabushi \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref67\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Tayyar Madabushi, Gow-Smith, Garcia, Scarton, Idiart and Villavicencio\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) proposed a task of multilingual idiomaticity detection and idiomaticity representation. The proposed idiomaticity detection is a binary task to identify whether a sentence contains an idiomatic expression with the help of two context sentences. This task has a zero-shot setting in which MWEs in the training set and test set are disjoint. It also has a One Shot setting in which the training set has one idiomatic and one non-idiomatic example of each MWE in the test set. The idiomaticity representation task is an idiomatic semantic textual similarity task, in which the semantic similarity between sentences with an idiomatic phrase, correct literal paraphrase of the idiomatic phrase, and incorrect literal paraphrase of the idiomatic phrase should be predicted.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Constant \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref9\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Constant, Eryiğit, Monti, van der Plas, Ramisch, Rosner and Todirascu\u003C\u002Fspan\u003E2017\u003C\u002Fa\u003E) did a detailed survey on multiword expression (MWE) processing by dividing it into two subtasks: MWE (type) discovery and MWE (token) identification. The MWE (type) discovery subtask is focused on identifying new MWEs from text and the MWE (token) identification subtask involves automatically annotating multiword expressions in running text by associating them with known multiword expression types. Our general idiom token identification task has aspects of both of these tasks in it. On the one hand, we are interested in identifying whether a particular piece of text contains a non-compositional usage from a given category of MWEs. So from this perspective, our task is similar to MWE (token) identification in that we are annotating text, although in our case the annotation is a binary label applied to the entire sentence rather than an annotation at the token level. However, because the models we train are in principle able to identify new idiomatic expressions from a given category our work also has application in the area of MWE (type) discovery, although this aspect of our work is not the primary focus in this paper (see Nedumpozhimana \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref45\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana, Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) for work on generalising to unknown—i.e., unseen during training—expressions within a given category).\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Hashimoto and Kawahara (\u003Ca class=\"xref bibr\" href=\"#ref24\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hashimoto and Kawahara\u003C\u002Fspan\u003E2008\u003C\u002Fa\u003E) report research on idiom token identification for Japanese idioms and found that features normally used in word sense disambiguation that are defined over the context surrounding an expression worked well. Around the same time, Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E) proposed methods for idiom token identification for English based on two assumptions. First, they assumed that each idiomatic expression has a relatively fixed canonical syntactic form and that idiomatic usages of an expression tend to have this canonical syntactic form, whereas literal usages are less syntactically restricted. They also assumed that literal and idiomatic usages of an expression tend to occur with different sets of words in the surrounding context. Their results indicate that their idiom token identification model based on the syntactic form of an expression outperformed their model based on the words in the surrounding context. However, they note that this somewhat surprising result may have been affected by the fact that the definition of the typical word sets for the surrounding context of idiomatic and non-idiomatic usages used in their experiments was created using an unsupervised approach that may have resulted in noisy definitions of surrounding contexts.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Li and Sporleder (\u003Ca class=\"xref bibr\" href=\"#ref35\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Li and Sporleder\u003C\u002Fspan\u003E2010\u003C\u002Fa\u003Ea) examined the efficacy of feature sets based on global lexical context, discourse cohesion and local lexical features, such as cue words, for idiom token identification. They found that features based on global lexical context and discourse cohesion were the most effective. Li and Sporleder (\u003Ca class=\"xref bibr\" href=\"#ref36\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Li and Sporleder\u003C\u002Fspan\u003E2010\u003C\u002Fa\u003Eb) confirmed the efficacy of discourse cohesion features for idiom token identification. Following this theme of contextual approaches to idiom token identification, Feldman and Peng (\u003Ca class=\"xref bibr\" href=\"#ref16\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Feldman and Peng\u003C\u002Fspan\u003E2013\u003C\u002Fa\u003E) and Peng \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref48\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Peng, Feldman and Vylomova\u003C\u002Fspan\u003E2014\u003C\u002Fa\u003E) explored topic features for the idiom token identification problem. Feldman \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref16\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Feldman and Peng\u003C\u002Fspan\u003E2013\u003C\u002Fa\u003E) and Peng \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref48\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Peng, Feldman and Vylomova\u003C\u002Fspan\u003E2014\u003C\u002Fa\u003E) based their work on the assumption that idiomatic usages will be semantically distant from the topics of the discourses in which they are present and so a candidate expression should be identified as an idiomatic usage if it is a semantic outlier with respect to the surrounding context.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Salton \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref59\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Salton, Ross and Kelleher\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) demonstrated the viability of building an idiom token identification model using a distributed sentence representation, specifically the sentence embeddings generated by Skip-Thought Kiros \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref32\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba and Fidler\u003C\u002Fspan\u003E2015\u003C\u002Fa\u003E). They proposed a model for both per-expression and general idiom token identification problems (i.e., developing a single idiom token identification model that works across multiple expressions from a given category of MWE). Unlike previous work which required separate contextual\u002Ftopic-based models for each expression being assessed, a distinctive aspect of the work by Salton \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref59\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Salton, Ross and Kelleher\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) is that, unlike previous work which required separate contextual\u002Ftopic-based models for each expression being assessed, their approach used a single model across all expressions within a category. Furthermore, their model only required the distributed embedding of the sentence the expression occurs within and did not need access to the surrounding context of the sentence. A natural question arising from the results reported by Salton \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref59\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Salton, Ross and Kelleher\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) is what are the kinds of information that the distributed representation of a sentence encodes which are so useful for idiom token identification? For example, these embeddings may be capturing syntactic or lexical fixedness features, similar to those proposed by Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E), or be efficiently encoding some form of topic-based signal (efficient both in the sense of only requiring a small sample of text—i.e., the sentence—to pick up the relevant context, and also in terms of being able to do this across multiple expressions with, presumably, variation in the topic signals associated with each expression). Hashempour and Villavicencio (\u003Ca class=\"xref bibr\" href=\"#ref23\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hashempour and Villavicencio\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) also performed an idiom token identification experiment and found that contextual word embeddings, such as those generated by BERT, outperform non-contextual word embeddings. However, probing experiments by Garcia \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref17\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Garcia, Kramer Vieira, Scarton, Idiart and Villavicencio\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E) on contextualised vector space models like ELMo and BERT concluded that idiomatic usage is not yet accurately represented by these contextualised models. Extending the work on using neural sentence embeddings for idiom token identification, Nedumpozhimana and Kelleher (\u003Ca class=\"xref bibr\" href=\"#ref44\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana and Kelleher\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E) showed that more recent contextual distributed representations such as those generated by BERT models encode idiomatic information, and their result suggest that a topic signal might be the key information encoded in the BERT representation.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E More recently, Nedumpozhimana \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref45\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana, Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) report experiments using the game theory concept of Shapley Values to analyse the type of information that idiom token identification models based on BERT find useful. They first report expression-wise experiments using Shapley Value analysis that analysed the relative contribution of different expressions to the generalisation ability of an idiom token identification model. Then they used the results of these experiments together with an expression-wise analysis of the association between different expressions and different linguistic phenomena (syntactic and\u002For lexical fixedness, topic, and so on) to assess which types of linguistic information are more useful for idiom token identification. They find that a combination of idiom-intrinsic and topic-based features are useful for achieving generalisability, and argue that their results point to BERT encoding different types of linguistic information, including topic, lexical and syntactic information. Prompted by these findings, within our examination of the role of the topic information in the distributed representations generated by Transformer-based neural language models, we put a particular focus on the extent to which Transformer-based models (BERT and RoBERTa) rely on the topic information to identify idiomatic usage. Accordingly building on previous work that performed expression-wise analysis here we directly analyse the contribution of the topic information in general by assessing the ability of general idiom token identification models trained on sentences from one topic to generalise to sentences from other topics.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec other\" data-magellan-destination=\"s3\" id=\"s3\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E3.\u003C\u002Fspan\u003E Topic-aware probing\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E Our topic-aware probing method is designed to investigate the role of topic signals in a probing task. The basic idea is to train the probing model on samples from a particular topic and then test it on samples from the topic the training data were sampled from and separately on samples from other topics. We then analyse the difference in performance on the samples from the topic seen during training and the samples from unseen topics. A large difference in performance would indicate that the topic information is an important factor in determining the performance of a model on a task.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E The first step in topic-aware probing is to partition the dataset into \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline1.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline1.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E different topics using a topic model. Next, we split the set of samples in each topic into \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline2.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"8\" height=\"13\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline2.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$k$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E folds, for cross-fold validation. We then iterate through the topics and for each fold \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline3.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"5\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline3.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$i$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E in a topic we:\u003C\u002Fp\u003E\u003Col class=\"list number nomark\"\u003E\n\u003Cli class=\"list-item\"\u003E\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003E(1)\u003C\u002Fspan\u003E train the probe (e.g., the general idiom token identification model) using the data from the other folds in the topic;\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli class=\"list-item\"\u003E\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003E(2)\u003C\u002Fspan\u003E evaluate the probe on the \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline4.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"15\" height=\"17\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline4.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$i^{th}$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E fold in the topic and record the performance of the probe as a \u003Cem class=\"italic\"\u003Eseen topic score\u003C\u002Fem\u003E;\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003Cli class=\"list-item\"\u003E\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003E(3)\u003C\u002Fspan\u003E iterate through the other \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline5.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"29\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline5.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n{-}1$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E topics in the topic model, evaluate the probe on the corresponding \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline6.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"15\" height=\"17\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline6.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$i^{th}$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E fold in these other topics, and record the performance of the probe on each of these folds as an \u003Cem class=\"italic\"\u003Eunseen topic score\u003C\u002Fem\u003E.\u003C\u002Fp\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Fol\u003E\n\n\u003Cp class=\"p\"\u003E At the end of this process, for each of the \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline7.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline7.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E topics we have calculated \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline8.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"8\" height=\"13\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline8.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$k$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E seen topic scores and \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline9.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"76\" height=\"17\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline9.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$k \\times (n-1)$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E unseen topic scores. We then calculate for each topic the average seen topic score and the average unseen topic score. Figure \u003Ca class=\"xref fig\" href=\"#f1\"\u003E1\u003C\u002Fa\u003E illustrates the topic-aware probing method.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f1\" id=\"f1\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig1.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"2250\" height=\"2370\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig1.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 1.\u003C\u002Fspan\u003E Topic-Aware probing method.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E If the topic signal is important in terms of helping the probe to predict the label for the task then the performance of the probe model should be significantly better on samples from seen topics compared to unseen topics (i.e., the seen topic scores should be higher than the unseen topic scores). On the other hand, if we do not observe a significant difference in performance between samples from seen topics compared to unseen topics this would indicate that the topic is not an important signal in terms of the probing task.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E In our experiments, we use Latent Semantic Indexing (LSI) (also known as Latent Semantic Analysis) for topic modelling: LSI is an unsupervised topic modelling approach based on the distributional hypothesis (Deerwester \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref11\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Deerwester, Dumais, Landauer, Furnas and Harshman\u003C\u002Fspan\u003E1990\u003C\u002Fa\u003E). We use LSI because it captures word co-occurrence (Manning and Schutze \u003Ca class=\"xref bibr\" href=\"#ref40\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Manning and Schutze\u003C\u002Fspan\u003E1999\u003C\u002Fa\u003E; Eisenstein \u003Ca class=\"xref bibr\" href=\"#ref13\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Eisenstein\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) which is essentially the topic signal that we are assessing in this experiment.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E For cross-fold validation, we use 5 folds, and in order to maintain the label distribution within a topic across the 5 folds we use stratified sampling to create the folds. Note that some of the topics identified by the topic model can have less than 5 samples from some label categories and so for these topics, we would not be able to split the samples into 5 stratified folds. One option for dealing with these small topics is to discard them. This, however, would result in some labelled examples being discarded, something that may be undesirable if we are working with a small dataset. Consequently, we consider any topics with less than 5 samples from some label as a tail topic, and we iteratively merge the tail topics with other topics (preferably with some other tail topic) to reduce the number of tail topics. The iterative procedure for merging tail topics is as follows: if there is only one tail topic then we randomly select one of the non-tail topics and merge the tail topic with the selected non-tail topic, and if there is more than one tail topic then we randomly select two of the tail topics and merge them. This process of tail topic merging continues until there is no tail topic left. This tail-reduction process may reduce the semantic coherence within some topics because it involves merging unrelated topics into a single topic. Furthermore, this dilution in the topic signal may result in the topic-aware probing method underestimating the importance of the topic signal on a given task (i.e., it may increase the likelihood of a Type II error-false-negative-in a topic-aware probing experiment). In other words, if topic merging has an effect on an experimental analysis the effect is to reduce the sensitivity of the method to the topic signal by reducing the difference between seen and unseen scores. Consequently, in situations where we do see a difference between seen and unseen scores the merging of topics will only have weakened this difference, and not caused it. So when we see a difference, topic merging won’t be the cause of the difference. The more problematic case is where we don’t see a difference between seen and unseen scores. In this case, a topic difference may in fact exist but the merging may have diluted it. Fortunately, however, the problem of tail topics typically only arises for runs of the topic modelling process where we extract a large number of topics from a relatively small dataset. The number of tail topics obtained from different topic models on different datasets are shown in the coming sections in Tables \u003Ca class=\"xref table\" href=\"#tbl2\"\u003E2\u003C\u002Fa\u003E and \u003Ca class=\"xref table\" href=\"#tbl5\"\u003E5\u003C\u002Fa\u003E. Consequently, the topic merging process does not affect all the runs of a topic modelling process, and so one way to mitigate the effect of topic merging is to report the average seen versus unseen difference across multiple runs of a topic model with different numbers of topics identified in each run. This is one of the reasons why in our experiments we report the average difference between seen and unseen topic scores across 10 topic models with the number of topics ranging from 5 to 50. The other reason is that by reporting average differences across multiple topic models we reduce the sensitivity of the analysis to the number of topics chosen by a single topic modelling process. Once the tail topics are merged we split samples from each topic into 5 stratified folds.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl2\" id=\"tbl2\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 2.\u003C\u002Fspan\u003E Number of tail topics from 10 topic models on Bigram shift and VNIC dataset\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"582\" height=\"92\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png\" data-zoomable=\"false\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E Multilayered perceptron (MLP) models with one hidden layer are one of the standard models used in the probing literature (Conneau \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref8\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Conneau, Kruszewski, Lample, Barrault and Baroni\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E). Furthermore, probing experiments have demonstrated that dense embeddings, such as BERT embeddings, can encode information in a distributed manner (e.g., in the embedding norm (Klubička and Kelleher \u003Ca class=\"xref bibr\" href=\"#ref33\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E)) and so using a model type that is able to integrate information from across an embedding (such as an MLP) allows the probe to utilise this distributed information. Therefore we use an MLP model, with one hidden layer using ReLU as the hidden layer activation function, as the probing model for predicting the label from the distributed representation of a sample sentence.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec other\" data-magellan-destination=\"s4\" id=\"s4\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E4.\u003C\u002Fspan\u003E Experimentation\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E In order to confirm that topic-aware probing functions as expected, we first apply it to a probing task that we expect will not be sensitive to the topic information, namely bigram shift—a probing task introduced by Conneau \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref8\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Conneau, Kruszewski, Lample, Barrault and Baroni\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E). The bigram shift task is to predict whether any two consecutive words within a sentence have been swapped. We do not expect the topic signal to be a useful information source for this task because swapping two consecutive words in a sentence will not change the topic of the sentence (at least not from the perspective of a word co-occurrence-based topic model because the sentence will contain the same set of words after the swapping as the original sentence). Consequently, the bigram shift task will enable us to validate topic-aware probing. Our expectation is that because the bigram shift task is not sensitive to topic information we should observe similar seen and unseen topic scores if the methodology is working as expected.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Having checked the topic-aware probing methodology works as expected for the bigram shift task, we switch our focus to investigate the role of the topic as a signal in BERT-based and RoBERTa-based general idiom token identification. To do this we first confirm that general idiom token identification is sensitive to topic signals by using topic-aware probing, and then isolate the contribution of the topic signal to the performance of BERT and RoBERTa on this task by comparing the performance of BERT and RoBERTa to a (primarily) topic-based embedding model. We use GloVe embeddings to act as this topic baseline because GloVe embeddings are trained on the nonzero elements in a word-word co-occurrence matrix Pennington \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref49\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pennington, Socher and Manning\u003C\u002Fspan\u003E2014\u003C\u002Fa\u003E), and so the primary information captured by GloVe embeddings directly relates to the concept of the topic we are examining here. Furthermore, as we shall see in Section \u003Ca class=\"xref sec\" href=\"#s5\"\u003E5\u003C\u002Fa\u003E, our analysis of the results from applying topic-aware probing to the bigram shift task not only confirms that the topic-aware probing method is functioning as expected but also supports the assumption that GloVe embeddings primarily encode topic (word co-occurrence) information.\u003C\u002Fp\u003E\n\u003Cdiv class=\"sec\" data-magellan-destination=\"s4-1\" id=\"s4-1\"\u003E\n\n\u003Ch3 class=\"B\"\u003E\u003Cspan class=\"label\"\u003E4.1\u003C\u002Fspan\u003E Data preparation\u003C\u002Fh3\u003E\n\u003Cp class=\"p\"\u003E For the experiments on the bigram shift task, we used 119,998 English sentences from the established bigram shift dataset.\u003Ca class=\"xref fn\" href=\"#fn3\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Ec\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E The bigram shift dataset labels original sentences as ‘Original’ and inverted sentence as ‘Inverted’. The dataset contains 59,999 ‘Inverted’ sentences and 59,999 ‘Original’ sentences.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E The experiments on the general idiom token identification task are based on the VNIC dataset (Cook \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref10\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Cook, Fazly and Stevenson\u003C\u002Fspan\u003E2008\u003C\u002Fa\u003E). The VNIC dataset is a set of 2,979 English sentences with each sentence containing an instance of one of 53 idiomatic expressions. An idiomatic expression can be used either in an idiomatic or literal sense. The VNIC dataset contains manually annotated labels, where every sentence is marked as ‘Idiomatic usage’, ‘Literal usage’ or ‘Unknown’. If the idiomatic expression in the sentence is used idiomatically then the sentence will be labelled as ‘Idiomatic usage’, if the idiomatic expression is used literally then the sentence will be labelled as ‘Literal usage’, and if the usage is ambiguous for human annotator then the sentence will be marked as ‘Unknown’. The sentences with the ‘Unknown’ label are either idiomatic or literal samples but the human annotator was unable to decide whether it is idiomatic or not. Unfortunately, from a practical point of view, the ‘Unknown’ sentences without a manually annotated label (idiom or literal) are impossible to use for training or for evaluation in a supervised learning setup. Therefore we removed all such sentences from our dataset for our experimentation. The removal of these sentences from the dataset does simplify the task of general idiom token identification with respect to the task that humans processing language face. However, the focus of our analysis is on understanding what linguistic information (topic \u003Cem class=\"italic\"\u003Evs.\u003C\u002Fem\u003E non-topic) Transformer-based pre-trained language models like BERT and RoBERTa use when they are processing language, and so removing these ‘Unknown’ sentences may lead to a cleaner signal within the analysis of the performance of these models on the task. This filtering of the VNIC dataset left 2,566 samples of which 2,016 were idiomatic usages and 550 were literal usages across the 53 expressions. When preparing the training and test sets, we split the data by sentence rather than by expression, and so an idiomatic expression may appear in sentences in the training set and the test set.\u003Ca class=\"xref fn\" href=\"#fn4\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Ed\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E\n\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec\" data-magellan-destination=\"s4-2\" id=\"s4-2\"\u003E\n\n\u003Ch3 class=\"B\"\u003E\u003Cspan class=\"label\"\u003E4.2\u003C\u002Fspan\u003E Representations\u003C\u002Fh3\u003E\n\u003Cp class=\"p\"\u003E For our probing experiments, we generated a distributed representation of each sample sentence by using a pre-trained BERT model (\u003Cem class=\"italic\"\u003Ebert-base-uncased\u003C\u002Fem\u003E) and a pre-trained RoBERTa model (\u003Cem class=\"italic\"\u003Eroberta-base\u003C\u002Fem\u003E). Both these models are based on the Transformer encoder architecture with 12 layers, 768 hidden dimensions, and 12 attention heads. Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref29\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E) observed that different layers of the BERT architecture capture different kinds of information. So we generated different embeddings from each of the 12 different layers of both BERT and RoBERTa architectures. Our models have 12 layers and therefore we generated 12 different BERT and RoBERTa embeddings for each sample sentence. In each layer, we generated an aggregate distributed representation of the sentence by averaging the distributed representations of each token in the sentence. There are a number of ways that we could have generated a sentence embedding from BERT and RoBERTa, for example, we could have used the CLS token. However, Mosbach \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref43\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Mosbach, Khokhlova, Hedderich and Klakow\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) suggest that, for probing tasks, the average of the embeddings of the token in a sentence is a better sentence-level representation than the embedding of the CLS token. Also, as we explain below, in our experiments we use GloVe as a baseline topic-based distributed representation, and using the average token embedding for BERT and RoBERTa makes the process we use to generate BERT and RoBERTa sentence representations more consistent with the process we use to generate GloVe sentence representations, which is the average of the GloVe representation of all words in the sentence.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E GloVe is a distributed representation based on a word-to-word co-occurrence matrix Pennington \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref49\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pennington, Socher and Manning\u003C\u002Fspan\u003E2014\u003C\u002Fa\u003E). This approach is very similar to the topic modelling approaches, particularly to Latent Semantic Indexing, the approach we are using for our probing experiment. So we assume that GloVe embeddings primarily capture topic information of words from a large corpus. To generate the GloVe representation of a sample sentence we averaged the GloVe representations of each word in it. Therefore the GloVe embeddings used in our experiment likely neglect the syntactic structure of the sentence, and this is a deliberate choice as part of our methodology so that GloVe can be used as a metric for the expected performance of an embedding that primarily encodes topic on a probing task. We consider GloVe distributed representations as one of the baseline representations for topic-aware probing. The comparison against the GloVe baseline enables us to estimate the amount of topic and non-topic signal BERT and RoBERTa encode.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Hewitt and Liang (\u003Ca class=\"xref bibr\" href=\"#ref26\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Hewitt and Liang\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) warned that the probe itself can learn the task without using the information in the sentence representation especially when the probing model is powerful enough to capture the task objective. To control for this possible confounding factor we also train a baseline probe model for each task on random vector representations to measure the performance due to the power of the probing model. The random vector representations are created by randomly generating a 768-dimensional vector for each of the input sentences (we use 768-dimensional vectors so that the random vectors have the same dimension as the BERT and RoBERTa embeddings).\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec\" data-magellan-destination=\"s4-3\" id=\"s4-3\"\u003E\n\n\u003Ch3 class=\"B\"\u003E\u003Cspan class=\"label\"\u003E4.3\u003C\u002Fspan\u003E Experimental design\u003C\u002Fh3\u003E\n\u003Cp class=\"p\"\u003E In the topic-aware probing with LSI topic modelling, we have to specify the number of topics. If the number of topics is kept small then each topic will be more generic and this may reduce the power of the topic-aware probing, although this may be mitigated by the fact that a small number of topics will also result in larger training and test sets. Conversely, if the number of topics is too large then each topic will be very specific but the training and testing sample size will be reduced and this may result in the underperformance of the probe. This will be reflected in the seen scores and unseen scores and therefore in our analysis.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E In order to control for the confounding effects of the number and size of topics on our analysis, for both probing tasks (bigram shift and general idiom token identification), we repeat our topic-aware probing experiment 10 times while varying the number of topics from 5 to 50 in increments of 5. Consequently, each iteration of an experiment uses a different topic model in the topic-aware probing as the basis for the experiment. Note that as the number of topics approaches 50, the chance that the initial topic model in the topic-aware probing will contain topics with \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline10.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"23\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline10.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$\\lt 5$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E samples with some label (i.e., tail topics) increases. The actual number of tail topics from each of the 10 topic models on both the Bigram shift and VNIC datasets is shown in Table \u003Ca class=\"xref table\" href=\"#tbl2\"\u003E2\u003C\u002Fa\u003E. In such cases, the actual number of topics used for probing will be less than the specified number of topics after the tail reduction (as discussed in Section \u003Ca class=\"xref sec\" href=\"#s3\"\u003E3\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E To apply our topic-aware probing we first divide the dataset into different partitions with different topics. The topic model can divide samples with the same class label into the same partition (or same set of partitions) and in such cases, we can see that the topic model itself will internally do the classification task. Similarly, in the case of the general idiom token identification dataset, the topic model can divide samples with the same idiomatic expression into the same partition (or the same set of partitions). We initially checked for such an interaction between topics, labels, and expressions by investigating how evenly the labels and expressions are distributed across different topics. For that, we calculated the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E of the distributions of each class label and each expression. To calculate the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E of a distribution, we first calculated the normalised entropy of the distribution across different topics in each of the 10 topic models and then averaged it. The normalised entropy of a probability distribution is the entropy of the distribution normalised with the maximum possible entropy.\u003Ca class=\"xref fn\" href=\"#fn5\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Ee\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E An interesting property of normalised entropy is that the value will be in the range of 0 to 1 and it will be independent of the base of the logarithm. If the distribution is uniformly distributed then we will get the maximum normalised entropy 1 and if the distribution is highly skewed (i.e., samples with the same class label are in the same partition or samples with the same expressions are in the same partition) then we will get the minimum entropy 0. In the VNIC dataset, the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E across topics for the ‘Idiomatic usage’ label was \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline20.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"28\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline20.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.85$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E bits and for the ‘Literal usage’ label was \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline21.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"27\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline21.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.81$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E bits. In the bigram shift dataset the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E across topics for the ‘Original’ label and the ‘Inverted’ label was \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline22.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"28\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline22.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.86$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E bits. These large entropy values indicate that on average both labels in both datasets are relatively evenly distributed across topics (i.e., the topic model process was neither doing general idiom token identification nor bigram shift classification). The average \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E of distributions from different idiomatic expressions in the VNIC dataset is \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline23.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"28\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline23.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.39$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E bits. This entropy is lower than that of the label distributions. Also, we note that there are a few expressions that have a \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E lower than \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline24.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"20\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline24.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.2$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E bits and for one expression the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E is less than \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline25.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"19\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline25.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.1$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E. This suggests that for these expressions with low entropy, most of the samples are grouped into the same topic. One intuitive explanation for this is that for each expression the literal instances tend to cluster within a topic and the idiomatic instances tend to be distributed across topics. To test this intuition we calculated the \u003Cem class=\"italic\"\u003Emean normalised entropy\u003C\u002Fem\u003E of the distribution of the expressions across topics when we consider only the literal instances of the expression, this entropy was found to be \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline26.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"44\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline26.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.3583$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E, and when we only consider the idiomatic instances the entropy was \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline27.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"44\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline27.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.4098$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E. The fact that in general, the distribution of an expression’s idiomatic instances across topics has a higher entropy than the literal instances suggests that our intuition is correct.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E For each topic-aware probing (i.e., for each combination of task plus embedding) with \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline28.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline28.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E topics, we get \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline29.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline29.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E seen and \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline30.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline30.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E unseen scores (each being an average across 5 folds). We have averaged (a micro average) these seen scores and unseen scores from all topic models to calculate an aggregate seen score and unseen score. We then calculate the average difference by taking the difference between these two averages.\u003Ca class=\"xref fn\" href=\"#fn6\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Ef\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E If the average difference is positive and not a negligibly small value, this is an indication that the topic signal contributes to the performance of the probing model on the task. Our experimental design is illustrated in Figure \u003Ca class=\"xref fig\" href=\"#f2\"\u003E2\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f2\" id=\"f2\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig2.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3501\" height=\"2306\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig2.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 2.\u003C\u002Fspan\u003E Experimental design.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E The label distribution in the VNIC dataset for general idiom token identification is somewhat imbalanced. Most imbalanced datasets contain more negative samples and fewer positive samples. However, somewhat unusually in this case, there are more positive samples (idiomatic usage) than negative samples (2566 idiomatic and 550 literal usages). Savary \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref60\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Savary, Cordeiro, Lichte, Ramisch, nurrieta and Giouli\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) proposed heuristics to automatically identify sample sentences that might contain literal occurrences of MWE and this approach can be considered for balancing the general idiom token identification dataset. However, using this approach would still require manual verification and therefore in our experimentation we used the VNIC dataset as it is and selected an evaluation metric that accounts for imbalanced distributions. Accuracy is a popular and intuitive evaluation metric, but it is not suitable for an imbalanced dataset: if 99 samples out of 100 are idiomatic, a model could report high accuracy by blindly labelling every sample as positive. The F1 score and Area Under Precision Recall curve (AUC-PR) are both suitable for the standard imbalanced scenario where the positive class is the minority, this is because they both focus on the identification of positive samples. However, because they both exclusively focus on the performance of the positive class they are not suitable when the positive class is the majority. In this context, although we could use the F1 scores as our evaluation metric by treating the literal class as the positive class, doing this would essentially change the task to literal token identification and so would add an extra layer of complexity to the interpretation of the results in terms of idiom identification. An alternative is to use other metrics that are suitable for imbalanced datasets that consider both the positive and the negative classes. The most suitable evaluation metrics for an imbalanced dataset with a majority of positive instances are the AUC ROC and Mathew Correlation Coefficient (MCC). An empirical comparative study by Halimu \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref22\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Halimu, Kasem and Newaz\u003C\u002Fspan\u003E2019\u003C\u002Fa\u003E) showed that both AUC ROC and MCC are statistically consistent with each other, however, AUC ROC is more discriminating than MCC. Therefore we selected the AUC ROC as the most suitable metric for these experiments.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E For this experiment, we used the gensim\u003Ca class=\"xref fn\" href=\"#fn7\"\u003E\u003Cspan class=\"show-for-sr\"\u003EFootnote \u003C\u002Fspan\u003E\n\u003Csup class=\"sup\"\u003Eg\u003C\u002Fsup\u003E\n\u003C\u002Fa\u003E library implementation of Latent Semantic Indexing with default parameters for training the topic model. We used the bigram phrase model, tf-idf model, and data lemmatisation from gensim library to prepare the corpus for training the LSI model. For training the MLP probing model we used the scikit-learn Pedregosa \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref47\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot and Duchesnay\u003C\u002Fspan\u003E2011\u003C\u002Fa\u003E) implementation of MLP with default parameters.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec results\" data-magellan-destination=\"s5\" id=\"s5\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E5.\u003C\u002Fspan\u003E Results and discussion\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E We report results obtained from our probing experiments: micro-averaged seen and unseen AUC ROC scores, and the differences between seen and unseen scores for each representation in Table \u003Ca class=\"xref table\" href=\"#tbl3\"\u003E3\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl3\" id=\"tbl3\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 3.\u003C\u002Fspan\u003E Average seen and unseen AUC ROC scores and their differences along with standard deviations for different embeddings on the Bigram Shift Probing task and the General Idiom Token Identification task\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"666\" height=\"820\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E Focusing first on the bigram shift task, we observe very small differences in performance between seen AUC scores and unseen AUC scores using any of the representations (GloVe, BERT, or RoBERTa) with a maximum 2.55% difference, and in most of the cases less than 1% difference. This is evident in Figure \u003Ca class=\"xref fig\" href=\"#f3\"\u003E3\u003C\u002Fa\u003E where the GloVe seen and unseen scores are plotted on top of each other as are the BERT and RoBERTa seen and unseen scores across all the layers. This is an expected result (see Section \u003Ca class=\"xref sec\" href=\"#s4\"\u003E4\u003C\u002Fa\u003E) and one that we take to indicate that topic-aware probing works as expected. Furthermore, this result also indicates that the bigram shift task is not sensitive to a topic signal.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f3\" id=\"f3\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig3.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"4062\" height=\"1300\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig3.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 3.\u003C\u002Fspan\u003E Seen and Unseen AUC ROC scores from GloVe and different layers of BERT and RoBERTa on the Bigram Shift Task.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E Given that the bigram shift task does not appear to be sensitive to the topic signal it is interesting to observe that GloVe embeddings and random embeddings give the same performance on this task. If GloVe encodes non-topic signals along with topic signals then we would expect that this information would help the GloVe-based probing model to achieve better performance (as compared to a random embedding) on a non-topic sensitive task such as bigram shift. But in this case, GloVe has the same performance as the random baseline, which suggests that GloVe embeddings do not encode non-topic signals. This is in line with our assumption that GloVe primarily captures the topic signal in a text (Section \u003Ca class=\"xref sec\" href=\"#s4-2\"\u003E4.2\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E From the bigram shift results on BERT representations, we can observe that the initial layer of BERT (BERT0) and GloVe have similar performance (same as that of random embedding). Similar to our previous argument, if the initial layer of BERT encodes non-topic signals along with topic signals then we would expect that the probing model achieves a better performance (as compared to a random embedding) on a non-topic sensitive task such as bigram shift. So similar to GloVe representation, we can argue that the initial layer of BERT also primarily encodes the topic signal. But, when we look at the RoBERTa results on the bigram shift task, the initial layer achieves better performance than the GloVe baseline. This suggests that, unlike BERT, RoBERTa encodes some non-topic signals even in the initial layer. But, both BERT and RoBERTa improve their performance by using their later layers and we hypothesise that this is because both the models encode more non-topic signals in their later layers. In that case, the difference in performance between GloVe and the later layers of BERT and RoBERTa can be attributed to the encoded non-topic information that is useful to this non-topic sensitive task, for example, syntactic information. When we compare the later layer performances of BERT and RoBERTa, BERT’s performance converges with that of RoBERTa and BERT’s best seen score performance of 0.9447 AUC surpasses the best RoBERTa seen score of 0.9390 AUC. This suggests that even though BERT encodes less (or no) non-topic signal in its initial layers, compared to RoBERTa it encodes more non-topic signal in its later layers. Note that the drop in performance in the final layers of BERT and RoBERTa for the bigram shift probing task is likely due to the fine-tuning of the embeddings in these layers to the specific tasks that BERT and RoBERTa was trained on, namely: masked language modelling (in the case of both BERT and RoBERTa) and next-sentence prediction (in the case of BERT). This pattern of performance drop is also reported in other layer-wise studies of BERT (see e.g., Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref29\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E)). We also observe a similar drop in performance in the last layers of BERT and RoBERTa for the general idiom token identification task and attribute the same root cause to it for that task.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E The results from the bigram shift task also suggest two methods for assessing the sensitivity of a probing task to topic information. The first method is to consider the difference between a probe’s performance on a task when it is trained on random embeddings versus GloVe embeddings. Tasks which have the same performance on random embeddings and GloVe embeddings are likely insensitive to the topic information (such as is the case with the bigram shift task). A corollary to this is that tasks for which there is a large difference in performance between GloVe and random embeddings are likely to be sensitive to information relating to topic. The second method to measure a task’s sensitivity to the topic information is to use the difference between seen and unseen topic scores. For example, on the bigram shift task, the difference between seen and unseen topic scores is negligible for GloVe, and also for each of the layers of BERT and RoBERTa. Later in the paper, we will use both of these methods (GloVe \u003Cem class=\"italic\"\u003Evs.\u003C\u002Fem\u003E random, and seen \u003Cem class=\"italic\"\u003Evs.\u003C\u002Fem\u003E unseen topic scores) to assess task sensitivity to the topic information, and we will show across a range of probing tasks that these two methods are highly correlated.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Switching focus to the results for the general idiom token identification task listed in Table \u003Ca class=\"xref table\" href=\"#tbl3\"\u003E3\u003C\u002Fa\u003E there are very large (as compared with the bigram shift task) performance differences between average seen and unseen topic scores on GloVe embeddings with 11.77% difference (as compared to \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline31.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"37\" height=\"13\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline31.png\" data-zoomable=\"false\"\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$\\lt 1\\%$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E difference on GloVe for bigram shift) and all 12 layers of BERT and RoBERTa embeddings with all differences in the range of 8.81% to 13.34% (as compared with a maximum difference of 2.55% for the bigram shift task). This difference in performance between seen and unseen topics is also apparent in Figure \u003Ca class=\"xref fig\" href=\"#f4\"\u003E4\u003C\u002Fa\u003E. This difference indicates the importance of the topic signal to the task of general idiom token identification. We also observed a large standard deviation for seen and unseen scores including for random embedding and we believe that this is an artefact of the relatively small dataset (2,566 sentences) used for the general idiom token identification task experiments.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f4\" id=\"f4\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig4.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"4064\" height=\"1302\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig4.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 4.\u003C\u002Fspan\u003E Seen and Unseen AUC ROC scores from different layers of BERT and RoBERTa with GloVe baseline on General Idiom Token Identification Task.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E The fact that the task of general idiom token identification is sensitive to topic signal is also evidenced by the fact that GloVe performs much better than random on both seen topic and unseen topic samples. Furthermore, the difference in performance for GloVe embeddings between seen topic and unseen topic samples also reflects the ability of GloVe to encode the topic information. Also, from the results for the bigram shift task, we noted that GloVe had a similar performance to BERT0. We see a similar pattern of results here, with GloVe and BERT0 embeddings resulting in a similar performance for both the seen and unseen probing conditions. This reinforces our earlier observation that BERT0 may be primarily encoding the topic information but also suggests that GloVe and BERT generally have a similar capacity to encode the topic signal (with BERT being slightly better at capturing this signal). In the case of RoBERTa, we observed that the initial layer encodes some non-topic signal in the bigram shift task results. But, from the results for general idiom identification on RoBERTa, we can see that the initial layer performance is similar to BERT’s initial layer and GloVe. This means that, even though the first layer of RoBERTa encodes some non-topic signal, that non-topical information is not that useful for the idiom identification task. Also, as with the bigram shift task, we can observe on the general idiom token identification task an improvement in the performance of BERT and RoBERTa embeddings as we move into deeper layers, with the best BERT performance being BERT7 and the best RoBERTa performance being RoBERTa9 (on both seen and unseen conditions). This suggests that similar to our observation on the bigram shift task, the improvement in performance observed in BERT’s and RoBERTa’s deeper layers is attributable to non-topic-based information encoded in BERT and RoBERTa. This hypothesis is reinforced by the fact that the improvement in performance across BERT and RoBERTa layers is similar across both the seen and unseen conditions (i.e., an improvement in one layer for the seen is matched by a similar improvement for the same layer for the unseen condition). Indeed, Figure \u003Ca class=\"xref fig\" href=\"#f5\"\u003E5\u003C\u002Fa\u003E plots the difference in BERT’s and RoBERTa’a performance between seen and unseen conditions across the different layers and highlights that this difference is relatively stable and similar to the difference between GloVe for seen and unseen. It is also important to note that the difference between seen score and unseen score of all RoBERTa layers is consistently greater than that of the corresponding layers of BERT. This suggests that even though RoBERTa achieves a better performance than BERT on general idiom token identification it is more sensitive to the topic information.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f5\" id=\"f5\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig5.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3126\" height=\"2181\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig5.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 5.\u003C\u002Fspan\u003E Difference between seen scores and unseen scores from different layers of BERT and RoBERTa on General Idiom Token Identification Task.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E To summarise, our results suggest that (a) the bigram shift task is not sensitive to the topic information whereas the general idiom token identification task is; (b) the GloVe embeddings primarily encode topic information; (c) that initial layer embeddings of BERT (not RoBERTa) behave similarly to GloVe embeddings; (d) that later layers of BERT and RoBERTa encode non-topic information that is useful for both bigram shift and general idiom token identification (we hypothesise that this information may be syntactic in nature); (e) that both BERT and RoBERTa are sensitive to the topic information on general idiom identification.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec other\" data-magellan-destination=\"s6\" id=\"s6\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E6.\u003C\u002Fspan\u003E Other probing tasks\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E Broadening the focus beyond predicting idiomatic usage, we have analysed the relationship between topic, task, and the performance of probing models trained on transformer-generated embeddings across a range of standard probing tasks. We selected 8 probing tasks introduced by Conneau \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref8\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Conneau, Kruszewski, Lample, Barrault and Baroni\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E) and the explanation and summary statistics of the dataset of each of these are shown in Table \u003Ca class=\"xref table\" href=\"#tbl4\"\u003E4\u003C\u002Fa\u003E. We did a topic-aware probing on each of these 8 probing tasks and the number of tail topics obtained from different topic models on different datasets are shown in Table \u003Ca class=\"xref table\" href=\"#tbl5\"\u003E5\u003C\u002Fa\u003E. Figures \u003Ca class=\"xref fig\" href=\"#f6\"\u003E6\u003C\u002Fa\u003E and \u003Ca class=\"xref fig\" href=\"#f7\"\u003E7\u003C\u002Fa\u003E show for the 8 probing tasks the performance of GloVe and the different layers of BERT and RoBERTa on each of the tasks. Looking at Figures \u003Ca class=\"xref fig\" href=\"#f6\"\u003E6\u003C\u002Fa\u003E and \u003Ca class=\"xref fig\" href=\"#f7\"\u003E7\u003C\u002Fa\u003E, a number of general observations can be made. First for nearly all the tasks—with the exception of Sentence Length, and Object Number (to a lesser degree)—the performance of the initial layer of BERT and GloVe is very similar in both the unseen and seen conditions. It is worth noting that in this set of probing tasks, the Sentence Length dataset is distinctive because the sentences for this task are not controlled for sentence length whereas sentences used for all the other tasks have a similar length. As pointed out by Adi \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref2\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Adi, Kermany, Belinkov, Lavi and Goldberg\u003C\u002Fspan\u003E2017\u003C\u002Fa\u003E), sentence length can have a significant impact on the norm of the sentence embedding. Norms of sentence embeddings from Sentence Length datasets generated by averaging the embeddings of each word in the sentence can have a higher variance compared to other datasets, and this variance may be the reason for the different behaviour observed for the Sentence Length probing task. Deviating from this pattern, the initial layer of RoBERTa achieves better performance than GloVe and the initial layer of BERT on a number of probing tasks. But for some tasks like SOMO, Coordination Inversion, and Past-Present up to some extent (and for general idiom identification task in Section \u003Ca class=\"xref sec\" href=\"#s5\"\u003E5\u003C\u002Fa\u003E), the initial layer of RoBERTa shows similar performance to that of GloVe and the initial layer of BERT. Second, in all tasks for GloVe, BERT, and RoBERTa, the performance in the seen condition is better than the unseen condition. Third, the difference in performance between the seen and unseen conditions remains relatively stable across all the layers of BERT and RoBERTa.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl4\" id=\"tbl4\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 4.\u003C\u002Fspan\u003E Descriptions and summary statistics of the datasets for the VNIC, Bigram shift, and 8 other probing tasks\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"666\" height=\"451\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl5\" id=\"tbl5\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 5.\u003C\u002Fspan\u003E Number of tail topics from 10 topic models on datasets of other 8 probing tasks\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"666\" height=\"260\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f6\" id=\"f6\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig6.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3367\" height=\"5625\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig6.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 6.\u003C\u002Fspan\u003E Seen and Unseen AUC ROC scores from different layers of BERT with GloVe baseline on Probing Tasks.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f7\" id=\"f7\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig7.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3373\" height=\"5625\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig7.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 7.\u003C\u002Fspan\u003E Seen and Unseen AUC ROC scores from different layers of RoBERTa with GloVe baseline on Probing Tasks.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E For each task, we list the average seen score obtained from random embeddings, the average GloVe seen and unseen scores along with their difference, and the average seen and unseen scores and their difference from the best BERT layer and the best RoBERTa layer in Table \u003Ca class=\"xref table\" href=\"#tbl6\"\u003E6\u003C\u002Fa\u003E. The best BERT layer and RoBERTa layer are the layers which obtain the best average seen AUC ROC scores, and in most of the tasks (except a very small difference in Subject Number and Tree Depth for BERT and Past-Present, Tree Depth, and Object Number for RoBERTa), the best BERT layer and RoBERTa layer also gives the best average unseen AUC ROC score. For all these exceptional cases, the difference between the best unseen score and the unseen score from the best layer is negligibly small (0.0003 for Subject Number and 0.0024 for Tree Depth on BERT best layers; 0.0019 for Tree Depth, 0.0014 for Past-Present, and 0.0041 for Object Number on RoBERTa best layers).\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cdiv class=\"table-wrap\" data-magellan-destination=\"tbl6\" id=\"tbl6\"\u003E\n\n\u003Cdiv class=\"caption\"\u003E\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003ETable 6.\u003C\u002Fspan\u003E Average seen and unseen AUC ROC scores and their differences for GloVe and best BERT and RoBERTa layer embeddings on different probing tasks—tasks are ranked in the descending order of the difference between GloVe Seen score and GloVe Unseen score\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cspan\u003E\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"666\" height=\"372\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fdiv\u003E\n\n\u003Cp class=\"p\"\u003E Earlier we proposed two metrics derived from the topic-aware probing methodology for measuring the sensitivity of a probing task to the topic signal. The first metric is the difference between seen and unseen scores from the topic-aware probe, the larger the difference the more sensitive the task to the topic signal. The second method is based on the premise that GloVe embeddings primarily encode topic information, and so the difference between the seen scores on a task obtained from GloVe embeddings and from random embeddings can be considered as another measure of the sensitivity of a task to topic information. If both these measures are true indications of the topic sensitivity of tasks, then there should be a high correlation between them across tasks. We used the set of selected probing tasks to verify whether these measures are correlated or not. From the scores reported in Table \u003Ca class=\"xref table\" href=\"#tbl6\"\u003E6\u003C\u002Fa\u003E, we calculated the correlation between these two measures of topic sensitivity from all probing tasks, that is (a) the difference between GloVe seen scores and Random seen scores (b) the difference between GloVe seen scores and GloVe unseen scores. We found a very high correlation coefficient, 0.80, between these measures which indicates that both these measures of task sensitivity to the topic information are consistent with each other.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f8\" id=\"f8\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig8.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3251\" height=\"2142\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig8.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 8.\u003C\u002Fspan\u003E GloVe Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task (Note that scores of SOMO and CI are very similar and therefore both of them are overlapping in the plot).\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Csection\u003E\u003Cdiv class=\"fig\" data-magellan-destination=\"f9\" id=\"f9\"\u003E\n\n\n\u003Cdiv class=\"figure-thumb\"\u003E\u003Cimg src=\"data:image\u002Fgif;base64,R0lGODlhAQABAIAAAMLCwgAAACH5BAAAAAAALAAAAAABAAEAAAICRAEAOw==\" data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig9.png?pub-status=live\" class=\"aop-lazy-load-image\" width=\"3249\" height=\"2589\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_fig9.png\" data-zoomable=\"true\"\u003E\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"caption\"\u003E\u003Cp class=\"p\"\u003E \n\u003C\u002Fp\u003E\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003EFigure 9.\u003C\u002Fspan\u003E BERT and RoBERTa Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fsection\u003E\n\n\u003Cp class=\"p\"\u003E By taking the difference between GloVe seen performance and GloVe unseen performance as the measure of topic sensitivity, we generated two scatter plots to check how the performance of GloVe and Transformer-based neural language models (BERT and RoBERTa) varies across different tasks with different topic sensitivity. Figure \u003Ca class=\"xref fig\" href=\"#f8\"\u003E8\u003C\u002Fa\u003E plots for each task the GloVe seen performance versus the difference between GloVe seen and unseen and Figure \u003Ca class=\"xref fig\" href=\"#f9\"\u003E9\u003C\u002Fa\u003E plots for each task the best performance by BERT and RoBERTa (for any layer) versus the difference between GloVe seen and unseen. In these scatter plots, we can see a general trend that as sensitivity to topic increases (i.e., the difference between GloVe seen and unseen gets larger and we move to the right on the x-axis) there is a tendency for the performance of GloVe and Transformer-based neural language models (BERT and RoBERTa) to increase. This suggests that the less sensitive a probing task is to topic information the more difficult the task is for Transformer-based neural language models (BERT and RoBERTa). We also calculated the correlation coefficients between performances of neural language models (seen scores) and topic sensitivity scores (difference between GloVe seen and unseen score), and we found a coefficient of 0.2829 for BERT and 0.3345 for RoBERTa. This suggests that neural language models rely on topical information to solve different tasks.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E When we compare the BERT and RoBERTa neural language models, we observe slightly more topic reliance for RoBERTa than BERT. This difference is evident in Figure \u003Ca class=\"xref fig\" href=\"#f9\"\u003E9\u003C\u002Fa\u003E (steeper trend line for RoBERTa) and in a slightly higher value of correlation coefficient for RoBERTa than BERT (0.3345 \u003Cem class=\"italic\"\u003Evs.\u003C\u002Fem\u003E 0.2829). Our results indicate that newer and generally better neural language models, such as RoBERTa, are more reliant on topic information as compared with BERT. We will return to this point in our conclusions.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"sec conclusions\" data-magellan-destination=\"s7\" id=\"s7\"\u003E\n\n\u003Ch2 class=\"A\"\u003E\u003Cspan class=\"label\"\u003E7.\u003C\u002Fspan\u003E Conclusions\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E We proposed a topic-aware probing method to measure the role of the topic signal in distributed representations and validated this method using baseline representations (GloVe and random) and a baseline task (bigram shift). The results of our analysis on the bigram shift probing task supported our hypothesis that GloVe embeddings primarily encode topic information and furthermore suggested that the initial layer of BERT also primarily encodes topic information and that later layers of BERT, and all layers of RoBERTa encode non-topic information (the observation that this information was non-topic related is based on the fact that this information was useful for the non-topic sensitive probing task of bigram shift).\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E With respect to the task of general idiom token identification, previous research has pointed to the importance of topic information for identifying idiomatic usage (Feldman \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref16\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Feldman and Peng\u003C\u002Fspan\u003E2013\u003C\u002Fa\u003E; Peng \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref48\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Peng, Feldman and Vylomova\u003C\u002Fspan\u003E2014\u003C\u002Fa\u003E). However, work such as by Fazly \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref15\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Fazly, Cook and Stevenson\u003C\u002Fspan\u003E2009\u003C\u002Fa\u003E) has highlighted the importance of non-topic information for idiom token identification and the results of Salton \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref59\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Salton, Ross and Kelleher\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) suggested that by using distributed embeddings it is possible to create a general idiom token identification model (i.e., a model that works across multiple expressions within a category) without requiring large amounts of topic information. The results of our topic-aware probing experiments confirm the importance of the topic signal to the task of general idiom token identification. One potential reason for the topic signal being important for general idiomatic token identification systems is that distinctions between topics may align with seen versus unseen expressions. Our analysis of the distribution of expressions across topics (see Section \u003Ca class=\"xref sec\" href=\"#s4-3\"\u003E4.3\u003C\u002Fa\u003E) suggested that for many expressions, the sample sentences containing the expression tend to cluster within a topic, particularly the literal uses of an expression. Consequently, training a probe on one topic and testing it on other topics is similar to training on one set of expressions and testing on another set of expressions. Indeed, Nedumpozhimana \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref45\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana, Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) report an expression-based analysis of idiom token identification that includes a seen versus unseen experiment. The results of that experiment also reported a drop in performance, in this case from seen expressions to unseen expressions rather than seen versus unseen topics. The fact that the distinction between seen and unseen topics and seen and unseen expressions overlaps is not surprising. However, the overlap does not mean that the phenomena are identical. For example, whereas the expression-based distinction is solely based on the words within an expression, the definition of topic in our experimentation considers all words in a sentence. The importance of considering both the information within an expression and the surrounding context for idiom token identification has been demonstrated by Nedumpozhimana and Kelleher (\u003Ca class=\"xref bibr\" href=\"#ref44\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana and Kelleher\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E) who show that BERT not only relies on information within the expression but also in the surrounding context.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Switching to our primary question of the extent to which Transformer-based language models rely on word order\u002Fsyntactic information versus word co-occurrence\u002Ftopic information, the results of our topic-aware probing experiments suggest that BERT and RoBERTa encode both topic and non-topic information. This is indicated by the fact that across all the probing tasks the embeddings generated by BERT and RoBERTa’s middle layers result in a higher performance than GloVe, see Figure \u003Ca class=\"xref fig\" href=\"#f6\"\u003E6\u003C\u002Fa\u003E. These results are in line with various layer-wise studies on BERT in the literature, such as by Jawahar \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref29\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Jawahar, Sagot and Seddah\u003C\u002Fspan\u003E2019b\u003C\u002Fa\u003E), which suggest that the syntactic features of a sentence are encoded in the middle layers of BERT. However, despite the fact that BERT can capture useful non-topic information our analysis suggests that in general BERT (and RoBERTa) primarily rely on the topic information. Furthermore, our analysis of BERT and RoBERTa’s performance across a set of standard probing tasks suggests that tasks that are relatively insensitive to the topic information are also tasks that are relatively difficult for BERT and RoBERTa. These observations agree with the findings of Pham \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref50\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pham, Bui, Mai and Nguyen\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E), in which they observe that most of the BERT-based models behave similarly to bag-of-word models on GLUE tasks. They also agree with Sinha \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref63\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Sinha, Jia, Hupkes, Pineau, Williams and Kiela\u003C\u002Fspan\u003E2021a\u003C\u002Fa\u003E) who argue that the success of pre-trained language models on many tasks is primarily based on their ability to encode distributional information.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E From our experiments, we observed that the RoBERTa model is more reliant on the topic information than the BERT model. One of the most notable differences between BERT and RoBERTa is in the pretraining objectives used for the two models. BERT is trained on the masked language model and the next-sentence prediction objectives, whereas the RoBERTa model excludes the next-sentence prediction objective. Mickus \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref42\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Mickus, Paperno, Constant and van Deemter\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) argue that the use of a next-sentence prediction objective adulterates the distributional nature of the semantics learned by BERT. Building on this argument, the removal of the next-sentence prediction from the pretraining objective for RoBERTa may result in the RoBERTa model being more focused towards distributional semantics, and this may explain the relatively stronger reliance of RoBERTa on topic compared to BERT.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E The broader implications of our findings for NLP are that the performance of Transformer-based systems on NLP tasks can be improved by incorporating more word order or syntactic information into these language models. Wang \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref71\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Wang, Bi, Yan, Wu, Xia, Bao, Peng and Si\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E) is an example of recent work that attempts to do this. They show that adding word-order and sentence-order learning objectives into BERT pretraining could lead to improved performance on language processing tasks. Also, Pham \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref50\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Pham, Bui, Mai and Nguyen\u003C\u002Fspan\u003E2021\u003C\u002Fa\u003E) show that fine-tuning on a word-order-sensitive task prior to fine-tuning on a downstream task increases a language model’s sensitivity to word order on the downstream task. Another potential way forward in this direction is to explicitly integrate syntactic information into the language modelling architecture, for example by using methods like Recursive Neural Network in which the representation of a sentence is composed of representations of words by applying compositions recursively through the parsed tree (Socher \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref65\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Socher, Perelygin, Wu, Chuang, Manning, Ng and Potts\u003C\u002Fspan\u003E2013\u003C\u002Fa\u003E). Such approaches utilise compositional semantics which can be helpful to capture more non-distributional semantics in language models.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E There are a number of limitations of our analysis that should be noted. The first is that the analysis is based on two neural language models BERT and RoBERTa, both are based on Transformer encoder architecture. Recent neural language models like GPT, Llama, etc. are based on Transformer decoder architecture. It is natural to ask whether we can extend our study to neural language models with Transformer decoder architecture and whether the observations from our experiment can be applicable to those models. Also, our analyses are based only on English datasets which have relatively fixed word order. It would be interesting to examine whether the observations we have drawn from our experiments are limited to the English language or whether the reliance of Transformer-based language models on topic as strong for other languages that have a relatively more flexible word order and richer morphology. In future work, we will extend our experiments by using different neural language models based on the Transformer decoder architecture and different datasets like PARSEME (Savary \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref61\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Savary, Ramisch, Cordeiro, Sangati, Vincze, QasemiZadeh, Candito, Cap, Giouli, Stoyanova and Doucet\u003C\u002Fspan\u003E2017\u003C\u002Fa\u003E; Ramisch \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E \u003Ca class=\"xref bibr\" href=\"#ref53\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ramisch, Cordeiro, Savary, Vincze, Barbu Mititelu, Bhatia, Buljan, Candito, Gantar, Giouli, Güngör, Hawwari, Iñurrieta, Kovalevskaitė, Krek, Lichte, Liebeskind, Monti, Parra Escartín, QasemiZadeh, Ramisch, Schneider, Stoyanova, Vaidya and Walsh\u003C\u002Fspan\u003E2018\u003C\u002Fa\u003E, \u003Ca class=\"xref bibr\" href=\"#ref54\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Ramisch, Savary, Guillaume, Waszczuk, Candito, Vaidya, Barbu Mititelu, Bhatia, Iñurrieta, Giouli, Güngör, Jiang, Lichte, Liebeskind, Monti, Ramisch, Stymne, Walsh and Xu\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E), MAGPIE (Haagsma, Bos, and Nissim \u003Ca class=\"xref bibr\" href=\"#ref21\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Haagsma, Bos and Nissim\u003C\u002Fspan\u003E2020\u003C\u002Fa\u003E), etc., which contains both English and Non-English data to address these questions.\u003C\u002Fp\u003E\n\u003Cp class=\"p\"\u003E Our findings regarding the topic reliance of neural language models have implications for NLP tasks such as machine translation and question answering. For example, the findings of Amponsah-Kaakyire \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref3\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Amponsah-Kaakyire, Pylypenko, Genabith and España-Bonet\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) indicate that part of BERT’s performance on the task of identifying translationese is due to topic differences learned by BERT. So, another direction for future work is to use topic-aware probing to investigate the topic reliance of neural language models on identifying translationese.\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"back\"\u003E\n\u003Cdiv class=\"sec coi-statement\" data-magellan-destination=\"s50\" id=\"s50\"\u003E\n\u003Ch2 class=\"A\"\u003E Competing interests\u003C\u002Fh2\u003E\n\u003Cp class=\"p\"\u003E The authors declare none\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\n\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E",tableOfContent:[{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003EIntroduction\u003C\u002Fdiv\u003E",url:"s1"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003ERelated work\u003C\u002Fdiv\u003E",url:"s2"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003ETopic-aware probing\u003C\u002Fdiv\u003E",url:"s3"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003EExperimentation\u003C\u002Fdiv\u003E",url:"s4"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003EResults and discussion\u003C\u002Fdiv\u003E",url:"s5"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003EOther probing tasks\u003C\u002Fdiv\u003E",url:"s6"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003EConclusions\u003C\u002Fdiv\u003E",url:"s7"},{level:k,current:b,title:"\u003Cdiv class=\"toc-title\"\u003ECompeting interests\u003C\u002Fdiv\u003E",url:"s50"}],footnotes:[],fulltextNotes:[{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Ea\u003C\u002Fspan\u003E Note that in previous work, Nedumpozhimana \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref45\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Nedumpozhimana, Klubička and Kelleher\u003C\u002Fspan\u003E2022\u003C\u002Fa\u003E) have demonstrated that it is possible to train a model to generalise from a set of known (trained on) idiomatic expressions within a category to unknown (unseen during training) idiomatic expressions from the same category.\u003C\u002Fp\u003E\n",targetId:"fn1",displayNumber:E},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Eb\u003C\u002Fspan\u003E Our code is available at \u003Ca class=\"uri\" href=\"https:\u002F\u002Fgithub.com\u002Fvasudev2020\u002FBERTAnalysis\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fvasudev2020\u002FBERTAnalysis\u003C\u002Fa\u003E\n\u003C\u002Fp\u003E\n",targetId:"fn2",displayNumber:"2"},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Ec\u003C\u002Fspan\u003E \n\u003Ca class=\"uri\" href=\"https:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002FSentEval\u002Ftree\u002Fmaster\u002Fdata\u002Fprobing\"\u003Ehttps:\u002F\u002Fgithub.com\u002Ffacebookresearch\u002FSentEval\u002Ftree\u002Fmaster\u002Fdata\u002Fprobing\u003C\u002Fa\u003E\n\u003C\u002Fp\u003E\n",targetId:"fn3",displayNumber:"3"},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Ed\u003C\u002Fspan\u003E There are multiple samples for each of the 53 idiomatic expressions in the general idiom token identification dataset—some marked as ‘Idiomatic usage’, others as ‘Literal usage’—and these samples may be distributed across multiple topics. Consequently, some topics may include both ‘Idiomatic usage’ and ‘Literal usage’ samples for some expressions, or only samples of one type for an expression, or no samples for a given expression. Consequently, the seen topic versus unseen topic distinction is different to the seen expression versus unseen expression distinction examined by Salton \u003Cem class=\"italic\"\u003Eet al.\u003C\u002Fem\u003E (\u003Ca class=\"xref bibr\" href=\"#ref59\"\u003E\u003Cspan class=\"show-for-sr\"\u003EReference Salton, Ross and Kelleher\u003C\u002Fspan\u003E2016\u003C\u002Fa\u003E) (which is not our focus in this paper).\u003C\u002Fp\u003E\n",targetId:"fn4",displayNumber:"4"},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Ee\u003C\u002Fspan\u003E The normalised entropy of a label (or an expression) distribution \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline11.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"10\" height=\"14\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline11.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$p$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E across a set of \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline12.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"9\" height=\"8\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline12.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$n$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E topics is \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline13.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"139\" height=\"29\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline13.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$ -\\sum _{i=1}^{n} \\frac{p(x_i)log_b(p(x_i))}{log_b(n)}$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E. For example the normalised entropy of distribution: \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline14.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"48\" height=\"17\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline14.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$(1,0,0)$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E will be \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline15.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"8\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline15.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E, \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline16.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"56\" height=\"22\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline16.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$\\big(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\big)$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E will be \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline17.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"6\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline17.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$1$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E, and \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline18.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"53\" height=\"22\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline18.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$\\big(\\frac{1}{2},\\frac{1}{2},0\\big)$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E will be \u003Cspan data-mathjax-status=\"alt-graphic\" class=\"inline-formula\"\u003E\n\u003Cspan class=\"alternatives\"\u003E\n\u003Cimg data-src=\"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline19.png?pub-status=live\" class=\"aop-lazy-load-image mathjax-alternative mathjax-alt-graphic mathjax-off\" width=\"28\" height=\"12\" data-original-image=\"\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_inline19.png\" data-zoomable=\"false\" \u002F\u003E\n\u003Cspan class=\"mathjax-tex-wrapper\" data-mathjax-type=\"texmath\"\u003E\u003Cspan class=\"tex-math mathjax-tex-math mathjax-on\"\u003E\n$0.63$\n\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fspan\u003E\n\u003C\u002Fp\u003E\n",targetId:"fn5",displayNumber:v},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Ef\u003C\u002Fspan\u003E Note that the difference between the averages is the same as the average of the differences.\u003C\u002Fp\u003E\n",targetId:"fn6",displayNumber:"6"},{content:"\n\n\u003Cp class=\"p\"\u003E\u003Cspan class=\"label\"\u003Eg\u003C\u002Fspan\u003E \n\u003Ca class=\"uri\" href=\"https:\u002F\u002Fradimrehurek.com\u002Fgensim\u002F\"\u003Ehttps:\u002F\u002Fradimrehurek.com\u002Fgensim\u002F\u003C\u002Fa\u003E\n\u003C\u002Fp\u003E\n",targetId:"fn7",displayNumber:"7"}],references:[{id:"ref1",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EAbdou\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERavishankar\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKulmizev\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESøgaard\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). Word order does matter and shuffled language models know it. In \u003Cem class=\"italic\"\u003EProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EDublin, Ireland\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E6907\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E6919\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.acl-long.476\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Abdou,+M.,+Ravishankar,+V.,+Kulmizev,+A.+and+Søgaard,+A.+(2022).+Word+order+does+matter+and+shuffled+language+models+know+it.+In+Proceedings+of+the+60th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Dublin,+Ireland:+Association+for+Computational+Linguistics,+pp.+6907–6919.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Abdou,+M.,+Ravishankar,+V.,+Kulmizev,+A.+and+Søgaard,+A.+(2022).+Word+order+does+matter+and+shuffled+language+models+know+it.+In+Proceedings+of+the+60th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Dublin,+Ireland:+Association+for+Computational+Linguistics,+pp.+6907–6919.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r1",title:"Abdou, M., Ravishankar, V., Kulmizev, A. and Søgaard, A. (2022). Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland: Association for Computational Linguistics, pp. 6907–6919.",doi:"10.18653\u002Fv1\u002F2022.acl-long.476",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.acl-long.476",pubMedLink:a}]},{id:"ref2",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EAdi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKermany\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBelinkov\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELavi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGoldberg\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2017\u003C\u002Fspan\u003E). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, \u003Cem class=\"italic\"\u003E5th International Conference on Learning Representations, ICLR. 2017\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EToulon, France\u003C\u002Fspan\u003E: Conference Track Proceedings, OpenReview.net.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Adi, Y., Kermany, E., Belinkov, Y., Lavi, O. and Goldberg, Y. (2017). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 5th International Conference on Learning Representations, ICLR. 2017, Toulon, France: Conference Track Proceedings, OpenReview.net.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Adi,+Y.,+Kermany,+E.,+Belinkov,+Y.,+Lavi,+O.+and+Goldberg,+Y.+(2017).+Fine-grained+analysis+of+sentence+embeddings+using+auxiliary+prediction+tasks,+5th+International+Conference+on+Learning+Representations,+ICLR.+2017,+Toulon,+France:+Conference+Track+Proceedings,+OpenReview.net.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Adi,+Y.,+Kermany,+E.,+Belinkov,+Y.,+Lavi,+O.+and+Goldberg,+Y.+(2017).+Fine-grained+analysis+of+sentence+embeddings+using+auxiliary+prediction+tasks,+5th+International+Conference+on+Learning+Representations,+ICLR.+2017,+Toulon,+France:+Conference+Track+Proceedings,+OpenReview.net.",openUrlParams:{genre:g,date:w,sid:e,title:d},innerRefId:"r2",title:"Adi, Y., Kermany, E., Belinkov, Y., Lavi, O. and Goldberg, Y. (2017). Fine-grained analysis of sentence embeddings using auxiliary prediction tasks, 5th International Conference on Learning Representations, ICLR. 2017, Toulon, France: Conference Track Proceedings, OpenReview.net.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref3",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EAmponsah-Kaakyire\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPylypenko\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGenabith\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EEspaña-Bonet\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). Explaining translationese: why are neural classifiers better and what do they learn? In \u003Cem class=\"italic\"\u003EProceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EAbu Dhabi, United Arab Emirates (Hybrid)\u003C\u002Fspan\u003E: Association for Computational Linguistics, \u003Cspan class=\"fpage\"\u003E281\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E296\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.blackboxnlp-1.23\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Amponsah-Kaakyire,+K.,+Pylypenko,+D.,+Genabith,+J.+and+España-Bonet,+C.+(2022).+Explaining+translationese:+why+are+neural+classifiers+better+and+what+do+they+learn?+In+Proceedings+of+the+Fifth+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Abu+Dhabi,+United+Arab+Emirates+(Hybrid):+Association+for+Computational+Linguistics,+281–296.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Amponsah-Kaakyire,+K.,+Pylypenko,+D.,+Genabith,+J.+and+España-Bonet,+C.+(2022).+Explaining+translationese:+why+are+neural+classifiers+better+and+what+do+they+learn?+In+Proceedings+of+the+Fifth+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Abu+Dhabi,+United+Arab+Emirates+(Hybrid):+Association+for+Computational+Linguistics,+281–296.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r3",title:"Amponsah-Kaakyire, K., Pylypenko, D., Genabith, J. and España-Bonet, C. (2022). Explaining translationese: why are neural classifiers better and what do they learn? In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Abu Dhabi, United Arab Emirates (Hybrid): Association for Computational Linguistics, 281–296.",doi:"10.18653\u002Fv1\u002F2022.blackboxnlp-1.23",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.blackboxnlp-1.23",pubMedLink:a}]},{id:"ref4",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EArps\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESamih\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKallmeyer\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESajjad\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EH.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003EProbing for constituency structure in neural language models\u003C\u002Fspan\u003E. In Goldberg Y., Kozareva Z. and Zhang Y., (eds), \u003Cspan class=\"source\"\u003EFindings of the Association for Computational Linguistics: EMNLP.\u003C\u002Fspan\u003E Abu Dhabi, United Arab Emirates. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E6738\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E6757\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Findings of the Association for Computational Linguistics: EMNLP.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.findings-emnlp.502\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Findings of the Association for Computational Linguistics: EMNLP.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP.&author=Arps+D.&author=Samih+Y.&author=Kallmeyer+L.&author=Sajjad+H.&publication+year=2022&pages=6738-6757\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics%3A+EMNLP.&author=Arps+D.&author=Samih+Y.&author=Kallmeyer+L.&author=Sajjad+H.&publication+year=2022&pages=6738-6757",openUrlParams:{genre:n,btitle:x,title:x,atitle:"Probing for constituency structure in neural language models",aulast:a,aufirst:a,au:a,pub:d,date:l,spage:"6738",epage:"6757",doi:K,sid:e},innerRefId:"r4",title:x,doi:K,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.findings-emnlp.502",pubMedLink:a}]},{id:"ref5",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBalasubramanian\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJain\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJindal\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EAwasthi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESarawagi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). What’s in a name? are BERT named entity representations just as good for any other name? In \u003Cem class=\"italic\"\u003EProceedings of the 5th Workshop on Representation Learning for NLP\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E205\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E214\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Balasubramanian, S., Jain, N., Jindal, G., Awasthi, A. and Sarawagi, S. (2020). What’s in a name? are BERT named entity representations just as good for any other name? In Proceedings of the 5th Workshop on Representation Learning for NLP, Online. Association for Computational Linguistics, pp. 205–214.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Balasubramanian,+S.,+Jain,+N.,+Jindal,+G.,+Awasthi,+A.+and+Sarawagi,+S.+(2020).+What’s+in+a+name?+are+BERT+named+entity+representations+just+as+good+for+any+other+name?+In+Proceedings+of+the+5th+Workshop+on+Representation+Learning+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+205–214.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Balasubramanian,+S.,+Jain,+N.,+Jindal,+G.,+Awasthi,+A.+and+Sarawagi,+S.+(2020).+What’s+in+a+name?+are+BERT+named+entity+representations+just+as+good+for+any+other+name?+In+Proceedings+of+the+5th+Workshop+on+Representation+Learning+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+205–214.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r5",title:"Balasubramanian, S., Jain, N., Jindal, G., Awasthi, A. and Sarawagi, S. (2020). What’s in a name? are BERT named entity representations just as good for any other name? In Proceedings of the 5th Workshop on Representation Learning for NLP, Online. Association for Computational Linguistics, pp. 205–214.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref6",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EZ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGao\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EQ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, \u003Cspan class=\"fpage\"\u003E10509\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E10517\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.' href=https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v36i10.21294\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Chen,+Z.+and+Gao,+Q.+(2022).+Probing+linguistic+information+for+logical+inference+in+pre-trained+language+models.+Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+36,+10509–10517.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Chen,+Z.+and+Gao,+Q.+(2022).+Probing+linguistic+information+for+logical+inference+in+pre-trained+language+models.+Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence+36,+10509–10517.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r6",title:"Chen, Z. and Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10509–10517.",doi:"10.1609\u002Faaai.v36i10.21294",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v36i10.21294",pubMedLink:a}]},{id:"ref7",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EClark\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKhandelwal\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EU.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELevy\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). What does BERT look at? an analysis of BERT’s attention. In \u003Cem class=\"italic\"\u003EProceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EFlorence, Italy\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E276\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E286\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW19-4828\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Clark,+K.,+Khandelwal,+U.,+Levy,+O.+and+Manning,+C.+D.+(2019).+What+does+BERT+look+at?+an+analysis+of+BERT’s+attention.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+276–286.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Clark,+K.,+Khandelwal,+U.,+Levy,+O.+and+Manning,+C.+D.+(2019).+What+does+BERT+look+at?+an+analysis+of+BERT’s+attention.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+276–286.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r7",title:"Clark, K., Khandelwal, U., Levy, O. and Manning, C. D. (2019). What does BERT look at? an analysis of BERT’s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 276–286.",doi:"10.18653\u002Fv1\u002FW19-4828",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW19-4828",pubMedLink:a}]},{id:"ref8",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EConneau\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKruszewski\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELample\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBarrault\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBaroni\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2018\u003C\u002Fspan\u003E). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In \u003Cem class=\"italic\"\u003EProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EMelbourne, Australia\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E2126\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E2136\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP18-1198\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Conneau,+A.,+Kruszewski,+G.,+Lample,+G.,+Barrault,+L.+and+Baroni,+M.+(2018).+What+you+can+cram+into+a+single+vector:+Probing+sentence+embeddings+for+linguistic+properties.+In+Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Melbourne,+Australia:+Association+for+Computational+Linguistics,+pp.+2126–2136.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Conneau,+A.,+Kruszewski,+G.,+Lample,+G.,+Barrault,+L.+and+Baroni,+M.+(2018).+What+you+can+cram+into+a+single+vector:+Probing+sentence+embeddings+for+linguistic+properties.+In+Proceedings+of+the+56th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Melbourne,+Australia:+Association+for+Computational+Linguistics,+pp.+2126–2136.",openUrlParams:{genre:g,date:q,sid:e,title:d},innerRefId:"r8",title:"Conneau, A., Kruszewski, G., Lample, G., Barrault, L. and Baroni, M. (2018). What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia: Association for Computational Linguistics, pp. 2126–2136.",doi:"10.18653\u002Fv1\u002FP18-1198",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP18-1198",pubMedLink:a}]},{id:"ref9",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EConstant\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EEryiğit\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMonti\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003Evan der Plas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERosner\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETodirascu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2017\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003ESurvey: multiword expression processing: a survey\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EComputational Linguistics\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E43\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E837\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E892\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Survey: multiword expression processing: a survey' href=https:\u002F\u002Fdx.doi.org\u002F10.1162\u002FCOLI_a_00302\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Survey: multiword expression processing: a survey' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Survey%3A+multiword+expression+processing%3A+a+survey&author=Constant+M.&author=Eryi%C4%9Fit+G.&author=Monti+J.&author=van+der+Plas+L.&author=Ramisch+C.&author=Rosner+M.&author=Todirascu+A.&publication+year=2017&journal=Computational+Linguistics&volume=43&doi=10.1162%2FCOLI_a_00302&pages=837-892\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Survey%3A+multiword+expression+processing%3A+a+survey&author=Constant+M.&author=Eryi%C4%9Fit+G.&author=Monti+J.&author=van+der+Plas+L.&author=Ramisch+C.&author=Rosner+M.&author=Todirascu+A.&publication+year=2017&journal=Computational+Linguistics&volume=43&doi=10.1162%2FCOLI_a_00302&pages=837-892",openUrlParams:{genre:j,atitle:L,jtitle:r,title:r,volume:"43",artnum:"671a725f21c8183ee4264ac3",spage:"837",epage:"892",date:w,sid:e,aulast:a,aufirst:a,doi:M,au:a},innerRefId:"r9",title:L,doi:M,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1162\u002FCOLI_a_00302",pubMedLink:a}]},{id:"ref10",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECook\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFazly\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStevenson\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2008\u003C\u002Fspan\u003E). The VNC-tokens dataset. In \u003Cem class=\"italic\"\u003EProceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008)\u003C\u002Fem\u003E, pp. \u003Cspan class=\"fpage\"\u003E19\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E22\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Cook, P., Fazly, A. and Stevenson, S. (2008). The VNC-tokens dataset. In Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), pp. 19–22.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Cook,+P.,+Fazly,+A.+and+Stevenson,+S.+(2008).+The+VNC-tokens+dataset.+In+Proceedings+of+the+LREC+Workshop+Towards+a+Shared+Task+for+Multiword+Expressions+(MWE+2008),+pp.+19–22.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Cook,+P.,+Fazly,+A.+and+Stevenson,+S.+(2008).+The+VNC-tokens+dataset.+In+Proceedings+of+the+LREC+Workshop+Towards+a+Shared+Task+for+Multiword+Expressions+(MWE+2008),+pp.+19–22.",openUrlParams:{genre:g,date:N,sid:e,title:d},innerRefId:"r10",title:"Cook, P., Fazly, A. and Stevenson, S. (2008). The VNC-tokens dataset. In Proceedings of the LREC Workshop Towards a Shared Task for Multiword Expressions (MWE 2008), pp. 19–22.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref11",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDeerwester\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. C.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDumais\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. T.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELandauer\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET. K.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFurnas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG. W.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHarshman\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER. A.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E1990\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EIndexing by latent semantic analysis\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EJournal of the American Society of Information Science\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E41\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E391\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E407\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Indexing by latent semantic analysis' href=https:\u002F\u002Fdx.doi.org\u002F10.1002\u002F(SICI)1097-4571(199009)41:6\u003C391::AID-ASI1\u003E3.0.CO;2-9\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Indexing by latent semantic analysis' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Indexing+by+latent+semantic+analysis&author=Deerwester+S.+C.&author=Dumais+S.+T.&author=Landauer+T.+K.&author=Furnas+G.+W.&author=Harshman+R.+A.&publication+year=1990&journal=Journal+of+the+American+Society+of+Information+Science&volume=41&doi=10.1002%2F(SICI)1097-4571(199009)41%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9&pages=391-407\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Indexing+by+latent+semantic+analysis&author=Deerwester+S.+C.&author=Dumais+S.+T.&author=Landauer+T.+K.&author=Furnas+G.+W.&author=Harshman+R.+A.&publication+year=1990&journal=Journal+of+the+American+Society+of+Information+Science&volume=41&doi=10.1002%2F(SICI)1097-4571(199009)41%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9&pages=391-407",openUrlParams:{genre:j,atitle:O,jtitle:P,title:P,volume:"41",artnum:"671a725f21c8183ee4264ac5",spage:"391",epage:"407",date:"1990",sid:e,aulast:a,aufirst:a,doi:Q,au:a},innerRefId:"r11",title:O,doi:Q,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1002\u002F(SICI)1097-4571(199009)41:6\u003C391::AID-ASI1\u003E3.0.CO;2-9",pubMedLink:a}]},{id:"ref12",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDevlin\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELee\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EToutanova\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2018\u003C\u002Fspan\u003E). BERT: pre-training of deep bidirectional transformers for language understanding. \u003Cem class=\"italic\"\u003ENAACL-HLT\u003C\u002Fem\u003E, pp. 4171–4186.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Devlin, J., Chang, M., Lee, K. and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. NAACL-HLT, pp. 4171–4186.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Devlin,+J.,+Chang,+M.,+Lee,+K.+and+Toutanova,+K.+(2018).+BERT:+pre-training+of+deep+bidirectional+transformers+for+language+understanding.+NAACL-HLT,+pp.+4171–4186.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Devlin,+J.,+Chang,+M.,+Lee,+K.+and+Toutanova,+K.+(2018).+BERT:+pre-training+of+deep+bidirectional+transformers+for+language+understanding.+NAACL-HLT,+pp.+4171–4186.",openUrlParams:{genre:g,date:q,sid:e,title:d},innerRefId:"r12",title:"Devlin, J., Chang, M., Lee, K. and Toutanova, K. (2018). BERT: pre-training of deep bidirectional transformers for language understanding. NAACL-HLT, pp. 4171–4186.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref13",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EEisenstein\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). \u003Cspan class=\"source\"\u003EIntroduction to Natural Language Processing\u003C\u002Fspan\u003E. \u003Cspan class=\"publisher-loc\"\u003ECambridge, MA\u003C\u002Fspan\u003E: MIT Press.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Introduction to Natural Language Processing' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Introduction+to+Natural+Language+Processing&author=Eisenstein+J.&publication+year=2019\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Introduction+to+Natural+Language+Processing&author=Eisenstein+J.&publication+year=2019",openUrlParams:{genre:R,btitle:y,title:y,aulast:a,aufirst:a,au:a,pub:d,date:h,tpages:d,doi:a,sid:e},innerRefId:"r13",title:y,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref14",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EEttinger\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EWhat BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003ETransactions of the Association for Computational Linguistics\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E8\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E34\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E48\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models' href=https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Ftacl_a_00298\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=What+BERT+is+not%3A+lessons+from+a+new+suite+of+psycholinguistic+diagnostics+for+language+models&author=Ettinger+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00298&pages=34-48\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=What+BERT+is+not%3A+lessons+from+a+new+suite+of+psycholinguistic+diagnostics+for+language+models&author=Ettinger+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00298&pages=34-48",openUrlParams:{genre:j,atitle:S,jtitle:s,title:s,volume:T,artnum:"671a725f21c8183ee4264ac8",spage:"34",epage:"48",date:i,sid:e,aulast:a,aufirst:a,doi:U,au:a},innerRefId:"r14",title:S,doi:U,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Ftacl_a_00298",pubMedLink:a}]},{id:"ref15",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFazly\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECook\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStevenson\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2009\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EUnsupervised type and token identification of idiomatic expressions\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EComputational Linguistics\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E35\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E61\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E103\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Unsupervised type and token identification of idiomatic expressions' href=https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Fcoli.08-010-R1-07-048\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Unsupervised type and token identification of idiomatic expressions' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Unsupervised+type+and+token+identification+of+idiomatic+expressions&author=Fazly+A.&author=Cook+P.&author=Stevenson+S.&publication+year=2009&journal=Computational+Linguistics&volume=35&doi=10.1162%2Fcoli.08-010-R1-07-048&pages=61-103\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Unsupervised+type+and+token+identification+of+idiomatic+expressions&author=Fazly+A.&author=Cook+P.&author=Stevenson+S.&publication+year=2009&journal=Computational+Linguistics&volume=35&doi=10.1162%2Fcoli.08-010-R1-07-048&pages=61-103",openUrlParams:{genre:j,atitle:V,jtitle:r,title:r,volume:W,artnum:"671a725f21c8183ee4264ac9",spage:"61",epage:"103",date:"2009",sid:e,aulast:a,aufirst:a,doi:X,au:a},innerRefId:"r15",title:V,doi:X,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Fcoli.08-010-R1-07-048",pubMedLink:a}]},{id:"ref16",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFeldman\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPeng\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2013\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003EAutomatic detection of idiomatic clauses\u003C\u002Fspan\u003E. In Gelbukh A., (ed), \u003Cspan class=\"source\"\u003EComputational Linguistics and Intelligent Text Processing\u003C\u002Fspan\u003E. \u003Cspan class=\"publisher-loc\"\u003EBerlin, Heidelberg\u003C\u002Fspan\u003E: Springer, pp. \u003Cspan class=\"fpage\"\u003E435\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E446\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Computational Linguistics and Intelligent Text Processing' href=https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-642-37247-6_35\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Computational Linguistics and Intelligent Text Processing' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Computational+Linguistics+and+Intelligent+Text+Processing&author=Feldman+A.&author=Peng+J.&publication+year=2013&pages=435-446\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Computational+Linguistics+and+Intelligent+Text+Processing&author=Feldman+A.&author=Peng+J.&publication+year=2013&pages=435-446",openUrlParams:{genre:n,btitle:z,title:z,atitle:"Automatic detection of idiomatic clauses",aulast:a,aufirst:a,au:a,pub:d,date:Y,spage:"435",epage:"446",doi:Z,sid:e},innerRefId:"r16",title:z,doi:Z,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F978-3-642-37247-6_35",pubMedLink:a}]},{id:"ref17",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGarcia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKramer Vieira\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EScarton\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EIdiart\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVillavicencio\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). Probing for idiomaticity in vector space models. In \u003Cem class=\"italic\"\u003EProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E3551\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E3564\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.eacl-main.310\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Garcia,+M.,+Kramer+Vieira,+T.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2021).+Probing+for+idiomaticity+in+vector+space+models.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3551–3564.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Garcia,+M.,+Kramer+Vieira,+T.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2021).+Probing+for+idiomaticity+in+vector+space+models.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3551–3564.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r17",title:"Garcia, M., Kramer Vieira, T., Scarton, C., Idiart, M. and Villavicencio, A. (2021). Probing for idiomaticity in vector space models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3551–3564.",doi:"10.18653\u002Fv1\u002F2021.eacl-main.310",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.eacl-main.310",pubMedLink:a}]},{id:"ref18",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGlavaš\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVulić\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In \u003Cem class=\"italic\"\u003EProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E3090\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E3104\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.eacl-main.270\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Glavaš,+G.+and+Vulić,+I.+(2021).+Is+supervised+syntactic+parsing+beneficial+for+language+understanding+tasks?+an+empirical+investigation.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3090–3104.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Glavaš,+G.+and+Vulić,+I.+(2021).+Is+supervised+syntactic+parsing+beneficial+for+language+understanding+tasks?+an+empirical+investigation.+In+Proceedings+of+the+16th+Conference+of+the+European+Chapter+of+the+Association+for+Computational+Linguistics:+Main+Volume,+Online.+Association+for+Computational+Linguistics,+pp.+3090–3104.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r18",title:"Glavaš, G. and Vulić, I. (2021). Is supervised syntactic parsing beneficial for language understanding tasks? an empirical investigation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics, pp. 3090–3104.",doi:"10.18653\u002Fv1\u002F2021.eacl-main.270",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.eacl-main.270",pubMedLink:a}]},{id:"ref19",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGoldberg\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). \u003Cspan class=\"source\"\u003EAssessing BERT’s Syntactic Abilities\u003C\u002Fspan\u003E. CoRR, abs\u002F1901.05287.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Goldberg, Y. (2019). Assessing BERT’s Syntactic Abilities. CoRR, abs\u002F1901.05287.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Goldberg,+Y.+(2019).+Assessing+BERT’s+Syntactic+Abilities.+CoRR,+abs\u002F1901.05287.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Goldberg,+Y.+(2019).+Assessing+BERT’s+Syntactic+Abilities.+CoRR,+abs\u002F1901.05287.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r19",title:"Goldberg, Y. (2019). Assessing BERT’s Syntactic Abilities. CoRR, abs\u002F1901.05287.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref20",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGupta\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKvernadze\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESrikumar\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EBert &amp; family eat word salad: experiments with text understanding\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EProceedings of the AAAI Conference on Artificial Intelligence\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E35\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E12946\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E12954\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Bert & family eat word salad: experiments with text understanding' href=https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v35i14.17531\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Bert & family eat word salad: experiments with text understanding' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Bert+%26+family+eat+word+salad%3A+experiments+with+text+understanding&author=Gupta+A.&author=Kvernadze+G.&author=Srikumar+V.&publication+year=2021&journal=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence&volume=35&doi=10.1609%2Faaai.v35i14.17531&pages=12946-12954\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Bert+%26+family+eat+word+salad%3A+experiments+with+text+understanding&author=Gupta+A.&author=Kvernadze+G.&author=Srikumar+V.&publication+year=2021&journal=Proceedings+of+the+AAAI+Conference+on+Artificial+Intelligence&volume=35&doi=10.1609%2Faaai.v35i14.17531&pages=12946-12954",openUrlParams:{genre:j,atitle:_,jtitle:$,title:$,volume:W,artnum:"671a725f21c8183ee4264ace",spage:"12946",epage:"12954",date:m,sid:e,aulast:a,aufirst:a,doi:aa,au:a},innerRefId:"r20",title:_,doi:aa,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v35i14.17531",pubMedLink:a}]},{id:"ref21",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHaagsma\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EH.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBos\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENissim\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). MAGPIE: A large corpus of potentially idiomatic expressions. In \u003Cem class=\"italic\"\u003EProceedings of the Twelfth Language Resources and Evaluation Conference\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EMarseille, France\u003C\u002Fspan\u003E: European Language Resources Association, pp. \u003Cspan class=\"fpage\"\u003E279\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E287\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Haagsma, H., Bos, J. and Nissim, M. (2020). MAGPIE: A large corpus of potentially idiomatic expressions. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France: European Language Resources Association, pp. 279–287.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Haagsma,+H.,+Bos,+J.+and+Nissim,+M.+(2020).+MAGPIE:+A+large+corpus+of+potentially+idiomatic+expressions.+In+Proceedings+of+the+Twelfth+Language+Resources+and+Evaluation+Conference,+Marseille,+France:+European+Language+Resources+Association,+pp.+279–287.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Haagsma,+H.,+Bos,+J.+and+Nissim,+M.+(2020).+MAGPIE:+A+large+corpus+of+potentially+idiomatic+expressions.+In+Proceedings+of+the+Twelfth+Language+Resources+and+Evaluation+Conference,+Marseille,+France:+European+Language+Resources+Association,+pp.+279–287.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r21",title:"Haagsma, H., Bos, J. and Nissim, M. (2020). MAGPIE: A large corpus of potentially idiomatic expressions. In Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France: European Language Resources Association, pp. 279–287.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref22",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHalimu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKasem\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENewaz\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. H. S.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In \u003Cem class=\"italic\"\u003EProceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ENew York, NY, USA\u003C\u002Fspan\u003E: Association for Computing Machinery, pp. \u003Cspan class=\"fpage\"\u003E1\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E6\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.' href=https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3310986.3311023\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Halimu,+C.,+Kasem,+A.+and+Newaz,+S.+H.+S.+(2019).+Empirical+comparison+of+area+under+ROC+curve+(AUC)+and+mathew+correlation+coefficient+(MCC)+for+evaluating+machine+learning+algorithms+on+imbalanced+datasets+for+binary+classification.+In+Proceedings+of+the+3rd+International+Conference+on+Machine+Learning+and+Soft+Computing,+ICMLSC,+New+York,+NY,+USA:+Association+for+Computing+Machinery,+pp.+1–6.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Halimu,+C.,+Kasem,+A.+and+Newaz,+S.+H.+S.+(2019).+Empirical+comparison+of+area+under+ROC+curve+(AUC)+and+mathew+correlation+coefficient+(MCC)+for+evaluating+machine+learning+algorithms+on+imbalanced+datasets+for+binary+classification.+In+Proceedings+of+the+3rd+International+Conference+on+Machine+Learning+and+Soft+Computing,+ICMLSC,+New+York,+NY,+USA:+Association+for+Computing+Machinery,+pp.+1–6.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r22",title:"Halimu, C., Kasem, A. and Newaz, S. H. S. (2019). Empirical comparison of area under ROC curve (AUC) and mathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets for binary classification. In Proceedings of the 3rd International Conference on Machine Learning and Soft Computing, ICMLSC, New York, NY, USA: Association for Computing Machinery, pp. 1–6.",doi:"10.1145\u002F3310986.3311023",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1145\u002F3310986.3311023",pubMedLink:a}]},{id:"ref23",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHashempour\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVillavicencio\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In \u003Cem class=\"italic\"\u003EProceedings of the Workshop on the Cognitive Aspects of the Lexicon\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E72\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E80\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Hashempour, R. and Villavicencio, A. (2020). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In Proceedings of the Workshop on the Cognitive Aspects of the Lexicon, Online. Association for Computational Linguistics, pp. 72–80.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hashempour,+R.+and+Villavicencio,+A.+(2020).+Leveraging+contextual+embeddings+and+idiom+principle+for+detecting+idiomaticity+in+potentially+idiomatic+expressions.+In+Proceedings+of+the+Workshop+on+the+Cognitive+Aspects+of+the+Lexicon,+Online.+Association+for+Computational+Linguistics,+pp.+72–80.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hashempour,+R.+and+Villavicencio,+A.+(2020).+Leveraging+contextual+embeddings+and+idiom+principle+for+detecting+idiomaticity+in+potentially+idiomatic+expressions.+In+Proceedings+of+the+Workshop+on+the+Cognitive+Aspects+of+the+Lexicon,+Online.+Association+for+Computational+Linguistics,+pp.+72–80.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r23",title:"Hashempour, R. and Villavicencio, A. (2020). Leveraging contextual embeddings and idiom principle for detecting idiomaticity in potentially idiomatic expressions. In Proceedings of the Workshop on the Cognitive Aspects of the Lexicon, Online. Association for Computational Linguistics, pp. 72–80.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref24",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHashimoto\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKawahara\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2008\u003C\u002Fspan\u003E). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In \u003Cem class=\"italic\"\u003EProceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EStroudsburg, PA, USA\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E992\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E1001\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.' href=https:\u002F\u002Fdx.doi.org\u002F10.3115\u002F1613715.1613844\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hashimoto,+C.+and+Kawahara,+D.+(2008).+Construction+of+an+idiom+corpus+and+its+application+to+idiom+identification+based+on+wsd+incorporating+idiom-specific+features.+In+Proceedings+of+the+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+EMNLP+’08,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+992–1001.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hashimoto,+C.+and+Kawahara,+D.+(2008).+Construction+of+an+idiom+corpus+and+its+application+to+idiom+identification+based+on+wsd+incorporating+idiom-specific+features.+In+Proceedings+of+the+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+EMNLP+’08,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+992–1001.",openUrlParams:{genre:g,date:N,sid:e,title:d},innerRefId:"r24",title:"Hashimoto, C. and Kawahara, D. (2008). Construction of an idiom corpus and its application to idiom identification based on wsd incorporating idiom-specific features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 992–1001.",doi:"10.3115\u002F1613715.1613844",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.3115\u002F1613715.1613844",pubMedLink:a}]},{id:"ref25",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHessel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESchofield\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). How effective is BERT without word ordering? implications for language understanding and data privacy. In \u003Cem class=\"italic\"\u003EProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E204\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E211\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.acl-short.27\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hessel,+J.+and+Schofield,+A.+(2021).+How+effective+is+BERT+without+word+ordering?+implications+for+language+understanding+and+data+privacy.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+2:+Short+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+204–211.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hessel,+J.+and+Schofield,+A.+(2021).+How+effective+is+BERT+without+word+ordering?+implications+for+language+understanding+and+data+privacy.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+2:+Short+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+204–211.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r25",title:"Hessel, J. and Schofield, A. (2021). How effective is BERT without word ordering? implications for language understanding and data privacy. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Online. Association for Computational Linguistics, pp. 204–211.",doi:"10.18653\u002Fv1\u002F2021.acl-short.27",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.acl-short.27",pubMedLink:a}]},{id:"ref26",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHewitt\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). Designing and interpreting probes with control tasks. In \u003Cem class=\"italic\"\u003EProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EHong Kong, China\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E2733\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E2743\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FD19-1275\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hewitt,+J.+and+Liang,+P.+(2019).+Designing+and+interpreting+probes+with+control+tasks.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing,+EMNLP-IJCNLP.+2019,+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+2733–2743.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hewitt,+J.+and+Liang,+P.+(2019).+Designing+and+interpreting+probes+with+control+tasks.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing,+EMNLP-IJCNLP.+2019,+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+2733–2743.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r26",title:"Hewitt, J. and Liang, P. (2019). Designing and interpreting probes with control tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP. 2019, Hong Kong, China: Association for Computational Linguistics, pp. 2733–2743.",doi:"10.18653\u002Fv1\u002FD19-1275",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FD19-1275",pubMedLink:a}]},{id:"ref27",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHewitt\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). A structural probe for finding syntax in word representations. In \u003Cem class=\"italic\"\u003EProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EMinneapolis, Minnesota\u003C\u002Fspan\u003E: Association for Computational Linguistics, vol \u003Cspan class=\"volume\"\u003E1\u003C\u002Fspan\u003E, pp. \u003Cspan class=\"fpage\"\u003E4129\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E4138\u003C\u002Fspan\u003E, \u003Cspan class=\"series\"\u003E(Long and Short Papers)\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Minnesota: Association for Computational Linguistics, vol 1, pp. 4129–4138, (Long and Short Papers).' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hewitt,+J.+and+Manning,+C.+D.+(2019).+A+structural+probe+for+finding+syntax+in+word+representations.+In+Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics:+Human+Language+Technologies,+Minneapolis,+Minnesota:+Association+for+Computational+Linguistics,+vol+1,+pp.+4129–4138,+(Long+and+Short+Papers).\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Hewitt,+J.+and+Manning,+C.+D.+(2019).+A+structural+probe+for+finding+syntax+in+word+representations.+In+Proceedings+of+the+2019+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics:+Human+Language+Technologies,+Minneapolis,+Minnesota:+Association+for+Computational+Linguistics,+vol+1,+pp.+4129–4138,+(Long+and+Short+Papers).",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r27",title:"Hewitt, J. and Manning, C. D. (2019). A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Minnesota: Association for Computational Linguistics, vol 1, pp. 4129–4138, (Long and Short Papers).",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref28",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJawahar\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESagot\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESeddah\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Ea). What does BERT learn about the structure of language? In \u003Cem class=\"italic\"\u003EProceedings of the 57th Annual Meeting of the Association for Computational Linguistics\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EFlorence, Italy\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E3651\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E3657\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP19-1356\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019a).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+3651–3657.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019a).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+3651–3657.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r28",title:"Jawahar, G., Sagot, B. and Seddah, D. (2019a). What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 3651–3657.",doi:"10.18653\u002Fv1\u002FP19-1356",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP19-1356",pubMedLink:a}]},{id:"ref29",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJawahar\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESagot\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESeddah\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Eb). What does BERT learn about the structure of language? In \u003Cem class=\"italic\"\u003EProceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EFlorence, Italy\u003C\u002Fspan\u003E: Association for Computational Linguistics, vol \u003Cspan class=\"volume\"\u003E1\u003C\u002Fspan\u003E, pp. \u003Cspan class=\"fpage\"\u003E3651\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E3657\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Jawahar, G., Sagot, B. and Seddah, D. (2019b). What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, Florence, Italy: Association for Computational Linguistics, vol 1, pp. 3651–3657.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019b).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Conference+of+the+Association+for+Computational+Linguistics,+ACL.+2019,+Florence,+Italy:+Association+for+Computational+Linguistics,+vol+1,+pp.+3651–3657.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Jawahar,+G.,+Sagot,+B.+and+Seddah,+D.+(2019b).+What+does+BERT+learn+about+the+structure+of+language?+In+Proceedings+of+the+57th+Conference+of+the+Association+for+Computational+Linguistics,+ACL.+2019,+Florence,+Italy:+Association+for+Computational+Linguistics,+vol+1,+pp.+3651–3657.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r29",title:"Jawahar, G., Sagot, B. and Seddah, D. (2019b). What does BERT learn about the structure of language? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL. 2019, Florence, Italy: Association for Computational Linguistics, vol 1, pp. 3651–3657.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref30",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKacmajor\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKelleher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003ECapturing and measuring thematic relatedness\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003ELanguage Resources and Evaluation\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E54\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E645\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E682\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Capturing and measuring thematic relatedness' href=https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs10579-019-09452-w\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Capturing and measuring thematic relatedness' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Capturing+and+measuring+thematic+relatedness&author=Kacmajor+M.&author=Kelleher+J.+D.&publication+year=2020&journal=Language+Resources+and+Evaluation&volume=54&doi=10.1007%2Fs10579-019-09452-w&pages=645-682\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Capturing+and+measuring+thematic+relatedness&author=Kacmajor+M.&author=Kelleher+J.+D.&publication+year=2020&journal=Language+Resources+and+Evaluation&volume=54&doi=10.1007%2Fs10579-019-09452-w&pages=645-682",openUrlParams:{genre:j,atitle:ab,jtitle:ac,title:ac,volume:ad,artnum:"671a725f21c8183ee4264ad8",spage:"645",epage:"682",date:i,sid:e,aulast:a,aufirst:a,doi:ae,au:a},innerRefId:"r30",title:ab,doi:ae,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002Fs10579-019-09452-w",pubMedLink:a}]},{id:"ref31",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKim\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChoi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EEdmiston\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003Egoo Lee\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. \u003Cspan class=\"source\"\u003EArXiv\u003C\u002Fspan\u003E, abs\u002F2002.00737.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Kim, T., Choi, J., Edmiston, D. and goo Lee, S. (2020). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. ArXiv, abs\u002F2002.00737.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Kim,+T.,+Choi,+J.,+Edmiston,+D.+and+goo+Lee,+S.+(2020).+Are+pre-trained+language+models+aware+of+phrases?+simple+but+strong+baselines+for+grammar+induction.+ArXiv,+abs\u002F2002.00737.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Kim,+T.,+Choi,+J.,+Edmiston,+D.+and+goo+Lee,+S.+(2020).+Are+pre-trained+language+models+aware+of+phrases?+simple+but+strong+baselines+for+grammar+induction.+ArXiv,+abs\u002F2002.00737.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r31",title:"Kim, T., Choi, J., Edmiston, D. and goo Lee, S. (2020). Are pre-trained language models aware of phrases? simple but strong baselines for grammar induction. ArXiv, abs\u002F2002.00737.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref32",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKiros\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EZhu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESalakhutdinov\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER. R.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EZemel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EUrtasun\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETorralba\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFidler\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2015\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003ESkip-thought vectors\u003C\u002Fspan\u003E. In Cortes C., Lawrence N., Lee D., Sugiyama M. and Garnett R., (eds), \u003Cspan class=\"source\"\u003EAdvances in Neural Information Processing Systems\u003C\u002Fspan\u003E, \u003Cspan class=\"volume\"\u003E28\u003C\u002Fspan\u003E, Curran Associates, Inc.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Advances in Neural Information Processing Systems' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Kiros+R.&author=Zhu+Y.&author=Salakhutdinov+R.+R.&author=Zemel+R.&author=Urtasun+R.&author=Torralba+A.&author=Fidler+S.&publication+year=2015\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Kiros+R.&author=Zhu+Y.&author=Salakhutdinov+R.+R.&author=Zemel+R.&author=Urtasun+R.&author=Torralba+A.&author=Fidler+S.&publication+year=2015",openUrlParams:{genre:n,btitle:o,title:o,atitle:"Skip-thought vectors",aulast:a,aufirst:a,au:a,pub:d,date:"2015",spage:d,epage:d,doi:a,sid:e},innerRefId:"r32",title:o,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref33",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKlubička\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKelleher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). Probing with noise: Unpicking the warp and weft of embeddings. In \u003Cem class=\"italic\"\u003EProceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP\u003C\u002Fem\u003E, Association for Computational Linguistics.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.blackboxnlp-1.34\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Klubička,+F.+and+Kelleher,+J.+D.+(2022).+Probing+with+noise:+Unpicking+the+warp+and+weft+of+embeddings.+In+Proceedings+of+the+Fifth+BlackBoxNLP+Workshop+on+analyzing+and+interpreting+neural+networks+for+NLP,+Association+for+Computational+Linguistics.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Klubička,+F.+and+Kelleher,+J.+D.+(2022).+Probing+with+noise:+Unpicking+the+warp+and+weft+of+embeddings.+In+Proceedings+of+the+Fifth+BlackBoxNLP+Workshop+on+analyzing+and+interpreting+neural+networks+for+NLP,+Association+for+Computational+Linguistics.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r33",title:"Klubička, F. and Kelleher, J. D. (2022). Probing with noise: Unpicking the warp and weft of embeddings. In Proceedings of the Fifth BlackBoxNLP Workshop on analyzing and interpreting neural networks for NLP, Association for Computational Linguistics.",doi:"10.18653\u002Fv1\u002F2022.blackboxnlp-1.34",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2022.blackboxnlp-1.34",pubMedLink:a}]},{id:"ref34",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKovaleva\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERomanov\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERogers\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERumshisky\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). Revealing the dark secrets of BERT. In \u003Cem class=\"italic\"\u003EProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EHong Kong, China\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E4365\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E4374\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FD19-1445\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Kovaleva,+O.,+Romanov,+A.,+Rogers,+A.+and+Rumshisky,+A.+(2019).+Revealing+the+dark+secrets+of+BERT.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing+(EMNLP-IJCNLP),+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+4365–4374.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Kovaleva,+O.,+Romanov,+A.,+Rogers,+A.+and+Rumshisky,+A.+(2019).+Revealing+the+dark+secrets+of+BERT.+In+Proceedings+of+the+2019+Conference+on+Empirical+Methods+in+Natural+Language+Processing+and+the+9th+International+Joint+Conference+on+Natural+Language+Processing+(EMNLP-IJCNLP),+Hong+Kong,+China:+Association+for+Computational+Linguistics,+pp.+4365–4374.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r34",title:"Kovaleva, O., Romanov, A., Rogers, A. and Rumshisky, A. (2019). Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, pp. 4365–4374.",doi:"10.18653\u002Fv1\u002FD19-1445",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FD19-1445",pubMedLink:a}]},{id:"ref35",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESporleder\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2010\u003C\u002Fspan\u003Ea). Linguistic cues for distinguishing literal and non-literal usages. In \u003Cem class=\"italic\"\u003EProceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EStroudsburg, PA, USA\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E683\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E691\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Li, L. and Sporleder, C. (2010a). Linguistic cues for distinguishing literal and non-literal usages. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 683–691.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Li,+L.+and+Sporleder,+C.+(2010a).+Linguistic+cues+for+distinguishing+literal+and+non-literal+usages.+In+Proceedings+of+the+23rd+International+Conference+on+Computational+Linguistics:+Posters,+COLING+’10,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+683–691.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Li,+L.+and+Sporleder,+C.+(2010a).+Linguistic+cues+for+distinguishing+literal+and+non-literal+usages.+In+Proceedings+of+the+23rd+International+Conference+on+Computational+Linguistics:+Posters,+COLING+’10,+Stroudsburg,+PA,+USA:+Association+for+Computational+Linguistics,+pp.+683–691.",openUrlParams:{genre:g,date:A,sid:e,title:d},innerRefId:"r35",title:"Li, L. and Sporleder, C. (2010a). Linguistic cues for distinguishing literal and non-literal usages. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 683–691.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref36",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESporleder\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2010\u003C\u002Fspan\u003Eb). Using Gaussian mixture models to detect figurative language in context. In \u003Cem class=\"italic\"\u003EHuman Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ELos Angeles, California\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E297\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E300\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Li, L. and Sporleder, C. (2010b). Using Gaussian mixture models to detect figurative language in context. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California: Association for Computational Linguistics, pp. 297–300.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Li,+L.+and+Sporleder,+C.+(2010b).+Using+Gaussian+mixture+models+to+detect+figurative+language+in+context.+In+Human+Language+Technologies:+The+2010+Annual+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics,+Los+Angeles,+California:+Association+for+Computational+Linguistics,+pp.+297–300.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Li,+L.+and+Sporleder,+C.+(2010b).+Using+Gaussian+mixture+models+to+detect+figurative+language+in+context.+In+Human+Language+Technologies:+The+2010+Annual+Conference+of+the+North+American+Chapter+of+the+Association+for+Computational+Linguistics,+Los+Angeles,+California:+Association+for+Computational+Linguistics,+pp.+297–300.",openUrlParams:{genre:g,date:A,sid:e,title:d},innerRefId:"r36",title:"Li, L. and Sporleder, C. (2010b). Using Gaussian mixture models to detect figurative language in context. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California: Association for Computational Linguistics, pp. 297–300.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref37",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELin\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETan\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY. C.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFrank\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). Open sesame: Getting inside BERT’s linguistic knowledge. In \u003Cem class=\"italic\"\u003EProceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EFlorence, Italy\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E241\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E253\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW19-4825\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Lin,+Y.,+Tan,+Y.+C.+and+Frank,+R.+(2019).+Open+sesame:+Getting+inside+BERT’s+linguistic+knowledge.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+241–253.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Lin,+Y.,+Tan,+Y.+C.+and+Frank,+R.+(2019).+Open+sesame:+Getting+inside+BERT’s+linguistic+knowledge.+In+Proceedings+of+the+2019+ACL+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+241–253.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r37",title:"Lin, Y., Tan, Y. C. and Frank, R. (2019). Open sesame: Getting inside BERT’s linguistic knowledge. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Florence, Italy: Association for Computational Linguistics, pp. 241–253.",doi:"10.18653\u002Fv1\u002FW19-4825",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW19-4825",pubMedLink:a}]},{id:"ref38",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN. F.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGardner\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBelinkov\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPeters\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM. E.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESmith\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN. A.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Ea). Linguistic knowledge and transferability of contextual representations. \u003Cspan class=\"source\"\u003ECoRR\u003C\u002Fspan\u003E, abs\u002F1903.08855.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs\u002F1903.08855.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FN19-1112\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs\u002F1903.08855.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Liu,+N.+F.,+Gardner,+M.,+Belinkov,+Y.,+Peters,+M.+E.+and+Smith,+N.+A.+(2019a).+Linguistic+knowledge+and+transferability+of+contextual+representations.+CoRR,+abs\u002F1903.08855.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Liu,+N.+F.,+Gardner,+M.,+Belinkov,+Y.,+Peters,+M.+E.+and+Smith,+N.+A.+(2019a).+Linguistic+knowledge+and+transferability+of+contextual+representations.+CoRR,+abs\u002F1903.08855.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r38",title:"Liu, N. F., Gardner, M., Belinkov, Y., Peters, M. E. and Smith, N. A. (2019a). Linguistic knowledge and transferability of contextual representations. CoRR, abs\u002F1903.08855.",doi:"10.18653\u002Fv1\u002FN19-1112",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FN19-1112",pubMedLink:a}]},{id:"ref39",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EOtt\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGoyal\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJoshi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELevy\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELewis\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EZettlemoyer\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStoyanov\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Eb). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. (2019b). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Liu,+Y.,+Ott,+M.,+Goyal,+N.,+Du,+J.,+Joshi,+M.,+Chen,+D.,+Levy,+O.,+Lewis,+M.,+Zettlemoyer,+L.+and+Stoyanov,+V.+(2019b).+RoBERTa:+ARobustly+Optimized+BERT+Pretraining+Approach,+arXiv+preprint+arXiv:1907.11692.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Liu,+Y.,+Ott,+M.,+Goyal,+N.,+Du,+J.,+Joshi,+M.,+Chen,+D.,+Levy,+O.,+Lewis,+M.,+Zettlemoyer,+L.+and+Stoyanov,+V.+(2019b).+RoBERTa:+ARobustly+Optimized+BERT+Pretraining+Approach,+arXiv+preprint+arXiv:1907.11692.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r39",title:"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. and Stoyanov, V. (2019b). RoBERTa: ARobustly Optimized BERT Pretraining Approach, arXiv preprint arXiv:1907.11692.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref40",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESchutze\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EH.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E1999\u003C\u002Fspan\u003E). \u003Cspan class=\"source\"\u003EFoundations of Statistical Natural Language Processing\u003C\u002Fspan\u003E. \u003Cspan class=\"publisher-loc\"\u003ECambridge, MA\u003C\u002Fspan\u003E: MIT Press.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Foundations of Statistical Natural Language Processing' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Foundations+of+Statistical+Natural+Language+Processing&author=Manning+C.&author=Schutze+H.&publication+year=1999\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Foundations+of+Statistical+Natural+Language+Processing&author=Manning+C.&author=Schutze+H.&publication+year=1999",openUrlParams:{genre:R,btitle:B,title:B,aulast:a,aufirst:a,au:a,pub:d,date:"1999",tpages:d,doi:a,sid:e},innerRefId:"r40",title:B,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref41",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EClark\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHewitt\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKhandelwal\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EU.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELevy\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EEmergent linguistic structure in artificial neural networks trained by self-supervision\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EProceedings of the National Academy of Sciences of the United States of America\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E117\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E30046\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E30054\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https:\u002F\u002Fdx.doi.org\u002F10.1073\u002Fpnas.1907367117\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Emergent+linguistic+structure+in+artificial+neural+networks+trained+by+self-supervision&author=Manning+C.+D.&author=Clark+K.&author=Hewitt+J.&author=Khandelwal+U.&author=Levy+O.&publication+year=2020&journal=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&volume=117&doi=10.1073%2Fpnas.1907367117&pages=30046-30054\u003EGoogle Scholar\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='PubMed link for Emergent linguistic structure in artificial neural networks trained by self-supervision' href=https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F32493748\u003EPubMed\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Emergent+linguistic+structure+in+artificial+neural+networks+trained+by+self-supervision&author=Manning+C.+D.&author=Clark+K.&author=Hewitt+J.&author=Khandelwal+U.&author=Levy+O.&publication+year=2020&journal=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&volume=117&doi=10.1073%2Fpnas.1907367117&pages=30046-30054",openUrlParams:{genre:j,atitle:af,jtitle:ag,title:ag,volume:"117",artnum:"671a725f21c8183ee4264ae3",spage:"30046",epage:"30054",date:i,sid:e,aulast:a,aufirst:a,doi:ah,au:a},innerRefId:"r41",title:af,pubMedId:"32493748",doi:ah,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1073\u002Fpnas.1907367117",pubMedLink:"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F32493748"}]},{id:"ref42",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMickus\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPaperno\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EConstant\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003Evan Deemter\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). What do you mean, BERT? In \u003Cem class=\"italic\"\u003EProceedings of the Society for Computation in Linguistics.\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ENew York\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E279\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E290\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Mickus, T., Paperno, D., Constant, M. and van Deemter, K. (2020). What do you mean, BERT? In Proceedings of the Society for Computation in Linguistics., New York: Association for Computational Linguistics, pp. 279–290.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Mickus,+T.,+Paperno,+D.,+Constant,+M.+and+van+Deemter,+K.+(2020).+What+do+you+mean,+BERT?+In+Proceedings+of+the+Society+for+Computation+in+Linguistics.,+New+York:+Association+for+Computational+Linguistics,+pp.+279–290.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Mickus,+T.,+Paperno,+D.,+Constant,+M.+and+van+Deemter,+K.+(2020).+What+do+you+mean,+BERT?+In+Proceedings+of+the+Society+for+Computation+in+Linguistics.,+New+York:+Association+for+Computational+Linguistics,+pp.+279–290.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r42",title:"Mickus, T., Paperno, D., Constant, M. and van Deemter, K. (2020). What do you mean, BERT? In Proceedings of the Society for Computation in Linguistics., New York: Association for Computational Linguistics, pp. 279–290.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref43",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMosbach\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKhokhlova\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHedderich\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM. A.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKlakow\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). \n\u003Cem class=\"italic\"\u003EOn the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers\u003C\u002Fem\u003E\n. In \u003Cem class=\"italic\"\u003EProceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E68\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E82\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2020.blackboxnlp-1.7\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Mosbach,+M.,+Khokhlova,+A.,+Hedderich,+M.+A.+and+Klakow,+D.+(2020).+On+the+interplay+between+fine-tuning+and+sentence-level+probing+for+linguistic+knowledge+in+pre-trained+transformers+.+In+Proceedings+of+the+Third+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+68–82.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Mosbach,+M.,+Khokhlova,+A.,+Hedderich,+M.+A.+and+Klakow,+D.+(2020).+On+the+interplay+between+fine-tuning+and+sentence-level+probing+for+linguistic+knowledge+in+pre-trained+transformers+.+In+Proceedings+of+the+Third+BlackboxNLP+Workshop+on+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Online.+Association+for+Computational+Linguistics,+pp.+68–82.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r43",title:"Mosbach, M., Khokhlova, A., Hedderich, M. A. and Klakow, D. (2020). On the interplay between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers . In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Online. Association for Computational Linguistics, pp. 68–82.",doi:"10.18653\u002Fv1\u002F2020.blackboxnlp-1.7",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2020.blackboxnlp-1.7",pubMedLink:a}]},{id:"ref44",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENedumpozhimana\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKelleher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). Finding BERT’s idiomatic key. In \u003Cem class=\"italic\"\u003EProceedings of the 17th Workshop on Multiword Expressions (MWE 2021)\u003C\u002Fem\u003E, pp. \u003Cspan class=\"fpage\"\u003E57\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E62\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.mwe-1.7\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Nedumpozhimana,+V.+and+Kelleher,+J.+(2021).+Finding+BERT’s+idiomatic+key.+In+Proceedings+of+the+17th+Workshop+on+Multiword+Expressions+(MWE+2021),+pp.+57–62.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Nedumpozhimana,+V.+and+Kelleher,+J.+(2021).+Finding+BERT’s+idiomatic+key.+In+Proceedings+of+the+17th+Workshop+on+Multiword+Expressions+(MWE+2021),+pp.+57–62.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r44",title:"Nedumpozhimana, V. and Kelleher, J. (2021). Finding BERT’s idiomatic key. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pp. 57–62.",doi:"10.18653\u002Fv1\u002F2021.mwe-1.7",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.mwe-1.7",pubMedLink:a}]},{id:"ref45",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENedumpozhimana\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKlubička\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKelleher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EShapley idioms: analysing BERT sentence embeddings for general idiom token identification\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EFrontiers in Artificial Intelligence\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E5\u003C\u002Fspan\u003E, 813967.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffrai.2022.813967\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Shapley+idioms%3A+analysing+BERT+sentence+embeddings+for+general+idiom+token+identification&author=Nedumpozhimana+V.&author=Klubi%C4%8Dka+F.&author=Kelleher+J.+D.&publication+year=2022&journal=Frontiers+in+Artificial+Intelligence&volume=5&doi=10.3389%2Ffrai.2022.813967\u003EGoogle Scholar\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='PubMed link for Shapley idioms: analysing BERT sentence embeddings for general idiom token identification' href=https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F35360661\u003EPubMed\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Shapley+idioms%3A+analysing+BERT+sentence+embeddings+for+general+idiom+token+identification&author=Nedumpozhimana+V.&author=Klubi%C4%8Dka+F.&author=Kelleher+J.+D.&publication+year=2022&journal=Frontiers+in+Artificial+Intelligence&volume=5&doi=10.3389%2Ffrai.2022.813967",openUrlParams:{genre:j,atitle:ai,jtitle:aj,title:aj,volume:v,artnum:"671a725f21c8183ee4264ae7",spage:d,epage:d,date:l,sid:e,aulast:a,aufirst:a,doi:ak,au:a},innerRefId:"r45",title:ai,pubMedId:"35360661",doi:ak,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.3389\u002Ffrai.2022.813967",pubMedLink:"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F35360661"}]},{id:"ref46",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENiu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EW.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPenn\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). Does BERT rediscover a classical NLP pipeline? In \u003Cem class=\"italic\"\u003EProceedings of the 29th International Conference on Computational Linguistics\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EGyeongju, Republic of Korea\u003C\u002Fspan\u003E: International Committee on Computational Linguistics, \u003Cspan class=\"fpage\"\u003E3143\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E3153\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Niu, J., Lu, W. and Penn, G. (2022). Does BERT rediscover a classical NLP pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea: International Committee on Computational Linguistics, 3143–3153.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Niu,+J.,+Lu,+W.+and+Penn,+G.+(2022).+Does+BERT+rediscover+a+classical+NLP+pipeline?+In+Proceedings+of+the+29th+International+Conference+on+Computational+Linguistics,+Gyeongju,+Republic+of+Korea:+International+Committee+on+Computational+Linguistics,+3143–3153.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Niu,+J.,+Lu,+W.+and+Penn,+G.+(2022).+Does+BERT+rediscover+a+classical+NLP+pipeline?+In+Proceedings+of+the+29th+International+Conference+on+Computational+Linguistics,+Gyeongju,+Republic+of+Korea:+International+Committee+on+Computational+Linguistics,+3143–3153.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r46",title:"Niu, J., Lu, W. and Penn, G. (2022). Does BERT rediscover a classical NLP pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, Gyeongju, Republic of Korea: International Committee on Computational Linguistics, 3143–3153.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref47",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPedregosa\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVaroquaux\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGramfort\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMichel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EThirion\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGrisel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBlondel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPrettenhofer\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWeiss\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDubourg\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVanderplas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPassos\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECournapeau\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBrucher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPerrot\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDuchesnay\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2011\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EScikit-learn: machine learning in python\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EJournal of Machine Learning Research\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E12\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E2825\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E2830\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Scikit-learn: machine learning in python' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Scikit-learn%3A+machine+learning+in+python&author=Pedregosa+F.&author=Varoquaux+G.&author=Gramfort+A.&author=Michel+V.&author=Thirion+B.&author=Grisel+O.&author=Blondel+M.&author=Prettenhofer+P.&author=Weiss+R.&author=Dubourg+V.&author=Vanderplas+J.&author=Passos+A.&author=Cournapeau+D.&author=Brucher+M.&author=Perrot+M.&author=Duchesnay+E.&publication+year=2011&journal=Journal+of+Machine+Learning+Research&volume=12&pages=2825-2830\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Scikit-learn%3A+machine+learning+in+python&author=Pedregosa+F.&author=Varoquaux+G.&author=Gramfort+A.&author=Michel+V.&author=Thirion+B.&author=Grisel+O.&author=Blondel+M.&author=Prettenhofer+P.&author=Weiss+R.&author=Dubourg+V.&author=Vanderplas+J.&author=Passos+A.&author=Cournapeau+D.&author=Brucher+M.&author=Perrot+M.&author=Duchesnay+E.&publication+year=2011&journal=Journal+of+Machine+Learning+Research&volume=12&pages=2825-2830",openUrlParams:{genre:j,atitle:al,jtitle:am,title:am,volume:"12",artnum:"671a725f21c8183ee4264ae9",spage:"2825",epage:"2830",date:"2011",sid:e,aulast:a,aufirst:a,doi:a,au:a},innerRefId:"r47",title:al,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref48",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPeng\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFeldman\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVylomova\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2014\u003C\u002Fspan\u003E). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In \u003Cem class=\"italic\"\u003EProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EDoha, Qatar\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E2019\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E2027\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.' href=https:\u002F\u002Fdx.doi.org\u002F10.3115\u002Fv1\u002FD14-1216\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Peng,+J.,+Feldman,+A.+and+Vylomova,+E.+(2014).+Classifying+idiomatic+and+literal+expressions+using+topic+models+and+intensity+of+emotions.+In+Proceedings+of+the+2014+Conference+on+Empirical+Methods+in+Natural+Language+Processing+(EMNLP),+Doha,+Qatar:+Association+for+Computational+Linguistics,+pp.+2019–2027.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Peng,+J.,+Feldman,+A.+and+Vylomova,+E.+(2014).+Classifying+idiomatic+and+literal+expressions+using+topic+models+and+intensity+of+emotions.+In+Proceedings+of+the+2014+Conference+on+Empirical+Methods+in+Natural+Language+Processing+(EMNLP),+Doha,+Qatar:+Association+for+Computational+Linguistics,+pp.+2019–2027.",openUrlParams:{genre:g,date:an,sid:e,title:d},innerRefId:"r48",title:"Peng, J., Feldman, A. and Vylomova, E. (2014). Classifying idiomatic and literal expressions using topic models and intensity of emotions. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar: Association for Computational Linguistics, pp. 2019–2027.",doi:"10.3115\u002Fv1\u002FD14-1216",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.3115\u002Fv1\u002FD14-1216",pubMedLink:a}]},{id:"ref49",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPennington\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESocher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2014\u003C\u002Fspan\u003E). Glove: Global vectors for word representation. In \u003Cem class=\"italic\"\u003EProceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)\u003C\u002Fem\u003E, pp. \u003Cspan class=\"fpage\"\u003E1532\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E1543\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.' href=https:\u002F\u002Fdx.doi.org\u002F10.3115\u002Fv1\u002FD14-1162\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Pennington,+J.,+Socher,+R.+and+Manning,+C.+(2014).+Glove:+Global+vectors+for+word+representation.+In+Proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(EMNLP),+pp.+1532–1543.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Pennington,+J.,+Socher,+R.+and+Manning,+C.+(2014).+Glove:+Global+vectors+for+word+representation.+In+Proceedings+of+the+2014+conference+on+empirical+methods+in+natural+language+processing+(EMNLP),+pp.+1532–1543.",openUrlParams:{genre:g,date:an,sid:e,title:d},innerRefId:"r49",title:"Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543.",doi:"10.3115\u002Fv1\u002FD14-1162",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.3115\u002Fv1\u002FD14-1162",pubMedLink:a}]},{id:"ref50",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPham\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBui\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMai\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENguyen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003EOut of order: how important is the sequential order of words in a sentence in natural language understanding tasks?\u003C\u002Fspan\u003E. In \u003Cspan class=\"source\"\u003EFindings of the Association for Computational Linguistics\u003C\u002Fspan\u003E. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E1145\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E1160\u003C\u002Fspan\u003E,\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Findings of the Association for Computational Linguistics' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics&author=Pham+T.&author=Bui+T.&author=Mai+L.&author=Nguyen+A.&publication+year=2021&pages=1145-1160\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Findings+of+the+Association+for+Computational+Linguistics&author=Pham+T.&author=Bui+T.&author=Mai+L.&author=Nguyen+A.&publication+year=2021&pages=1145-1160",openUrlParams:{genre:n,btitle:C,title:C,atitle:"Out of order: how important is the sequential order of words in a sentence in natural language understanding tasks?",aulast:a,aufirst:a,au:a,pub:d,date:m,spage:"1145",epage:"1160",doi:a,sid:e},innerRefId:"r50",title:C,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref51",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPimentel\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EValvoda\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStoehr\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECotterell\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). The architectural bottleneck principle.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Pimentel, T., Valvoda, J., Stoehr, N. and Cotterell, R. (2022). The architectural bottleneck principle.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Pimentel,+T.,+Valvoda,+J.,+Stoehr,+N.+and+Cotterell,+R.+(2022).+The+architectural+bottleneck+principle.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Pimentel,+T.,+Valvoda,+J.,+Stoehr,+N.+and+Cotterell,+R.+(2022).+The+architectural+bottleneck+principle.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r51",title:"Pimentel, T., Valvoda, J., Stoehr, N. and Cotterell, R. (2022). The architectural bottleneck principle.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref52",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERaganato\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETiedemann\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2018\u003C\u002Fspan\u003E). An analysis of encoder representations in transformer-based machine translation. In \u003Cem class=\"italic\"\u003EProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EBrussels, Belgium\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E287\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E297\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW18-5431\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Raganato,+A.+and+Tiedemann,+J.+(2018).+An+analysis+of+encoder+representations+in+transformer-based+machine+translation.+In+Proceedings+of+the+2018+EMNLP+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Brussels,+Belgium:+Association+for+Computational+Linguistics,+pp.+287–297.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Raganato,+A.+and+Tiedemann,+J.+(2018).+An+analysis+of+encoder+representations+in+transformer-based+machine+translation.+In+Proceedings+of+the+2018+EMNLP+Workshop+BlackboxNLP:+Analyzing+and+Interpreting+Neural+Networks+for+NLP,+Brussels,+Belgium:+Association+for+Computational+Linguistics,+pp.+287–297.",openUrlParams:{genre:g,date:q,sid:e,title:d},innerRefId:"r52",title:"Raganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in transformer-based machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, Brussels, Belgium: Association for Computational Linguistics, pp. 287–297.",doi:"10.18653\u002Fv1\u002FW18-5431",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW18-5431",pubMedLink:a}]},{id:"ref53",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECordeiro\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. R.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESavary\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVincze\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBarbu Mititelu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBhatia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBuljan\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECandito\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGantar\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGiouli\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGüngör\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHawwari\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EIñurrieta\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EU.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKovalevskaitė\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKrek\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELichte\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiebeskind\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMonti\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EParra Escartín\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EQasemiZadeh\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESchneider\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStoyanova\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVaidya\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWalsh\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2018\u003C\u002Fspan\u003E). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In \u003Cem class=\"italic\"\u003EProceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ESanta Fe, New Mexico, USA\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E222\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E240\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Ramisch, C., Cordeiro, S. R., Savary, A., Vincze, V., Barbu Mititelu, V., Bhatia, A., Buljan, M., Candito, M., Gantar, P., Giouli, V., Güngör, T., Hawwari, A., Iñurrieta, U., Kovalevskaitė, J., Krek, S., Lichte, T., Liebeskind, C., Monti, J., Parra Escartín, C., QasemiZadeh, B., Ramisch, R., Schneider, N., Stoyanova, I., Vaidya, A. and Walsh, A. (2018). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Santa Fe, New Mexico, USA: Association for Computational Linguistics, pp. 222–240.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Ramisch,+C.,+Cordeiro,+S.+R.,+Savary,+A.,+Vincze,+V.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Buljan,+M.,+Candito,+M.,+Gantar,+P.,+Giouli,+V.,+Güngör,+T.,+Hawwari,+A.,+Iñurrieta,+U.,+Kovalevskaitė,+J.,+Krek,+S.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Parra+Escartín,+C.,+QasemiZadeh,+B.,+Ramisch,+R.,+Schneider,+N.,+Stoyanova,+I.,+Vaidya,+A.+and+Walsh,+A.+(2018).+Edition+1.1+of+the+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Linguistic+Annotation,+Multiword+Expressions+and+Constructions+(LAW-MWE-CxG-2018),+Santa+Fe,+New+Mexico,+USA:+Association+for+Computational+Linguistics,+pp.+222–240.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Ramisch,+C.,+Cordeiro,+S.+R.,+Savary,+A.,+Vincze,+V.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Buljan,+M.,+Candito,+M.,+Gantar,+P.,+Giouli,+V.,+Güngör,+T.,+Hawwari,+A.,+Iñurrieta,+U.,+Kovalevskaitė,+J.,+Krek,+S.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Parra+Escartín,+C.,+QasemiZadeh,+B.,+Ramisch,+R.,+Schneider,+N.,+Stoyanova,+I.,+Vaidya,+A.+and+Walsh,+A.+(2018).+Edition+1.1+of+the+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Linguistic+Annotation,+Multiword+Expressions+and+Constructions+(LAW-MWE-CxG-2018),+Santa+Fe,+New+Mexico,+USA:+Association+for+Computational+Linguistics,+pp.+222–240.",openUrlParams:{genre:g,date:q,sid:e,title:d},innerRefId:"r53",title:"Ramisch, C., Cordeiro, S. R., Savary, A., Vincze, V., Barbu Mititelu, V., Bhatia, A., Buljan, M., Candito, M., Gantar, P., Giouli, V., Güngör, T., Hawwari, A., Iñurrieta, U., Kovalevskaitė, J., Krek, S., Lichte, T., Liebeskind, C., Monti, J., Parra Escartín, C., QasemiZadeh, B., Ramisch, R., Schneider, N., Stoyanova, I., Vaidya, A. and Walsh, A. (2018). Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Santa Fe, New Mexico, USA: Association for Computational Linguistics, pp. 222–240.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref54",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESavary\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGuillaume\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWaszczuk\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECandito\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVaidya\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBarbu Mititelu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBhatia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EIñurrieta\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EU.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGiouli\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGüngör\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJiang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELichte\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiebeskind\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMonti\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStymne\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWalsh\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EXu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EH.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In \u003Cem class=\"italic\"\u003EProceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons\u003C\u002Fem\u003E, Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E107\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E118\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Ramisch, C., Savary, A., Guillaume, B., Waszczuk, J., Candito, M., Vaidya, A., Barbu Mititelu, V., Bhatia, A., Iñurrieta, U., Giouli, V., Güngör, T., Jiang, M., Lichte, T., Liebeskind, C., Monti, J., Ramisch, R., Stymne, S., Walsh, A. and Xu, H. (2020). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons, Association for Computational Linguistics, pp. 107–118.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Ramisch,+C.,+Savary,+A.,+Guillaume,+B.,+Waszczuk,+J.,+Candito,+M.,+Vaidya,+A.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Iñurrieta,+U.,+Giouli,+V.,+Güngör,+T.,+Jiang,+M.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Ramisch,+R.,+Stymne,+S.,+Walsh,+A.+and+Xu,+H.+(2020).+Edition+1.2+of+the+PARSEME+shared+task+on+semi-supervised+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Multiword+Expressions+and+Electronic+Lexicons,+Association+for+Computational+Linguistics,+pp.+107–118.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Ramisch,+C.,+Savary,+A.,+Guillaume,+B.,+Waszczuk,+J.,+Candito,+M.,+Vaidya,+A.,+Barbu+Mititelu,+V.,+Bhatia,+A.,+Iñurrieta,+U.,+Giouli,+V.,+Güngör,+T.,+Jiang,+M.,+Lichte,+T.,+Liebeskind,+C.,+Monti,+J.,+Ramisch,+R.,+Stymne,+S.,+Walsh,+A.+and+Xu,+H.+(2020).+Edition+1.2+of+the+PARSEME+shared+task+on+semi-supervised+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+Joint+Workshop+on+Multiword+Expressions+and+Electronic+Lexicons,+Association+for+Computational+Linguistics,+pp.+107–118.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r54",title:"Ramisch, C., Savary, A., Guillaume, B., Waszczuk, J., Candito, M., Vaidya, A., Barbu Mititelu, V., Bhatia, A., Iñurrieta, U., Giouli, V., Güngör, T., Jiang, M., Lichte, T., Liebeskind, C., Monti, J., Ramisch, R., Stymne, S., Walsh, A. and Xu, H. (2020). Edition 1.2 of the PARSEME shared task on semi-supervised identification of verbal multiword expressions. In Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons, Association for Computational Linguistics, pp. 107–118.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref55",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EReif\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EYuan\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWattenberg\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EViegas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF. B.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECoenen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPearce\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKim\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003EVisualizing and measuring the geometry of bert\u003C\u002Fspan\u003E. In Wallach H., Larochelle H., Beygelzimer A., d’Alché-Buc F., Fox E. and Garnett R., (eds), \u003Cspan class=\"source\"\u003EAdvances in Neural Information Processing Systems\u003C\u002Fspan\u003E, \u003Cspan class=\"volume\"\u003E32\u003C\u002Fspan\u003E, Curran Associates, Inc.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Advances in Neural Information Processing Systems' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Reif+E.&author=Yuan+A.&author=Wattenberg+M.&author=Viegas+F.+B.&author=Coenen+A.&author=Pearce+A.&author=Kim+B.&publication+year=2019\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Advances+in+Neural+Information+Processing+Systems&author=Reif+E.&author=Yuan+A.&author=Wattenberg+M.&author=Viegas+F.+B.&author=Coenen+A.&author=Pearce+A.&author=Kim+B.&publication+year=2019",openUrlParams:{genre:n,btitle:o,title:o,atitle:"Visualizing and measuring the geometry of bert",aulast:a,aufirst:a,au:a,pub:d,date:h,spage:d,epage:d,doi:a,sid:e},innerRefId:"r55",title:o,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref56",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERogers\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKovaleva\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EO.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERumshisky\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003EA primer in BERTology: what we know about how BERT works\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003ETransactions of the Association for Computational Linguistics\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E8\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E842\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E866\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for A primer in BERTology: what we know about how BERT works' href=https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Ftacl_a_00349\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for A primer in BERTology: what we know about how BERT works' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=A+primer+in+BERTology%3A+what+we+know+about+how+BERT+works&author=Rogers+A.&author=Kovaleva+O.&author=Rumshisky+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00349&pages=842-866\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=A+primer+in+BERTology%3A+what+we+know+about+how+BERT+works&author=Rogers+A.&author=Kovaleva+O.&author=Rumshisky+A.&publication+year=2020&journal=Transactions+of+the+Association+for+Computational+Linguistics&volume=8&doi=10.1162%2Ftacl_a_00349&pages=842-866",openUrlParams:{genre:j,atitle:ao,jtitle:s,title:s,volume:T,artnum:"671a725f21c8183ee4264af2",spage:"842",epage:"866",date:i,sid:e,aulast:a,aufirst:a,doi:ap,au:a},innerRefId:"r56",title:ao,doi:ap,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1162\u002Ftacl_a_00349",pubMedLink:a}]},{id:"ref57",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERosa\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMareček\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). Inducing syntactic trees from BERT representations, arXiv, abs\u002F1906.11511.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Rosa, R. and Mareček, D. (2019). Inducing syntactic trees from BERT representations, arXiv, abs\u002F1906.11511.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Rosa,+R.+and+Mareček,+D.+(2019).+Inducing+syntactic+trees+from+BERT+representations,+arXiv,+abs\u002F1906.11511.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Rosa,+R.+and+Mareček,+D.+(2019).+Inducing+syntactic+trees+from+BERT+representations,+arXiv,+abs\u002F1906.11511.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r57",title:"Rosa, R. and Mareček, D. (2019). Inducing syntactic trees from BERT representations, arXiv, abs\u002F1906.11511.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref58",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESag\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI. A.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBaldwin\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBond\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECopestake\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA. A.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EFlickinger\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2002\u003C\u002Fspan\u003E). Multiword expressions: A pain in the neck for nlp. In \u003Cem class=\"italic\"\u003EProceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing\u003C\u002Fem\u003E, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. \u003Cspan class=\"fpage\"\u003E1\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E15\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.' href=https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F3-540-45715-1_1\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sag,+I.+A.,+Baldwin,+T.,+Bond,+F.,+Copestake,+A.+A.+and+Flickinger,+D.+(2002).+Multiword+expressions:+A+pain+in+the+neck+for+nlp.+In+Proceedings+of+the+Third+International+Conference+on+Computational+Linguistics+and+Intelligent+Text+Processing,+Berlin,+Heidelberg:+Springer-Verlag,+vol+CICLing+’02,+pp.+1–15.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sag,+I.+A.,+Baldwin,+T.,+Bond,+F.,+Copestake,+A.+A.+and+Flickinger,+D.+(2002).+Multiword+expressions:+A+pain+in+the+neck+for+nlp.+In+Proceedings+of+the+Third+International+Conference+on+Computational+Linguistics+and+Intelligent+Text+Processing,+Berlin,+Heidelberg:+Springer-Verlag,+vol+CICLing+’02,+pp.+1–15.",openUrlParams:{genre:g,date:"2002",sid:e,title:d},innerRefId:"r58",title:"Sag, I. A., Baldwin, T., Bond, F., Copestake, A. A. and Flickinger, D. (2002). Multiword expressions: A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, Berlin, Heidelberg: Springer-Verlag, vol CICLing ’02, pp. 1–15.",doi:"10.1007\u002F3-540-45715-1_1",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1007\u002F3-540-45715-1_1",pubMedLink:a}]},{id:"ref59",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESalton\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EG.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERoss\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKelleher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2016\u003C\u002Fspan\u003E). Idiom token classification using sentential distributed semantics. In \u003Cem class=\"italic\"\u003EProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EBerlin, Germany\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E194\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E204\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP16-1019\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Salton,+G.,+Ross,+R.+and+Kelleher,+J.+(2016).+Idiom+token+classification+using+sentential+distributed+semantics.+In+Proceedings+of+the+54th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Berlin,+Germany:+Association+for+Computational+Linguistics,+pp.+194–204.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Salton,+G.,+Ross,+R.+and+Kelleher,+J.+(2016).+Idiom+token+classification+using+sentential+distributed+semantics.+In+Proceedings+of+the+54th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+(Volume+1:+Long+Papers),+Berlin,+Germany:+Association+for+Computational+Linguistics,+pp.+194–204.",openUrlParams:{genre:g,date:aq,sid:e,title:d},innerRefId:"r59",title:"Salton, G., Ross, R. and Kelleher, J. (2016). Idiom token classification using sentential distributed semantics. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany: Association for Computational Linguistics, pp. 194–204.",doi:"10.18653\u002Fv1\u002FP16-1019",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP16-1019",pubMedLink:a}]},{id:"ref60",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESavary\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECordeiro\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. R.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELichte\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ET.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003Enurrieta\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EU. I.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGiouli\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003E). \u003Cspan class=\"article-title\"\u003ELiteral occurrences of multiword expressions: rare birds that cause a stir\u003C\u002Fspan\u003E. \u003Cspan class=\"source\"\u003EThe Prague Bulletin of Mathematical Linguistics\u003C\u002Fspan\u003E \u003Cspan class=\"volume\"\u003E112\u003C\u002Fspan\u003E, \u003Cspan class=\"fpage\"\u003E5\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E54\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Literal occurrences of multiword expressions: rare birds that cause a stir' href=https:\u002F\u002Fdx.doi.org\u002F10.2478\u002Fpralin-2019-0001\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Literal occurrences of multiword expressions: rare birds that cause a stir' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Literal+occurrences+of+multiword+expressions%3A+rare+birds+that+cause+a+stir&author=Savary+A.&author=Cordeiro+S.+R.&author=Lichte+T.&author=Ramisch+C.&author=nurrieta+U.+I.&author=Giouli+V.&publication+year=2019&journal=The+Prague+Bulletin+of+Mathematical+Linguistics&volume=112&doi=10.2478%2Fpralin-2019-0001&pages=5-54\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=Literal+occurrences+of+multiword+expressions%3A+rare+birds+that+cause+a+stir&author=Savary+A.&author=Cordeiro+S.+R.&author=Lichte+T.&author=Ramisch+C.&author=nurrieta+U.+I.&author=Giouli+V.&publication+year=2019&journal=The+Prague+Bulletin+of+Mathematical+Linguistics&volume=112&doi=10.2478%2Fpralin-2019-0001&pages=5-54",openUrlParams:{genre:j,atitle:ar,jtitle:as,title:as,volume:"112",artnum:"671a725f21c8183ee4264af6",spage:v,epage:ad,date:h,sid:e,aulast:a,aufirst:a,doi:at,au:a},innerRefId:"r60",title:ar,doi:at,crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.2478\u002Fpralin-2019-0001",pubMedLink:a}]},{id:"ref61",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESavary\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ERamisch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECordeiro\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESangati\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVincze\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EQasemiZadeh\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECandito\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECap\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EF.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGiouli\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EV.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStoyanova\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDoucet\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2017\u003C\u002Fspan\u003E). The PARSEME shared task on automatic identification of verbal multiword expressions. In \u003Cem class=\"italic\"\u003EProceedings of the 13th Workshop on Multiword Expressions (MWE 2017)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EValencia, Spain\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E31\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E47\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW17-1704\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Savary,+A.,+Ramisch,+C.,+Cordeiro,+S.,+Sangati,+F.,+Vincze,+V.,+QasemiZadeh,+B.,+Candito,+M.,+Cap,+F.,+Giouli,+V.,+Stoyanova,+I.+and+Doucet,+A.+(2017).+The+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+13th+Workshop+on+Multiword+Expressions+(MWE+2017),+Valencia,+Spain:+Association+for+Computational+Linguistics,+pp.+31–47.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Savary,+A.,+Ramisch,+C.,+Cordeiro,+S.,+Sangati,+F.,+Vincze,+V.,+QasemiZadeh,+B.,+Candito,+M.,+Cap,+F.,+Giouli,+V.,+Stoyanova,+I.+and+Doucet,+A.+(2017).+The+PARSEME+shared+task+on+automatic+identification+of+verbal+multiword+expressions.+In+Proceedings+of+the+13th+Workshop+on+Multiword+Expressions+(MWE+2017),+Valencia,+Spain:+Association+for+Computational+Linguistics,+pp.+31–47.",openUrlParams:{genre:g,date:w,sid:e,title:d},innerRefId:"r61",title:"Savary, A., Ramisch, C., Cordeiro, S., Sangati, F., Vincze, V., QasemiZadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I. and Doucet, A. (2017). The PARSEME shared task on automatic identification of verbal multiword expressions. In Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), Valencia, Spain: Association for Computational Linguistics, pp. 31–47.",doi:"10.18653\u002Fv1\u002FW17-1704",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FW17-1704",pubMedLink:a}]},{id:"ref62",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESchneider\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHovy\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJohannsen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ECarpuat\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2016\u003C\u002Fspan\u003E). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In \u003Cem class=\"italic\"\u003EProceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ESan Diego, California\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E546\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E559\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Schneider, N., Hovy, D., Johannsen, A. and Carpuat, M. (2016). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California: Association for Computational Linguistics, pp. 546–559.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Schneider,+N.,+Hovy,+D.,+Johannsen,+A.+and+Carpuat,+M.+(2016).+SemEval-.+2016+task+10:+Detecting+minimal+semantic+units+and+their+meanings+(DiMSUM).+In+Proceedings+of+the+10th+International+Workshop+on+Semantic+Evaluation+(SemEval-2016),+San+Diego,+California:+Association+for+Computational+Linguistics,+pp.+546–559.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Schneider,+N.,+Hovy,+D.,+Johannsen,+A.+and+Carpuat,+M.+(2016).+SemEval-.+2016+task+10:+Detecting+minimal+semantic+units+and+their+meanings+(DiMSUM).+In+Proceedings+of+the+10th+International+Workshop+on+Semantic+Evaluation+(SemEval-2016),+San+Diego,+California:+Association+for+Computational+Linguistics,+pp.+546–559.",openUrlParams:{genre:g,date:aq,sid:e,title:d},innerRefId:"r62",title:"Schneider, N., Hovy, D., Johannsen, A. and Carpuat, M. (2016). SemEval-. 2016 task 10: Detecting minimal semantic units and their meanings (DiMSUM). In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California: Association for Computational Linguistics, pp. 546–559.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref63",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESinha\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EJia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EHupkes\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPineau\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWilliams\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKiela\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003Ea). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In \u003Cem class=\"italic\"\u003EProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EOnline and Punta Cana, Dominican Republic\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E2888\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E2913\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.emnlp-main.230\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sinha,+K.,+Jia,+R.,+Hupkes,+D.,+Pineau,+J.,+Williams,+A.+and+Kiela,+D.+(2021a).+Masked+language+modeling+and+the+distributional+hypothesis:+Order+word+matters+pre-training+for+little.+In+Proceedings+of+the+2021+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Online+and+Punta+Cana,+Dominican+Republic:+Association+for+Computational+Linguistics,+pp.+2888–2913.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sinha,+K.,+Jia,+R.,+Hupkes,+D.,+Pineau,+J.,+Williams,+A.+and+Kiela,+D.+(2021a).+Masked+language+modeling+and+the+distributional+hypothesis:+Order+word+matters+pre-training+for+little.+In+Proceedings+of+the+2021+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Online+and+Punta+Cana,+Dominican+Republic:+Association+for+Computational+Linguistics,+pp.+2888–2913.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r63",title:"Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A. and Kiela, D. (2021a). Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, pp. 2888–2913.",doi:"10.18653\u002Fv1\u002F2021.emnlp-main.230",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.emnlp-main.230",pubMedLink:a}]},{id:"ref64",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESinha\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EK.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EParthasarathi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPineau\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWilliams\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2021\u003C\u002Fspan\u003Eb). UnNatural Language Inference. In \u003Cem class=\"italic\"\u003EProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E7329\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E7346\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.acl-long.569\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sinha,+K.,+Parthasarathi,+P.,+Pineau,+J.+and+Williams,+A.+(2021b).+UnNatural+Language+Inference.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+1:+Long+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+7329–7346.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Sinha,+K.,+Parthasarathi,+P.,+Pineau,+J.+and+Williams,+A.+(2021b).+UnNatural+Language+Inference.+In+Proceedings+of+the+59th+Annual+Meeting+of+the+Association+for+Computational+Linguistics+and+the+11th+International+Joint+Conference+on+Natural+Language+Processing+(Volume+1:+Long+Papers),+Online.+Association+for+Computational+Linguistics,+pp.+7329–7346.",openUrlParams:{genre:g,date:m,sid:e,title:d},innerRefId:"r64",title:"Sinha, K., Parthasarathi, P., Pineau, J. and Williams, A. (2021b). UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), Online. Association for Computational Linguistics, pp. 7329–7346.",doi:"10.18653\u002Fv1\u002F2021.acl-long.569",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2021.acl-long.569",pubMedLink:a}]},{id:"ref65",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESocher\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPerelygin\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChuang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EManning\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC. D.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ENg\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPotts\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2013\u003C\u002Fspan\u003E). Recursive deep models for semantic compositionality over a sentiment treebank. In \u003Cem class=\"italic\"\u003EProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003ESeattle, Washington, USA\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E1631\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E1642\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA: Association for Computational Linguistics, pp. 1631–1642.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Socher,+R.,+Perelygin,+A.,+Wu,+J.,+Chuang,+J.,+Manning,+C.+D.,+Ng,+A.+and+Potts,+C.+(2013).+Recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+In+Proceedings+of+the+2013+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Seattle,+Washington,+USA:+Association+for+Computational+Linguistics,+pp.+1631–1642.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Socher,+R.,+Perelygin,+A.,+Wu,+J.,+Chuang,+J.,+Manning,+C.+D.,+Ng,+A.+and+Potts,+C.+(2013).+Recursive+deep+models+for+semantic+compositionality+over+a+sentiment+treebank.+In+Proceedings+of+the+2013+Conference+on+Empirical+Methods+in+Natural+Language+Processing,+Seattle,+Washington,+USA:+Association+for+Computational+Linguistics,+pp.+1631–1642.",openUrlParams:{genre:g,date:Y,sid:e,title:d},innerRefId:"r65",title:"Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA: Association for Computational Linguistics, pp. 1631–1642.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref66",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESporleder\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGorinski\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKoch\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EX.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2010\u003C\u002Fspan\u003E). \u003Cspan class=\"chapter-title\"\u003EIdioms in context: the idix corpus\u003C\u002Fspan\u003E. In \u003Cspan class=\"source\"\u003ELREC\u003C\u002Fspan\u003E\n\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for LREC' href=https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=LREC&author=Sporleder+C.&author=Li+L.&author=Gorinski+P.&author=Koch+X.&publication+year=2010\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar_lookup?title=LREC&author=Sporleder+C.&author=Li+L.&author=Gorinski+P.&author=Koch+X.&publication+year=2010",openUrlParams:{genre:n,btitle:D,title:D,atitle:"Idioms in context: the idix corpus",aulast:a,aufirst:a,au:a,pub:d,date:A,spage:d,epage:d,doi:a,sid:e},innerRefId:"r66",title:D,doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref67",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETayyar Madabushi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EH.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGow-Smith\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EGarcia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EScarton\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EIdiart\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVillavicencio\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2022\u003C\u002Fspan\u003E). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In \u003Cem class=\"italic\"\u003EProceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)\u003C\u002Fem\u003E, Seattle, United States. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E107\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E121\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Tayyar Madabushi, H., Gow-Smith, E., Garcia, M., Scarton, C., Idiart, M. and Villavicencio, A. (2022). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Seattle, United States. Association for Computational Linguistics, pp. 107–121.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tayyar+Madabushi,+H.,+Gow-Smith,+E.,+Garcia,+M.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2022).+SemEval-.+2022+task+2:+Multilingual+idiomaticity+detection+and+sentence+embedding.+In+Proceedings+of+the+16th+International+Workshop+on+Semantic+Evaluation+(SemEval-2022),+Seattle,+United+States.+Association+for+Computational+Linguistics,+pp.+107–121.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tayyar+Madabushi,+H.,+Gow-Smith,+E.,+Garcia,+M.,+Scarton,+C.,+Idiart,+M.+and+Villavicencio,+A.+(2022).+SemEval-.+2022+task+2:+Multilingual+idiomaticity+detection+and+sentence+embedding.+In+Proceedings+of+the+16th+International+Workshop+on+Semantic+Evaluation+(SemEval-2022),+Seattle,+United+States.+Association+for+Computational+Linguistics,+pp.+107–121.",openUrlParams:{genre:g,date:l,sid:e,title:d},innerRefId:"r67",title:"Tayyar Madabushi, H., Gow-Smith, E., Garcia, M., Scarton, C., Idiart, M. and Villavicencio, A. (2022). SemEval-. 2022 task 2: Multilingual idiomaticity detection and sentence embedding. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), Seattle, United States. Association for Computational Linguistics, pp. 107–121.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref68",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETenney\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPavlick\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Ea). BERT rediscovers the classical NLP pipeline. In \u003Cem class=\"italic\"\u003EProceedings of the 57th Annual Meeting of the Association for Computational Linguistics\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EFlorence, Italy\u003C\u002Fspan\u003E: Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E4593\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E4601\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP19-1452\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tenney,+I.,+Das,+D.+and+Pavlick,+E.+(2019a).+BERT+rediscovers+the+classical+NLP+pipeline.+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+4593–4601.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tenney,+I.,+Das,+D.+and+Pavlick,+E.+(2019a).+BERT+rediscovers+the+classical+NLP+pipeline.+In+Proceedings+of+the+57th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Florence,+Italy:+Association+for+Computational+Linguistics,+pp.+4593–4601.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r68",title:"Tenney, I., Das, D. and Pavlick, E. (2019a). BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy: Association for Computational Linguistics, pp. 4593–4601.",doi:"10.18653\u002Fv1\u002FP19-1452",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002FP19-1452",pubMedLink:a}]},{id:"ref69",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ETenney\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EI.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EXia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EP.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPoliak\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EMcCoy\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ER. T.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKim\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EN.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDurme\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB. V.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBowman\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ES. R.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EDas\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPavlick\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EE.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2019\u003C\u002Fspan\u003Eb). What do you learn from context? probing for sentence structure in contextualized word representations. \u003Cspan class=\"source\"\u003ECoRR\u003C\u002Fspan\u003E, abs\u002F1905.06316.\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D. and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs\u002F1905.06316.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tenney,+I.,+Xia,+P.,+Chen,+B.,+Wang,+A.,+Poliak,+A.,+McCoy,+R.+T.,+Kim,+N.,+Durme,+B.+V.,+Bowman,+S.+R.,+Das,+D.+and+Pavlick,+E.+(2019b).+What+do+you+learn+from+context?+probing+for+sentence+structure+in+contextualized+word+representations.+CoRR,+abs\u002F1905.06316.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Tenney,+I.,+Xia,+P.,+Chen,+B.,+Wang,+A.,+Poliak,+A.,+McCoy,+R.+T.,+Kim,+N.,+Durme,+B.+V.,+Bowman,+S.+R.,+Das,+D.+and+Pavlick,+E.+(2019b).+What+do+you+learn+from+context?+probing+for+sentence+structure+in+contextualized+word+representations.+CoRR,+abs\u002F1905.06316.",openUrlParams:{genre:g,date:h,sid:e,title:d},innerRefId:"r69",title:"Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., Kim, N., Durme, B. V., Bowman, S. R., Das, D. and Pavlick, E. (2019b). What do you learn from context? probing for sentence structure in contextualized word representations. CoRR, abs\u002F1905.06316.",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref70",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EVilares\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003ED.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EStrzyz\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESøgaard\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EA.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EG’omez-Rodr’iguez\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Parsing as pretraining. In \u003Cem class=\"italic\"\u003EAAAI Conference on Artificial Intelligence\u003C\u002Fem\u003E\n\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence' href=https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v34i05.6446\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Vilares,+D.,+Strzyz,+M.,+Søgaard,+A.+and+G’omez-Rodr’iguez,+C.+(2020).+Parsing+as+pretraining.+In+AAAI+Conference+on+Artificial+Intelligence\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Vilares,+D.,+Strzyz,+M.,+Søgaard,+A.+and+G’omez-Rodr’iguez,+C.+(2020).+Parsing+as+pretraining.+In+AAAI+Conference+on+Artificial+Intelligence",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r70",title:"Vilares, D., Strzyz, M., Søgaard, A. and G’omez-Rodr’iguez, C. (2020). Parsing as pretraining. In AAAI Conference on Artificial Intelligence",doi:"10.1609\u002Faaai.v34i05.6446",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.1609\u002Faaai.v34i05.6446",pubMedLink:a}]},{id:"ref71",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWang\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EW.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EYan\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EM.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EC.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EXia\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EJ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EBao\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EZ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EPeng\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ESi\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EL.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Structbert: Incorporating language structures into pre-training for deep language understanding. In \u003Cem class=\"italic\"\u003E8th International Conference on Learning Representations, ICLR\u003C\u002Fem\u003E, \u003Cspan class=\"publisher-loc\"\u003EEthiopia\u003C\u002Fspan\u003E: Addis Ababa, OpenReview.net,\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Wang, W., Bi, B., Yan, M., Wu, C., Xia, J., Bao, Z., Peng, L. and Si, L. (2020). Structbert: Incorporating language structures into pre-training for deep language understanding. In 8th International Conference on Learning Representations, ICLR, Ethiopia: Addis Ababa, OpenReview.net,' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Wang,+W.,+Bi,+B.,+Yan,+M.,+Wu,+C.,+Xia,+J.,+Bao,+Z.,+Peng,+L.+and+Si,+L.+(2020).+Structbert:+Incorporating+language+structures+into+pre-training+for+deep+language+understanding.+In+8th+International+Conference+on+Learning+Representations,+ICLR,+Ethiopia:+Addis+Ababa,+OpenReview.net,\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Wang,+W.,+Bi,+B.,+Yan,+M.,+Wu,+C.,+Xia,+J.,+Bao,+Z.,+Peng,+L.+and+Si,+L.+(2020).+Structbert:+Incorporating+language+structures+into+pre-training+for+deep+language+understanding.+In+8th+International+Conference+on+Learning+Representations,+ICLR,+Ethiopia:+Addis+Ababa,+OpenReview.net,",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r71",title:"Wang, W., Bi, B., Yan, M., Wu, C., Xia, J., Bao, Z., Peng, L. and Si, L. (2020). Structbert: Incorporating language structures into pre-training for deep language understanding. In 8th International Conference on Learning Representations, ICLR, Ethiopia: Addis Ababa, OpenReview.net,",doi:a,crossRefLink:a,pubMedLink:a}]},{id:"ref72",displayNumber:a,existInContent:c,content:"\u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EWu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EZ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EChen\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EY.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E, \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003EKao\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EB.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E and \u003Cspan class=\"string-name\"\u003E\u003Cspan class=\"surname\"\u003ELiu\u003C\u002Fspan\u003E, \u003Cspan class=\"given-names\"\u003EQ.\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E (\u003Cspan class=\"year\"\u003E2020\u003C\u002Fspan\u003E). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In \u003Cem class=\"italic\"\u003EProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\u003C\u002Fem\u003E, Online. Association for Computational Linguistics, pp. \u003Cspan class=\"fpage\"\u003E4166\u003C\u002Fspan\u003E–\u003Cspan class=\"lpage\"\u003E4176\u003C\u002Fspan\u003E.\u003Ca class='ref-link' target='_blank' aria-label='CrossRef link for Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.' href=https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2020.acl-main.383\u003ECrossRef\u003C\u002Fa\u003E\u003Ca class='ref-link' target='_blank' aria-label='Google Scholar link for Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.' href=https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Wu,+Z.,+Chen,+Y.,+Kao,+B.+and+Liu,+Q.+(2020).+Perturbed+masking:+Parameter-free+probing+for+analyzing+and+interpreting+BERT.+In+Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Online.+Association+for+Computational+Linguistics,+pp.+4166–4176.\u003EGoogle Scholar\u003C\u002Fa\u003E",item:[{googleScholarLink:"https:\u002F\u002Fscholar.google.com\u002Fscholar?q=Wu,+Z.,+Chen,+Y.,+Kao,+B.+and+Liu,+Q.+(2020).+Perturbed+masking:+Parameter-free+probing+for+analyzing+and+interpreting+BERT.+In+Proceedings+of+the+58th+Annual+Meeting+of+the+Association+for+Computational+Linguistics,+Online.+Association+for+Computational+Linguistics,+pp.+4166–4176.",openUrlParams:{genre:g,date:i,sid:e,title:d},innerRefId:"r72",title:"Wu, Z., Chen, Y., Kao, B. and Liu, Q. (2020). Perturbed masking: Parameter-free probing for analyzing and interpreting BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics, pp. 4166–4176.",doi:"10.18653\u002Fv1\u002F2020.acl-main.383",crossRefLink:"https:\u002F\u002Fdx.doi.org\u002F10.18653\u002Fv1\u002F2020.acl-main.383",pubMedLink:a}]}],figures:[{contentId:"tbl1",label:"Table 1.",description:"\u003Cspan class=\"p\"\u003ESample input and corresponding target output for the general idiom token identification task\u003C\u002Fspan\u003E",thumbnailSrc:au,enlargedSrc:au,attrib:[]},{contentId:"f1",label:"Figure 1.",description:"\u003Cspan class=\"p\"\u003ETopic-Aware probing method.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-96676-mediumThumb-png-S2977042424000438_fig1.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-86121-optimisedImage-png-S2977042424000438_fig1.jpg",attrib:[]},{contentId:"tbl2",label:"Table 2.",description:"\u003Cspan class=\"p\"\u003ENumber of tail topics from 10 topic models on Bigram shift and VNIC dataset\u003C\u002Fspan\u003E",thumbnailSrc:av,enlargedSrc:av,attrib:[]},{contentId:"f2",label:"Figure 2.",description:"\u003Cspan class=\"p\"\u003EExperimental design.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-52050-mediumThumb-png-S2977042424000438_fig2.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-84769-optimisedImage-png-S2977042424000438_fig2.jpg",attrib:[]},{contentId:"tbl3",label:"Table 3.",description:"\u003Cspan class=\"p\"\u003EAverage seen and unseen AUC ROC scores and their differences along with standard deviations for different embeddings on the Bigram Shift Probing task and the General Idiom Token Identification task\u003C\u002Fspan\u003E",thumbnailSrc:aw,enlargedSrc:aw,attrib:[]},{contentId:"f3",label:"Figure 3.",description:"\u003Cspan class=\"p\"\u003ESeen and Unseen AUC ROC scores from GloVe and different layers of BERT and RoBERTa on the Bigram Shift Task.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-68511-mediumThumb-png-S2977042424000438_fig3.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-99766-optimisedImage-png-S2977042424000438_fig3.jpg",attrib:[]},{contentId:"f4",label:"Figure 4.",description:"\u003Cspan class=\"p\"\u003ESeen and Unseen AUC ROC scores from different layers of BERT and RoBERTa with GloVe baseline on General Idiom Token Identification Task.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-83266-mediumThumb-png-S2977042424000438_fig4.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-49180-optimisedImage-png-S2977042424000438_fig4.jpg",attrib:[]},{contentId:"f5",label:"Figure 5.",description:"\u003Cspan class=\"p\"\u003EDifference between seen scores and unseen scores from different layers of BERT and RoBERTa on General Idiom Token Identification Task.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-77274-mediumThumb-png-S2977042424000438_fig5.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-13883-optimisedImage-png-S2977042424000438_fig5.jpg",attrib:[]},{contentId:"tbl4",label:"Table 4.",description:"\u003Cspan class=\"p\"\u003EDescriptions and summary statistics of the datasets for the VNIC, Bigram shift, and 8 other probing tasks\u003C\u002Fspan\u003E",thumbnailSrc:ax,enlargedSrc:ax,attrib:[]},{contentId:"tbl5",label:"Table 5.",description:"\u003Cspan class=\"p\"\u003ENumber of tail topics from 10 topic models on datasets of other 8 probing tasks\u003C\u002Fspan\u003E",thumbnailSrc:ay,enlargedSrc:ay,attrib:[]},{contentId:"f6",label:"Figure 6.",description:"\u003Cspan class=\"p\"\u003ESeen and Unseen AUC ROC scores from different layers of BERT with GloVe baseline on Probing Tasks.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-93684-mediumThumb-png-S2977042424000438_fig6.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-02891-optimisedImage-png-S2977042424000438_fig6.jpg",attrib:[]},{contentId:"f7",label:"Figure 7.",description:"\u003Cspan class=\"p\"\u003ESeen and Unseen AUC ROC scores from different layers of RoBERTa with GloVe baseline on Probing Tasks.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-75777-mediumThumb-png-S2977042424000438_fig7.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-97774-optimisedImage-png-S2977042424000438_fig7.jpg",attrib:[]},{contentId:"tbl6",label:"Table 6.",description:"\u003Cspan class=\"p\"\u003EAverage seen and unseen AUC ROC scores and their differences for GloVe and best BERT and RoBERTa layer embeddings on different probing tasks—tasks are ranked in the descending order of the difference between GloVe Seen score and GloVe Unseen score\u003C\u002Fspan\u003E",thumbnailSrc:az,enlargedSrc:az,attrib:[]},{contentId:"f8",label:"Figure 8.",description:"\u003Cspan class=\"p\"\u003EGloVe Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task (Note that scores of SOMO and CI are very similar and therefore both of them are overlapping in the plot).\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-21219-mediumThumb-png-S2977042424000438_fig8.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-02417-optimisedImage-png-S2977042424000438_fig8.jpg",attrib:[]},{contentId:"f9",label:"Figure 9.",description:"\u003Cspan class=\"p\"\u003EBERT and RoBERTa Seen Score versus GloVe Score Difference (Task Topic Sensitivity) for each probing task.\u003C\u002Fspan\u003E",thumbnailSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-06673-mediumThumb-png-S2977042424000438_fig9.jpg",enlargedSrc:"https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary-alt:20241219095033-25832-optimisedImage-png-S2977042424000438_fig9.jpg",attrib:[]}]},pdf:{standardResolution:{fileUrl:"\u002Fcore\u002Fservices\u002Faop-cambridge-core\u002Fcontent\u002Fview\u002F0FFF33B18E284DAB8FE8DCF69A963A30\u002FS2977042424000438a.pdf\u002Ftopic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic.pdf",fileSizeInMb:k,articleTitle:t,slugTitle:"topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic"},highResolution:d,media:d},classification:[],supplementaryMaterials:[],relations:{corrections:[],correctionsOriginals:[],retractions:[],retractionsOriginals:[],addendums:[],addendumsOriginals:[],hasAnyRelations:b,hasAnyOriginalArticle:b},settings:{hasAccess:c,isOpenAccess:c,displayRightsLink:c,shouldDisplayCrossMark:b,shouldDisplayNasaAds:b,suppressPdf:b,isShareable:c,isAnnotationsEnabled:b,disableArticleCommentary:b,displayArticleCommentaryAsDiscussionLinks:b,isCommentsEnabled:b,hasContent:c,shouldDisplaySubmitContent:b,isResearchDirections:b,isQuestionCollection:b,isMathjaxEnabled:b},citationCount:u,openUrlParams:"?genre=article&atitle=Topic%20aware%20probing%3A%20From%20sentence%20length%20prediction%20to%20idiom%20identification%20how%20reliant%20are%20neural%20language%20models%20on%20topic%3F&jtitle=Natural%20Language%20Processing&title=Natural%20Language%20Processing&spage=1&epage=29&sid=https%3A%2F%2Fwww.cambridge.org%2Fcore&aulast=Nedumpozhimana&aufirst=Vasudevan&doi=10.1017\u002Fnlp.2024.43",ecommerceProducts:{digitalSku:"S2977042424000438",paperBackSku:d,hardBackSku:d},subject:[],permissionUrl:"https:\u002F\u002Fs100.copyright.com\u002FAppDispatchServlet?publisherName=CUP&publication=NLP&title=Topic%20aware%20probing%3A%20From%20sentence%20length%20prediction%20to%20idiom%20identification%20how%20reliant%20are%20neural%20language%20models%20on%20topic%3F&publicationDate=25%20October%202024&author=Vasudevan%20Nedumpozhimana%2C%20John%20D.%20Kelleher&copyright=%C2%A9%20The%20Author(s)%2C%202024.%20Published%20by%20Cambridge%20University%20Press&contentID=10.1017%2Fnlp.2024.43&startPage=1&endPage=29&orderBeanReset=True&volumeNum=&issueNum=&oa=CC-BY"},breadcrumbs:[{name:"Home",url:"\u002Fcore"},{name:"Journals",url:"\u002Fcore\u002Fpublications\u002Fjournals"},{name:p,url:H},{name:"FirstView",url:I},{name:"Topic aware probing: From sentence length prediction..."}],lang:J,isShare:b,coreCmsConfig:{shouldUseShareProductTool:c,shouldUseHypothesis:c,isUnsiloEnabled:c},debugHostName:"page-component-6bf8c574d5-r8w4l",debugTotalLoadingTime:u,debugRenderDate:"2025-03-03T21:28:34.214Z",debugHasDataIssue:b,debugHasContentIssue:b}],fetch:{},error:d,state:{errors:{hasAnyIssue:b,hasContentIssue:b},tabs:{supportedTabs:[],currentTab:a,stickyTabsEnabled:b},unleash:{repo:[{name:"authentication.useNewGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"authentication.useNewGlobalHeader.showDiscoveryTool",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"authentication.useShowSAPCDCScreenset",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"car-sup-mats",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"cart.useNewGlobalHeaderUrl",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"checkout.newGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"checkout.newGlobalHeader.showDiscoveryTool",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"checkout.useNewGlobalHeaderUrl",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreDisplayGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreDisplayKBARTAutomation",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreDisplayResearchOpen",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CorePageComponentGetUserInfoFromSharedSession",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CorePageComponentUseNewCombobox",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreUseCitationToolApi",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreUseKbartMetafile",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreUseNewCms",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"CoreUseOmnichannel",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"DT.newGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"DT.useUsageEvents",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"Ecommerce",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableContinueProcessReminderPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableEmailMessagesSqsPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableFirstProcessReminderPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableInternalArticlesPostProcessingSqsPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableInternalArticlesSqsPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableJmsMessagesPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableLogObfuscation",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnablePostPaymentEnrolment",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableSecondProcessReminderPolling",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"EnableZendeskRedirects",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.loadSupMatFolderMetrics",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.newCitationApi",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.NewReadAsGuestPopup",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.Omnichannel",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.platformHeader-micro-ui",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.readOnlinePrimaryButton",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"he.resourcesWidgetLandingPage",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useDefaultDigitalCopy",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useFixedLoginUrl",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useNewTitlesSections",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useQuickAccessPanel",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useRedesignedPill",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useResourcesRequestRedesigned",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.useStudyLevels",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"HE.verifySalesforceApiKey",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"homsy.Omnichannel",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"homsy.useNewIpDetection",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"new-publication-embargo-delivery-cron-job",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"sendEmailsToTargetRecipients",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"shoppingCart.newGlobalHeader.showDiscoveryTool",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"showSearchDashboardSortDropdown",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"showSearchDashboardVersionAndPublishedDate",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"use-new-publication-service",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseB2bPages",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseBlog",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseCourseWelcomeEmail",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseGroupBooking",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseKeycloakAuth",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"useNewGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"useOrcidAuthorSync",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UsePayByQuote",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UsePayPalPayment",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"UseSocialShareButton",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"wayf.newGlobalHeader",enabled:c,variant:{name:f,enabled:b},impressionData:b},{name:"wayf.newGlobalHeader.showDiscoveryTool",enabled:c,variant:{name:f,enabled:b},impressionData:b}]},i18n:{routeParams:{}}},serverRendered:c,routePath:"\u002Fcore\u002Fjournals\u002FjournalName\u002Farticle\u002FarticleName\u002F0FFF33B18E284DAB8FE8DCF69A963A30",config:{assetsService:"https:\u002F\u002Fstatic.cambridge.org",publicApi:"https:\u002F\u002Fwww.cambridge.org\u002Fcore\u002Fpage-component\u002Fapi\u002Fv2",domainUrl:e,shibbolethUrl:"https:\u002F\u002Fshibboleth.cambridge.org",seamlessAccessPs:"https:\u002F\u002Fservice.seamlessaccess.org\u002Fps\u002F",seamlessAccessContext:"seamlessaccess.org",shareaholicScript:"https:\u002F\u002Fcdn.shareaholic.net\u002Fassets\u002Fpub\u002Fshareaholic.js",shareaholicSiteId:"b60ec523a5bee2ad04c630bf0d3aa388",shareaholicAppId:33113081,unleashProxyUrl:"https:\u002F\u002Fcore-features.cambridge.org\u002Fproxy",unleashProxyClientKey:"B43LrdgqNKlNsaHfVzQ7l78gkVH0K7tf",cloudflareAuth:a,_app:{basePath:"\u002F",assetsPath:"\u002Fcore\u002Fpage-component\u002F",cdnURL:d}}}}("",false,true,null,"https:\u002F\u002Fwww.cambridge.org","disabled","unknown","2019","2020","article",1,"2022","2021","chapter","Advances in Neural Information Processing Systems","Natural Language Processing","2018","Computational Linguistics","Transactions of the Association for Computational Linguistics","Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?",0,"5","2017","Findings of the Association for Computational Linguistics: EMNLP.","Introduction to Natural Language Processing","Computational Linguistics and Intelligent Text Processing","2010","Foundations of Statistical Natural Language Processing","Findings of the Association for Computational Linguistics","LREC","1","6EF4F7A2CDB4AE9BC9DD279CA0D11FD7","natural-language-processing","\u002Fcore\u002Fjournals\u002Fnatural-language-processing","\u002Fcore\u002Fjournals\u002Fnatural-language-processing\u002Ffirstview","en","10.18653\u002Fv1\u002F2022.findings-emnlp.502","Survey: multiword expression processing: a survey","10.1162\u002FCOLI_a_00302","2008","Indexing by latent semantic analysis","Journal of the American Society of Information Science","10.1002\u002F(SICI)1097-4571(199009)41:6\u003C391::AID-ASI1\u003E3.0.CO;2-9","book","What BERT is not: lessons from a new suite of psycholinguistic diagnostics for language models","8","10.1162\u002Ftacl_a_00298","Unsupervised type and token identification of idiomatic expressions","35","10.1162\u002Fcoli.08-010-R1-07-048","2013","10.1007\u002F978-3-642-37247-6_35","Bert & family eat word salad: experiments with text understanding","Proceedings of the AAAI Conference on Artificial Intelligence","10.1609\u002Faaai.v35i14.17531","Capturing and measuring thematic relatedness","Language Resources and Evaluation","54","10.1007\u002Fs10579-019-09452-w","Emergent linguistic structure in artificial neural networks trained by self-supervision","Proceedings of the National Academy of Sciences of the United States of America","10.1073\u002Fpnas.1907367117","Shapley idioms: analysing BERT sentence embeddings for general idiom token identification","Frontiers in Artificial Intelligence","10.3389\u002Ffrai.2022.813967","Scikit-learn: machine learning in python","Journal of Machine Learning Research","2014","A primer in BERTology: what we know about how BERT works","10.1162\u002Ftacl_a_00349","2016","Literal occurrences of multiword expressions: rare birds that cause a stir","The Prague Bulletin of Mathematical Linguistics","10.2478\u002Fpralin-2019-0001","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab1.png","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab2.png","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab3.png","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab4.png","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab5.png","https:\u002F\u002Fstatic.cambridge.org\u002Fbinary\u002Fversion\u002Fid\u002Furn:cambridge.org:id:binary:20241024161419147-0788:S2977042424000438:S2977042424000438_tab6.png"));</script><script src="/core/page-component/06cd607.js" defer></script><script src="/core/page-component/20147ec.js" defer></script><script src="/core/page-component/66884fb.js" defer></script><script src="/core/page-component/6f4302c.js" defer></script>
</div>

</div>


<div id="article-new-home-productCitations" class="product-citations-modal reveal-modal medium" data-reveal role="dialog"
  aria-labelledby="article-new-home-citedByModalHeader">

  <div class="header">
    <div class="heading_07 margin-bottom" id="article-new-home-citedByModalHeader">Cited by</div>
  </div>



  <div class="citation-content">
    <div class="row collapse header margin-top productCitations-content">
      <ul class="small-12 small-centered columns citations">
        <li class="fade-in section-container active">
          <a href="#" class="section-button" data-id="article-new-home-crossref-citations">
            <div class="circular medium citation">
              <img src="https://assets.crossref.org/logo/crossref-logo-100.png" alt="Crossref logo">
              <span
                class="citation-count">0</span>
            </div>
          </a>
        </li>
        <li class="fade-in section-container">
          <a href="#" class="section-button" data-id="article-new-home-scholar-citations">
            <div class="circular medium citation">
              <img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Google_Scholar_logo_2015.PNG"
                alt="Google Scholar logo">
            </div>
          </a>
        </li>
      </ul>
    </div>
    
    <div class="row wrapper no-padding-top section-content">
      <div class="small-10 small-centered columns">
        <section class="content" id="article-new-home-crossref-citations">
          <div class="large-12 columns">
            <div class="panel margin-top-small citation-container">
              <div class="row">
                <div class="medium-12 columns">
                  <p>No CrossRef data available.</p>
                </div>
              </div>
            </div>
          </div>
        </section>
    
        <section style="display:none" class="content" id="article-new-home-scholar-citations">
          <div class="large-12 columns">
            <div class="panel margin-top-small citation-container">
              <div class="row">
                <div class="medium-12 columns">
                  <div class="print-only print-heading margin-bottom-large">Google Scholar Citations</div>
                  <p>View all <a
                      href="https://scholar.google.com/scholar?hl=en&lr=&cites=https://www.cambridge.org/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30"
                      target="_blank">Google Scholar citations</a>
                    for this article.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </section>
      </div>
    </div>  </div>

  <a href="#" class="close-reveal-modal" aria-label="Close cited by"><span aria-hidden="true">×</span></a>

</div>

<script>
  $(document).ready(function () {
    $('body').on('click', '#article-new-home-productCitations .section-button', function (e) {
      e.preventDefault();
      var id = $(this).attr('data-id');
      $('#article-new-home-productCitations .section-content section').hide();
      $('#article-new-home-productCitations .section-container').removeClass('active');
      $(this).parent('.section-container').addClass('active');
      $('#' + id).show();
    });
  });
</script>
  <div id='platform-footer'>
    <div class="__shared-elements-html ShEl"><div class="__shared-elements-head">

<link rel="stylesheet" href="/aca/shared-elements/_nuxt/entry.D9LY0ri8.css">

<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-404.B06nACMW.css">


<link rel="prefetch" as="style" href="/aca/shared-elements/_nuxt/error-500.WGRfNq7F.css">

</div><div class="__shared-elements-body"><div id="__sharedElements-axsj76"><!--[--><!----><div class="apl"><footer class="apl-footer apl-theme--core" data-v-20efd383><section class="apl-footer__top" data-v-20efd383><div class="apl-container apl-footer__container-top" data-v-20efd383><!--[--><a href="/" class="apl-footer__logo-link" data-v-20efd383><img src="data:image/png;base64,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" alt="Cambridge University Press" class="apl-footer__logo-image" data-v-20efd383></a><div class="apl-footer__nav-and-location" data-v-20efd383><div class="apl-footer__nav" data-v-20efd383><nav class="apl-link-block__nav" data-v-7c018d72 data-v-20efd383><h2 class="apl-link-block__heading" data-v-7c018d72>Our Site</h2><ul class="apl-link-block__list-platforms" data-v-7c018d72><!--[--><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="/core/accessibility" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Core accessibility page" data-v-7c018d72><!----><!--[--><span>Accessibility</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="/core/help/FAQs" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Core contact &amp; help page" data-v-7c018d72><!----><!--[--><span>Contact &amp; Help</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="/core/legal-notices/terms" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Core legal notices page" data-v-7c018d72><!----><!--[--><span>Legal Notices</span><!--]--><!----><!----></a></li><!--]--></ul></nav><nav class="apl-link-block__nav" data-v-7c018d72 data-v-20efd383><h2 class="apl-link-block__heading" data-v-7c018d72>Our Platforms</h2><ul class="apl-link-block__list-platforms" data-v-7c018d72><!--[--><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/core/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Core. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Cambridge Core</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/engage/coe/public-dashboard" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Open Engage. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Cambridge Open Engage</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/highereducation/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Cambridge Higher Education. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Cambridge Higher Education</span><!--]--><!----><!----></a></li><!--]--></ul></nav><nav class="apl-link-block__nav" data-v-7c018d72 data-v-20efd383><h2 class="apl-link-block__heading" data-v-7c018d72>Our Products</h2><ul class="apl-link-block__list-platforms" data-v-7c018d72><!--[--><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/core/publications/journals" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Journals. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Journals</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/core/search?aggs%5BproductTypes%5D%5Bfilters%5D=BOOK" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Books. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Books</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/core/publications/elements" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Elements. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Elements</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/highereducation/search?aggs=%24productTypes%24BOOK%3Atrue%3B%3B" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Textbooks. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Textbooks</span><!--]--><!----><!----></a></li><li class="apl-link-block__list-item" data-v-7c018d72><a tabindex="0" href="https://www.cambridge.org/highereducation/search?aggs=%24productTypes%24COURSEWARE%3Atrue%3B%3B" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-link-block__btn-link" aria-label="Courseware. Opens in a new window." data-v-7c018d72><!----><!--[--><span>Courseware</span><!--]--><!----><!----></a></li><!--]--></ul></nav></div><div class="apl-footer__socials-and-location" data-v-20efd383><div class="apl-footer__socials" data-v-20efd383><h2 class="apl-footer__heading" data-v-20efd383>Join us online</h2><ul class="apl-footer__list-socials" data-v-20efd383><!--[--><li class="apl-footer__list-item" data-v-20efd383><a tabindex="0" href="https://www.youtube.com/playlist?list=PLTK8KRW19hUVucVRHbIx73oLKUro8HXt0" target="_blank" class="apl-button apl-button--primary apl-button--sm apl-button--text apl-button--link apl-footer__list-item-link" aria-label="Visit Cambridge University Press Youtube account. Opens in a new tab." data-v-20efd383><!----><!--[--><div class="apl-icon apl-icon--youtube" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><!--]--><!----><!----></a></li><li class="apl-footer__list-item" data-v-20efd383><a tabindex="0" href="https://twitter.com/CambridgeCore" target="_blank" class="apl-button apl-button--primary apl-button--sm apl-button--text apl-button--link apl-footer__list-item-link" aria-label="Visit Cambridge University Press X account. Opens in a new tab." data-v-20efd383><!----><!--[--><div class="apl-icon apl-icon--x" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><!--]--><!----><!----></a></li><li class="apl-footer__list-item" data-v-20efd383><a tabindex="0" href="https://www.facebook.com/CambridgeCore" target="_blank" class="apl-button apl-button--primary apl-button--sm apl-button--text apl-button--link apl-footer__list-item-link" aria-label="Visit Cambridge University Press Facebook account. Opens in a new tab." data-v-20efd383><!----><!--[--><div class="apl-icon apl-icon--facebook" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><!--]--><!----><!----></a></li><li class="apl-footer__list-item" data-v-20efd383><a tabindex="0" href="https://www.instagram.com/cambridgeuniversitypress/" target="_blank" class="apl-button apl-button--primary apl-button--sm apl-button--text apl-button--link apl-footer__list-item-link" aria-label="Visit Cambridge University Press Instagram account. Opens in a new tab." data-v-20efd383><!----><!--[--><div class="apl-icon apl-icon--instagram" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><!--]--><!----><!----></a></li><li class="apl-footer__list-item" data-v-20efd383><a tabindex="0" href="https://www.linkedin.com/showcase/11096649" target="_blank" class="apl-button apl-button--primary apl-button--sm apl-button--text apl-button--link apl-footer__list-item-link" aria-label="Visit Cambridge University Press Linkedin account. Opens in a new tab." data-v-20efd383><!----><!--[--><div class="apl-icon apl-icon--linkedin" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><!--]--><!----><!----></a></li><!--]--></ul></div><div class="apl-footer__location apl-footer__location--desktop" data-v-20efd383><div class="apl-dropdown" data-v-6114426c data-v-20efd383><div class="apl-location" data-v-ae305bb6 data-v-6114426c><div class="apl-location__main" data-v-ae305bb6><div class="apl-location__location-picker" data-v-ae305bb6><label class="apl-location__label" for="footer-location-picker" aria-label="Select your location" data-v-ae305bb6>Location</label><div class="apl-location__input-wrapper" data-v-ae305bb6><input id="footer-location-picker" type="text" role="combobox" aria-expanded="false" placeholder="Choose your location" class="apl-location__input" value="GBR" data-v-ae305bb6><div class="apl-icon apl-icon--globe-web apl-location__globe" style="color:;" aria-hidden="true" data-v-27c0a44c data-v-ae305bb6><!----></div><!----><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--sm apl-button--icon-only apl-location__chevron-btn" aria-label="open location list" data-v-ae305bb6><!----><!--[--><!----><!--]--><div class="apl-icon apl-icon--chevron-down apl-button__icon--center" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div><!----></button><!----><div class="apl-location__error" data-v-ae305bb6><div class="apl-icon apl-icon--error apl-location__error-icon" style="color:;" data-v-27c0a44c data-v-ae305bb6><!----></div><p class="apl-location__error-message" data-v-ae305bb6> Please choose a valid location. </p></div></div></div><button tabindex="0" type="submit" class="apl-button apl-button--secondary apl-button--sm apl-location__button" aria-label="Submit to update location preference" data-v-ae305bb6><!----><!--[--><span>Update</span><!--]--><!----><!----></button></div></div></div><!----></div></div></div><!--]--></div></section><section class="apl-footer__bottom" data-v-20efd383><div class="apl-container apl-footer__container-bottom" data-v-20efd383><!--[--><button tabindex="0" type="button" class="apl-button apl-button--primary apl-button--lg apl-footer__btn-collapse" aria-label="Click to open legal information links list" aria-expanded="false" aria-controls="apl-footer__legal-links" data-v-20efd383><!----><!--[--><span>Legal Information</span><!--]--><!----><div class="apl-icon apl-icon--chevron-down apl-button__icon--right" style="color:;" tabindex="-1" data-v-27c0a44c><!----></div></button><ul class="apl-footer__list-legal" id="apl-footer__legal-links" data-v-20efd383><!--[--><li class="apl-footer__list-item apl-footer__list-item-legal" data-v-20efd383><a tabindex="0" href="https://www.cambridge.org/about-us/rights-permissions/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-footer__btn-link apl-footer__btn-link-legal" aria-label="Rights and Permissions. Opens in a new window." data-v-20efd383><!----><!--[--><span>Rights &amp; Permissions</span><!--]--><!----><!----></a></li><li class="apl-footer__list-item apl-footer__list-item-legal" data-v-20efd383><a tabindex="0" href="https://www.cambridge.org/about-us/legal-notices/copyright/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-footer__btn-link apl-footer__btn-link-legal" aria-label="Copyright. Opens in a new window." data-v-20efd383><!----><!--[--><span>Copyright</span><!--]--><!----><!----></a></li><li class="apl-footer__list-item apl-footer__list-item-legal" data-v-20efd383><a tabindex="0" href="https://www.cambridge.org/about-us/legal-notices/privacy-policy/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-footer__btn-link apl-footer__btn-link-legal" aria-label="Privacy Notice. Opens in a new window." data-v-20efd383><!----><!--[--><span>Privacy Notice</span><!--]--><!----><!----></a></li><li class="apl-footer__list-item apl-footer__list-item-legal" data-v-20efd383><a tabindex="0" href="https://www.cambridge.org/about-us/legal-notices/terms-use" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-footer__btn-link apl-footer__btn-link-legal" aria-label="Terms of Use. Opens in a new window." data-v-20efd383><!----><!--[--><span>Terms of Use</span><!--]--><!----><!----></a></li><li class="apl-footer__list-item apl-footer__list-item-legal" data-v-20efd383><a tabindex="0" href="https://www.cambridge.org/about-us/legal-notices/cookies-policy/" target="_blank" class="apl-button apl-button--primary apl-button--lg apl-button--text apl-button--link apl-footer__btn-link apl-footer__btn-link-legal" aria-label="Cookies Policy. Opens in a new window." data-v-20efd383><!----><!--[--><span>Cookies Policy</span><!--]--><!----><!----></a></li><!--]--></ul><div class="apl-footer__copyright" data-v-20efd383><div class="apl-icon apl-icon--copyright" style="color:;" data-v-27c0a44c data-v-20efd383><!----></div><span data-v-20efd383> Cambridge University Press 2025</span></div><!--]--></div></section></footer></div><!----><!----><!--]--></div>
</div></div><script>window.__PLATFORM_FOOTER_DATA__ = {"data":{"entry":{"slug":"global-config","platformLinks":[{"name":"navigationBarCategory","value":{"title":"Browse","navigationBarArea":[{"name":"navigationBarArea","value":{"title":"Subjects","hubPage":{"name":"url","value":{"title":"Subjects","url":"/core/browse-subjects","ariaLabel":"Subjects"}},"column1Heading":" Subjects (A-D)","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Anthropology","url":"/core/browse-subjects/anthropology","ariaLabel":"Anthropology"}},{"name":"url","value":{"title":"Archaeology","url":"/core/browse-subjects/archaeology","ariaLabel":"Archaeology"}},{"name":"url","value":{"title":"Area Studies","url":"/core/browse-subjects/area-studies","ariaLabel":"Area Studies"}},{"name":"url","value":{"title":"Art","url":"/core/browse-subjects/art","ariaLabel":"Art"}},{"name":"url","value":{"title":"Chemistry","url":"/core/browse-subjects/chemistry","ariaLabel":"Chemistry"}},{"name":"url","value":{"title":"Classical Studies","url":"/core/browse-subjects/classical-studies","ariaLabel":"Classical Studies"}},{"name":"url","value":{"title":"Computer Science","url":"/core/browse-subjects/computer-science","ariaLabel":"Computer Science"}},{"name":"url","value":{"title":"Drama, Theatre, Performance Studies","url":"/core/browse-subjects/drama-and-theatre","ariaLabel":"Drama, Theatre, Performance Studies"}}],"column2Heading":" Subjects (E-K)","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Earth and Environmental Science","url":"/core/browse-subjects/earth-and-environmental-sciences","ariaLabel":"Earth and Environmental Science"}},{"name":"url","value":{"title":"Economics","url":"/core/browse-subjects/economics","ariaLabel":"Economics"}},{"name":"url","value":{"title":"Education","url":"/core/browse-subjects/education","ariaLabel":"Education"}},{"name":"url","value":{"title":"Engineering","url":"/core/browse-subjects/engineering","ariaLabel":"Engineering"}},{"name":"url","value":{"title":"English Language Teaching – Resources for Teachers","url":"/core/browse-subjects/english-language-teaching-resources-for-teachers","ariaLabel":"English Language Teaching – Resources for Teachers"}},{"name":"url","value":{"title":"Film, Media, Mass Communication","url":"/core/browse-subjects/film-media-mass-ommunication","ariaLabel":"Film, Media, Mass Communication"}},{"name":"url","value":{"title":"General Science","url":"/core/browse-subjects/general-science","ariaLabel":"General Science"}},{"name":"url","value":{"title":"Geography","url":"/core/browse-subjects/geography","ariaLabel":"Geography"}},{"name":"url","value":{"title":"History","url":"/core/browse-subjects/history","ariaLabel":"History"}}],"column3Heading":" Subjects (L-O)","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Language and Linguistics","url":"/core/browse-subjects/language-and-linguistics","ariaLabel":"Language and Linguistics"}},{"name":"url","value":{"title":"Law","url":"/core/browse-subjects/law","ariaLabel":"Law"}},{"name":"url","value":{"title":"Life Sciences","url":"/core/browse-subjects/life-sciences","ariaLabel":"Life Sciences"}},{"name":"url","value":{"title":"Literature","url":"/core/browse-subjects/literature","ariaLabel":"Literature"}},{"name":"url","value":{"title":"Management","url":"/core/browse-subjects/management","ariaLabel":"Management"}},{"name":"url","value":{"title":"Materials Science","url":"/core/browse-subjects/materials-science","ariaLabel":"Materials Science"}},{"name":"url","value":{"title":"Mathematics","url":"/core/browse-subjects/mathematics","ariaLabel":"Mathematics"}},{"name":"url","value":{"title":"Medicine","url":"/core/browse-subjects/medicine","ariaLabel":"Medicine"}},{"name":"url","value":{"title":"Music","url":"/core/browse-subjects/music","ariaLabel":"Music"}},{"name":"url","value":{"title":"Nutrition","url":"/core/browse-subjects/nutrition","ariaLabel":"Nutrition"}}],"column4Heading":" Subjects (P-Z)","column4StaticPagesOrUrls":[{"name":"url","value":{"title":"Philosophy","url":"/core/browse-subjects/philosophy","ariaLabel":"Philosophy"}},{"name":"url","value":{"title":"Physics and Astronomy","url":"/core/browse-subjects/physics","ariaLabel":"Physics and Astronomy"}},{"name":"url","value":{"title":"Politics and International Relations","url":"/core/browse-subjects/politics-and-international-relations","ariaLabel":"Politics and International Relations"}},{"name":"url","value":{"title":"Psychiatry","url":"/core/browse-subjects/psychiatry","ariaLabel":"Psychiatry"}},{"name":"url","value":{"title":"Psychology","url":"/core/browse-subjects/psychology","ariaLabel":"Psychology"}},{"name":"url","value":{"title":"Religion","url":"/core/browse-subjects/religion","ariaLabel":"Religion"}},{"name":"url","value":{"title":"Social Science Research Methods","url":"/core/browse-subjects/social-science-research-methods","ariaLabel":"Social Science Research Methods"}},{"name":"url","value":{"title":"Sociology","url":"/core/browse-subjects/sociology","ariaLabel":"Sociology"}},{"name":"url","value":{"title":"Statistics and Probability","url":"/core/browse-subjects/statistics-and-probability","ariaLabel":"Statistics and Probability"}}],"slug":"subjects"}},{"name":"navigationBarArea","value":{"title":"Open access","hubPage":{"name":"url","value":{"title":"Open access","url":"/core/publications/open-access","ariaLabel":"Open access"}},"column1Heading":"All open access publishing","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Open access","url":"/core/publications/open-access","ariaLabel":"Open access"}},{"name":"url","value":{"title":"Open access journals","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Open access journals"}},{"name":"url","value":{"title":"Research open journals","url":"/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Research open journals"}},{"name":"url","value":{"title":"Journals containing open access","url":"/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Journals containing open access"}},{"name":"url","value":{"title":"Open access articles","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE","ariaLabel":"Open access articles"}},{"name":"url","value":{"title":"Open access books","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&sort=canonical.date:desc","ariaLabel":"Open access books"}},{"name":"url","value":{"title":"Open access Elements","url":"/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493","ariaLabel":"Open access Elements"}}],"slug":"open-access"}},{"name":"navigationBarArea","value":{"title":"Journals","hubPage":{"name":"url","value":{"title":"Journals","url":"/core/publications/journals","ariaLabel":"Journals"}},"column1Heading":"Explore","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"All journal subjects","url":"/core/publications/journals","ariaLabel":"All journal subjects"}},{"name":"url","value":{"title":"Search journals","url":"/core/publications/journals","ariaLabel":"Search journals"}}],"column2Heading":"Open access","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Open access journals","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Open access journals"}},{"name":"url","value":{"title":"Research open journals","url":"/core/publications/open-access/research-open?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Research open journals"}},{"name":"url","value":{"title":"Journals containing open access","url":"/core/publications/open-access/hybrid-open-access-journals?aggs[productTypes][filters]=JOURNAL&statuses=PUBLISHED&sort=titleSort:asc","ariaLabel":"Journals containing open access"}},{"name":"url","value":{"title":"Open access articles","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=JOURNAL_ARTICLE","ariaLabel":"Open access articles"}}],"column3Heading":"Collections","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Forum","url":"/core/publications/collections/cambridge-forum","ariaLabel":"Cambridge Forum"}},{"name":"url","value":{"title":"Cambridge Law Reports Collection","url":"/core/publications/collections/cambridge-law-reports-collection","ariaLabel":"Cambridge Law Reports Collection"}},{"name":"url","value":{"title":"Cambridge Prisms","url":"/core/publications/collections/cambridge-prisms","ariaLabel":"Cambridge Prisms"}},{"name":"url","value":{"title":"Research Directions","url":"/core/publications/collections/research-directions","ariaLabel":"Research Directions"}}],"slug":"journals"}},{"name":"navigationBarArea","value":{"title":"Books","hubPage":{"name":"url","value":{"title":"Books","url":"/core/publications/books","ariaLabel":"Books"}},"column1Heading":"Explore","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Books","url":"/core/publications/books","ariaLabel":"Books"}},{"name":"url","value":{"title":"Open access books","url":"/core/publications/open-access/listing?aggs[productTypes][filters]=BOOK&sort=canonical.date:desc","ariaLabel":"Open access books"}},{"name":"url","value":{"title":"New books","url":"/core/publications/books/listing?aggs[productDate][filters]=Last+3+months&aggs[productTypes][filters]=BOOK&sort=canonical.date:desc","ariaLabel":"New books"}},{"name":"url","value":{"title":"Flip it Open","url":"/core/publications/collections/flip-it-open","ariaLabel":"Flip it Open"}}],"column2Heading":"Collections","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Companions","url":"/core/publications/collections/cambridge-companions","ariaLabel":"Cambridge Companions"}},{"name":"url","value":{"title":"Cambridge Editions","url":"/core/publications/collections/cambridge-editions","ariaLabel":"Cambridge Editions"}},{"name":"url","value":{"title":"Cambridge Histories","url":"/core/publications/collections/cambridge-histories","ariaLabel":"Cambridge Histories"}},{"name":"url","value":{"title":"Cambridge Library Collection","url":"/core/publications/collections/cambridge-library-collection","ariaLabel":"Cambridge Library Collection"}},{"name":"url","value":{"title":"Cambridge Shakespeare","url":"/core/publications/collections/cambridge-shakespeare","ariaLabel":"Cambridge Shakespeare"}},{"name":"url","value":{"title":"Cambridge Handbooks","url":"/core/publications/collections/cambridgehandbooks","ariaLabel":"Cambridge Handbooks"}}],"column3Heading":" Collections (cont.)","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Dispute Settlement Reports Online","url":"/core/publications/collections/dispute-settlement-reports-online","ariaLabel":"Dispute Settlement Reports Online"}},{"name":"url","value":{"title":"Flip it Open","url":"/core/publications/collections/flip-it-open","ariaLabel":"Flip it Open"}},{"name":"url","value":{"title":"Hemingway Letters","url":"/core/publications/collections/hemingway-letters","ariaLabel":"Hemingway Letters"}},{"name":"url","value":{"title":"Shakespeare Survey","url":"/core/publications/collections/shakespeare-survey","ariaLabel":"Shakespeare Survey"}},{"name":"url","value":{"title":"Stahl Online","url":"/core/publications/collections/stahl-online","ariaLabel":"Stahl Online"}},{"name":"url","value":{"title":"The Correspondence of Isaac Newton","url":"/core/publications/collections/the-correspondence-of-isaac-newton","ariaLabel":"The Correspondence of Isaac Newton"}}],"slug":"books"}},{"name":"navigationBarArea","value":{"title":"Elements","hubPage":{"name":"url","value":{"title":"Elements","url":"/core/publications/elements","ariaLabel":"Elements"}},"column1Heading":"Explore","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"About Elements","url":"/core/publications/elements","ariaLabel":"About Elements"}},{"name":"url","value":{"title":"Elements series","url":"/core/publications/elements/cambridge-elements-series","ariaLabel":"Elements series"}},{"name":"url","value":{"title":"Open access Elements","url":"/core/publications/elements/published-elements?aggs%5BopenAccess%5D%5Bfilters%5D=7275BA1E84CA769210167A6A66523B47&aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493","ariaLabel":"Open access Elements"}},{"name":"url","value":{"title":"New Elements","url":"/core/publications/elements/published-elements?aggs%5BproductTypes%5D%5Bfilters%5D=ELEMENT&aggs%5BproductDate%5D%5Bfilters%5D=Last%203%20months&searchWithinIds=ECFD8F5C64F47F3F5A3D395C15B7C493","ariaLabel":"New Elements"}}],"column2Heading":"Subjects (A-E)","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Anthropology","url":"/core/elements/subject/Anthropology/2E44A5AF2838E017617A26DD79FAEAEE","ariaLabel":"Anthropology"}},{"name":"url","value":{"title":"Archaeology","url":"/core/elements/subject/Archaeology/63A50B5368A9F97F8AA2D6AB965B5F4C","ariaLabel":"Archaeology"}},{"name":"url","value":{"title":"Classical Studies","url":"/core/elements/subject/Classical%20Studies/DDC63B7F5792FE2A95D1FB15F76E3F42","ariaLabel":"Classical Studies"}},{"name":"url","value":{"title":"Computer Science","url":"/core/elements/subject/Computer%20Science/A57E10708F64FB69CE78C81A5C2A6555","ariaLabel":"Computer Science"}},{"name":"url","value":{"title":"Drama, Theatre, Performance Studies","url":"/core/elements/subject/Drama,%20Theatre,%20Performance%20Studies/2825E4E39F2D641B36543EE80FB1DEA3","ariaLabel":"Drama, Theatre, Performance Studies"}},{"name":"url","value":{"title":"Earth and Environmental Sciences","url":"/core/elements/subject/Earth%20and%20Environmental%20Sciences/F470FBF5683D93478C7CAE5A30EF9AE8","ariaLabel":"Earth and Environmental Sciences"}},{"name":"url","value":{"title":"Economics","url":"/core/elements/subject/Economics/FA44491F1F55F917C43E9832715B9DE7","ariaLabel":"Economics"}},{"name":"url","value":{"title":"Education","url":"/core/elements/subject/Education/550D00F8DF590F2598CF7CC0038E24D1","ariaLabel":"Education"}},{"name":"url","value":{"title":"Engineering","url":"/core/elements/subject/Engineering/CCC62FE56DCC1D050CA1340C1CCF46F5","ariaLabel":"Engineering"}}],"column3Heading":" Subjects (F-O)","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Film, Media, Mass Communication","url":"/core/elements/subject/Film,%20Media,%20Mass%20Communication/4B91F10E834814A90CE718E7831E492F","ariaLabel":"Film, Media, Mass Communication"}},{"name":"url","value":{"title":"History","url":"/core/elements/subject/History/66BE42A30172E280FDE64F8EE2F485B0","ariaLabel":"History"}},{"name":"url","value":{"title":"Language and Linguistics","url":"/core/elements/subject/Language%20and%20Linguistics/140D314098408C26BDF3009F7FF858E9","ariaLabel":"Language and Linguistics"}},{"name":"url","value":{"title":"Law","url":"/core/elements/subject/Law/7C9FB6788DD8D7E6696263BC774F4D5B","ariaLabel":"Law"}},{"name":"url","value":{"title":"Life Sciences","url":"/core/elements/subject/Life%20Sciences/E044EF2F61B601378786E9EDA901B2D5","ariaLabel":"Life Sciences"}},{"name":"url","value":{"title":"Literature","url":"/core/elements/subject/Literature/F2434ADC122145767C6C3B988A8E9BD5","ariaLabel":"Literature"}},{"name":"url","value":{"title":"Management","url":"/core/elements/subject/Management/0EDCC0540639B06A5669BDEEF50C4CBE","ariaLabel":"Management"}},{"name":"url","value":{"title":"Mathematics","url":"/core/elements/subject/Mathematics/FA1467C44B5BD46BB8AA6E58C2252153","ariaLabel":"Mathematics"}},{"name":"url","value":{"title":"Medicine","url":"/core/elements/subject/Medicine/66FF02B2A4F83D9A645001545197F287","ariaLabel":"Medicine"}},{"name":"url","value":{"title":"Music","url":"/core/elements/subject/Music/A370B5604591CB3C7F9AFD892DDF7BD1","ariaLabel":"Music"}}],"column4Heading":" Subjects (P-Z)","column4StaticPagesOrUrls":[{"name":"url","value":{"title":"Philosophy","url":"/core/elements/subject/Philosophy/2D1AC3C0E174F1F1A93F8C7DE19E0FAB","ariaLabel":"Philosophy"}},{"name":"url","value":{"title":"Physics and Astronomy","url":"/core/elements/subject/Physics%20and%20Astronomy/DBFB610E9FC5E012C011430C0573CC06","ariaLabel":"Physics and Astronomy"}},{"name":"url","value":{"title":"Politics and International Relations","url":"/core/elements/subject/Politics%20and%20International%20Relations/3BF83347E5E456DAC34F3FABFC8BBF4E","ariaLabel":"Politics and International Relations"}},{"name":"url","value":{"title":"Psychology","url":"/core/elements/subject/Psychology/21B42A72BA3E4CB0E3315E5B1B71B07F","ariaLabel":"Psychology"}},{"name":"url","value":{"title":"Religion","url":"/core/elements/subject/Religion/53E51D24FB488962B9364A2C4B45D1C3","ariaLabel":"Religion"}},{"name":"url","value":{"title":"Sociology","url":"/core/elements/subject/Sociology/0E2CD53A93003DF17E52D753F6E90683","ariaLabel":"Sociology"}},{"name":"url","value":{"title":"Statistics and Probability","url":"/core/elements/subject/Statistics%20and%20Probability/3150B8B0D1B0B4E8DC17EC9EDFD9CA26","ariaLabel":"Statistics and Probability"}}],"slug":"elements"}},{"name":"navigationBarArea","value":{"title":"Textbooks","hubPage":{"name":"url","value":{"title":"Textbooks","url":"/core/publications/textbooks","ariaLabel":"Textbooks"}},"column1Heading":"Explore","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Higher Education","url":"/highereducation/","ariaLabel":"Cambridge Higher Education"}},{"name":"url","value":{"title":"Title list","url":"/highereducation/services/librarians/title-list","ariaLabel":"Title list"}},{"name":"url","value":{"title":"New titles","url":"/highereducation/search?sortBy=publication_date&aggs=%24productDate%24Last%25206%2520months%3Atrue%26Last%252012%2520months%3Atrue%26Last%25203%2520years%3Atrue%26Over%25203%2520years%3Atrue%3B%3B&event=SE-AU_PREF","ariaLabel":"New titles"}}],"slug":"textbooks"}},{"name":"navigationBarArea","value":{"title":"Collections","hubPage":{"name":"url","value":{"title":"Collections","url":"/core/publications/collections","ariaLabel":"Collections"}},"column1Heading":"Book collections","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Companions","url":"/core/publications/collections/cambridge-companions","ariaLabel":"Cambridge Companions"}},{"name":"url","value":{"title":"Cambridge Editions","url":"/core/publications/collections/cambridge-editions","ariaLabel":"Cambridge Editions"}},{"name":"url","value":{"title":"Cambridge Histories","url":"/core/publications/collections/cambridge-histories","ariaLabel":"Cambridge Histories"}},{"name":"url","value":{"title":"Cambridge Library Collection","url":"/core/publications/collections/cambridge-library-collection","ariaLabel":"Cambridge Library Collection"}},{"name":"url","value":{"title":"Cambridge Shakespeare","url":"/core/publications/collections/cambridge-shakespeare","ariaLabel":"Cambridge Shakespeare"}},{"name":"url","value":{"title":"Cambridge Handbooks","url":"/core/publications/collections/cambridgehandbooks","ariaLabel":"Cambridge Handbooks"}}],"column2Heading":" Book collections (cont.)","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Dispute Settlement Reports Online","url":"/core/publications/collections/dispute-settlement-reports-online","ariaLabel":"Dispute Settlement Reports Online"}},{"name":"url","value":{"title":"Flip it Open","url":"/core/publications/collections/flip-it-open","ariaLabel":"Flip it Open"}},{"name":"url","value":{"title":"Hemingway Letters","url":"/core/publications/collections/hemingway-letters","ariaLabel":"Hemingway Letters"}},{"name":"url","value":{"title":"Shakespeare Survey","url":"/core/publications/collections/shakespeare-survey","ariaLabel":"Shakespeare Survey"}},{"name":"url","value":{"title":"Stahl Online","url":"/core/publications/collections/stahl-online","ariaLabel":"Stahl Online"}},{"name":"url","value":{"title":"The Correspondence of Isaac Newton","url":"/core/publications/collections/the-correspondence-of-isaac-newton","ariaLabel":"The Correspondence of Isaac Newton"}}],"column3Heading":"Journal collections","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Forum","url":"/core/publications/collections/cambridge-forum","ariaLabel":"Cambridge Forum"}},{"name":"url","value":{"title":"Cambridge Law Reports Collection","url":"/core/publications/collections/cambridge-law-reports-collection","ariaLabel":"Cambridge Law Reports Collection"}},{"name":"url","value":{"title":"Cambridge Prisms","url":"/core/publications/collections/cambridge-prisms","ariaLabel":"Cambridge Prisms"}},{"name":"url","value":{"title":"Research Directions","url":"/core/publications/collections/research-directions","ariaLabel":"Research Directions"}}],"column4Heading":"Series","column4StaticPagesOrUrls":[{"name":"url","value":{"title":"All series","url":"/core/publications/collections/series","ariaLabel":"All series"}}],"slug":"collections"}},{"name":"navigationBarArea","value":{"title":"Partners","hubPage":{"name":"url","value":{"title":"Publishing partners","url":"/core/publications/publishing-partners","ariaLabel":"Publishing partners"}},"column1Heading":"Partners","column1StaticPagesOrUrls":[{"name":"url","value":{"title":"Agenda Publishing","url":"/core/publications/publishing-partners/agenda-publishing","ariaLabel":"Agenda Publishing"}},{"name":"url","value":{"title":"Amsterdam University Press","url":"/core/publications/publishing-partners/amsterdam-university-press","ariaLabel":"Amsterdam University Press"}},{"name":"url","value":{"title":"Anthem Press","url":"/core/publications/publishing-partners/anthem-press","ariaLabel":"Anthem Press"}},{"name":"url","value":{"title":"Boydell & Brewer","url":"/core/publications/publishing-partners/boydell-brewer","ariaLabel":"Boydell & Brewer"}},{"name":"url","value":{"title":"Bristol University Press","url":"/core/publications/publishing-partners/bristol-university-press","ariaLabel":"Bristol University Press"}},{"name":"url","value":{"title":"Edinburgh University Press","url":"/core/publications/publishing-partners/edinburgh-university-press","ariaLabel":"Edinburgh University Press"}},{"name":"url","value":{"title":"Emirates Center for Strategic Studies and Research","url":"/core/publications/publishing-partners/emirates-center","ariaLabel":"Emirates Center for Strategic Studies and Research"}},{"name":"url","value":{"title":"Facet Publishing","url":"/core/publications/publishing-partners/facet-publishing","ariaLabel":"Facet Publishing"}}],"column2Heading":" Partners (cont.)","column2StaticPagesOrUrls":[{"name":"url","value":{"title":"Foundation Books","url":"/core/publications/publishing-partners/foundation-books","ariaLabel":"Foundation Books"}},{"name":"url","value":{"title":"Intersentia","url":"/core/publications/publishing-partners/intersentia","ariaLabel":"Intersentia"}},{"name":"url","value":{"title":"ISEAS-Yusof Ishak Institute","url":"/core/publications/publishing-partners/iseas","ariaLabel":"ISEAS-Yusof Ishak Institute"}},{"name":"url","value":{"title":"Jagiellonian University Press","url":"/core/publications/publishing-partners/jagiellonian-university-press","ariaLabel":"Jagiellonian University Press"}},{"name":"url","value":{"title":"Royal Economic Society","url":"/core/publications/publishing-partners/royal-economic-society","ariaLabel":"Royal Economic Society"}},{"name":"url","value":{"title":"Unisa Press","url":"/core/publications/publishing-partners/unisa-press","ariaLabel":"Unisa Press"}},{"name":"url","value":{"title":"The University of Adelaide Press","url":"/core/publications/publishing-partners/university-adelaide-press","ariaLabel":"The University of Adelaide Press"}},{"name":"url","value":{"title":"Wits University Press","url":"/core/publications/publishing-partners/wits-university-press","ariaLabel":"Wits University Press"}}],"slug":"partners"}}]}},{"name":"navigationBarCategory","value":{"title":"Services","navigationBarArea":[{"name":"navigationBarArea","value":{"title":"About","hubPage":{"name":"staticPage","value":{"pageTitle":"About","pageBlurb":"Cambridge Core is the home of academic content from Cambridge University Press. Built with our users in mind our online platform has been designed to help readers and researchers to make fast and easy journeys to a vast range of valuable content. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"about","introTitle":"About","introText":"Cambridge Core is the home of academic content from Cambridge University Press. Built with our users in mind our online platform has been designed to help readers and researchers to make fast and easy journeys to a vast range of valuable content.","contentBlocks":[],"areas":["about"]}},"column1Heading":"About Cambridge Core","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"About","pageBlurb":"Cambridge Core is the home of academic content from Cambridge University Press. Built with our users in mind our online platform has been designed to help readers and researchers to make fast and easy journeys to a vast range of valuable content. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"about","introTitle":"About","introText":"Cambridge Core is the home of academic content from Cambridge University Press. Built with our users in mind our online platform has been designed to help readers and researchers to make fast and easy journeys to a vast range of valuable content.","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Accessibility","pageBlurb":"We want everyone who visits Cambridge Core to feel welcome and find the experience rewarding.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"accessibility","introTitle":"Accessibility","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"CrossMark policy","pageBlurb":"By applying the CrossMark logo, Cambridge University Press is committing to maintaining the content it publishes and to alerting readers to changes if and when they occur. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"crossmark-policy","introTitle":"CrossMark policy","introText":"CrossMark is a multi-publisher initiative to provide a standard way for readers to locate the current version of a piece of content. By applying the CrossMark logo, Cambridge University Press is committing to maintaining the content it publishes and to alerting readers to changes if and when they occur. Clicking on the CrossMark logo will tell you the current status of a document and may also give you additional publication record information about the document.","areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Ethical Standards","pageBlurb":"As a leading publisher of scholarly journals and books, Cambridge University Press is committed to meeting high standards of ethical behaviour at all stages of the publication process. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"ethical-standards","introTitle":"Ethical Standards","contentBlocks":[],"areas":["about"]}}],"column2Heading":"Environment and sustainability","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Environment and sustainability","pageBlurb":"At Cambridge, we publish research and share knowledge that informs, educates, and inspires others to drive positive environmental change.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"environment-and-sustainability","introTitle":"Environment and sustainability","introText":"At Cambridge, we publish research and share knowledge that informs, educates, and inspires others to drive positive environmental change.","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Reducing print","pageBlurb":"As a department of the University of Cambridge, we are committed to reducing our carbon footprint. A significant reduction to the print component of our journals publishing activity will be a key element of realising this commitment. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"reducing-print","introTitle":"Reducing print","introText":"As a department of the University of Cambridge, we are committed to reducing our carbon footprint. A significant reduction to the print component of our journals publishing activity will be a key element of realising this commitment. ","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Journals moving to online only","pageBlurb":"As we work to reduce print in our journal publishing, more of our journals are moving to online only.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"journals-moving-to-online-only","introTitle":"Journals moving to online only","introText":"As we work to reduce print in our journal publishing, more of our journals are moving to online only.","contentBlocks":[],"areas":["about"]}}],"column3Heading":"Guides","column3StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"User guides","pageBlurb":"Below you will find a range of video user guides to help you navigate and use accounts on Cambridge Core.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"user-guides","introTitle":"User guides","introText":"Below you will find a range of video user guides to help you navigate and use accounts on Cambridge Core.","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"User Guides and Videos","pageBlurb":"The content in the page offers video guidance to help you make full use of the features available on Cambridge Core.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"user-guides-and-videos","introTitle":"User Guides and Videos","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Support Videos","pageBlurb":"For helpful content to support your Transformative Agreement for librarians and administrators, check out our LIVE WORKSHOP recording and slide deck below.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"support-videos","introTitle":"Support Videos","contentBlocks":[],"areas":["about"]}},{"name":"staticPage","value":{"pageTitle":"Training","pageBlurb":"Welcome to Training Services for Cambridge Core.  ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"training","introTitle":"Training","introText":"Welcome to Training Services for Cambridge Core.                                                      ","contentBlocks":[],"areas":["about"]}}],"column4Heading":"Help","column4StaticPagesOrUrls":[{"name":"url","value":{"title":"Cambridge Core help","url":"https://corehelp.cambridge.org/","ariaLabel":"Cambridge Core help pages"}},{"name":"url","value":{"title":"Contact us","url":"https://corehelp.cambridge.org/hc/en-gb/p/contact-information","ariaLabel":"This will take you to a contact form for technical support","pageBlurb":"Visit our knowledge bases and contact our Technical Support team for direct help and support with any technical issues or questions you may have","thumbnailImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}}}},{"name":"url","value":{"title":"Technical support","url":"https://corehelp.cambridge.org/hc/en-gb/requests/new","ariaLabel":"Technical support diagnostics report"}}],"slug":"about"}},{"name":"navigationBarArea","value":{"title":"Agents","hubPage":{"name":"staticPage","value":{"pageTitle":"Services for agents","pageBlurb":"Cambridge University Press is pleased to work with subscription agents in every country, whether those agents are multinational or specialised local companies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"services-for-agents","introTitle":"Services for agents","contentBlocks":[],"areas":["agents"]}},"column1Heading":"Services for agents","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Services for agents","pageBlurb":"Cambridge University Press is pleased to work with subscription agents in every country, whether those agents are multinational or specialised local companies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"services-for-agents","introTitle":"Services for agents","contentBlocks":[],"areas":["agents"]}},{"name":"staticPage","value":{"pageTitle":"Journals for agents","pageBlurb":"Cambridge is pleased to work with subscription agents in every country, whether those agents are multinational or specialised local companies. We now offer full content access to all Cambridge journals on Cambridge Core, including back issues.","linkImage":{"title":"GettyImages-2148427272","description":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1BYMYdALXI4YZz9sJKegO1/0c50d6ede207418c796a9b5691b5c20a/GettyImages-2148427272.jpg","details":{"size":185617,"image":{"width":1536,"height":1024}},"fileName":"GettyImages-2148427272.jpg","contentType":"image/jpeg"}},"altText":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","slug":"journals-for-agents","introTitle":"Journals for agents","introText":"Cambridge is pleased to work with subscription agents in every country, whether those agents are multinational or specialised local companies. We now offer full content access to all Cambridge journals on Cambridge Core, including access to back issues.","contentBlocks":[],"areas":["agents"]}},{"name":"staticPage","value":{"pageTitle":"Books for agents","pageBlurb":"There are a number of different ebook purchasing options available on Cambridge Core to suit all your needs.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"books-for-agents","introTitle":"Books for agents","introText":"There are a number of different ebook purchasing options available on Cambridge Core to suit all your needs.","contentBlocks":[],"areas":["agents"]}},{"name":"staticPage","value":{"pageTitle":"Price list","pageBlurb":"Cambridge University Press 2025 price lists for journals, books and Elements. For subscription agents and academic institutions.","linkImage":{"title":"GettyImages-2148427272","description":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1BYMYdALXI4YZz9sJKegO1/0c50d6ede207418c796a9b5691b5c20a/GettyImages-2148427272.jpg","details":{"size":185617,"image":{"width":1536,"height":1024}},"fileName":"GettyImages-2148427272.jpg","contentType":"image/jpeg"}},"altText":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","slug":"price-list","introTitle":"Price list","introText":"Cambridge University Press 2025 price lists for journals, books and Elements. For subscription agents and academic institutions.","contentBlocks":[],"areas":["librarians","agents"]}}],"slug":"agents"}},{"name":"navigationBarArea","value":{"title":"Authors","hubPage":{"name":"staticPage","value":{"pageTitle":"Authors","pageBlurb":"Cambridge University Press works closely with the global academic community to deliver the highest quality, peer-reviewed content, as well as a portfolio of innovative tools and services to advance learning and research.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"authors","introTitle":"Authors","introText":"Cambridge University Press works closely with the global academic community to deliver the highest quality, peer-reviewed content, as well as a portfolio of innovative tools and services to advance learning and research.","contentBlocks":[],"areas":[]}},"column1Heading":"Journals","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Journals","pageBlurb":"Cambridge University Press is proud to publish many of the world's leading journals across a wide range of subject areas in the humanities, social sciences and STM fields. We currently publish more than 400 peer-reviewed academic journals.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"journals","introTitle":"Journals","introText":"Cambridge University Press is proud to publish many of the world's leading journals across a wide range of subject areas in the humanities, social sciences and STM fields. We currently publish more than 400 peer-reviewed academic journals.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Journal publishing statistics","pageBlurb":"During 2022 we are updating our journal home pages to provide new statistics on each journal's publishing activity. On this page we provide some information about how these statistics are calculated.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"journal-publishing-statistics","introTitle":"Journal publishing statistics","introText":"During 2022 we are updating our journal home pages to provide new statistics on each journal's publishing activity. On this page we provide some information about how these statistics are calculated.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Corresponding author","pageBlurb":"The Corresponding Author is the person who handles the manuscript and correspondence during the publication process, including approving the article proofs.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"corresponding-author","introTitle":"Corresponding author","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Seeking permission to use copyrighted material","pageBlurb":"If your article contains any material in which you do not own copyright, including figures, charts, tables, photographs or excerpts of text, you must obtain permission from the copyright holder to reuse that material.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"seeking-permission-to-use-copyrighted-material","introTitle":"Seeking permission to use copyrighted material","introText":"If your article contains any material in which you do not own copyright, including figures, charts, tables, photographs or excerpts of text, you must obtain permission from the copyright holder to reuse that material. As the author it is your responsibility to obtain this permission and pay any related fees, and you will need to send us a copy of each permission statement at acceptance.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Publishing supplementary material","pageBlurb":"Supplementary materials are hosted online with the main article, and can include data sets, video files, sound clips, figures or tables.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-supplementary-material","introTitle":"Publishing supplementary material","introText":"Many of our journals encourage authors to submit and publish supplementary materials that are not essential for inclusion or cannot be accommodated in the main article, but would be of benefit to the reader. Supplementary materials are hosted online with the main article, and can include data sets, video files, sound clips, figures or tables. The main text of the article should stand alone without the supplementary material.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Writing an effective abstract","pageBlurb":"The title, abstract, and keywords you select for your manuscript play an important part in the discovery of your article after publication. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"writing-an-effective-abstract","introTitle":"Writing an effective abstract","introText":"The title, abstract, and keywords you select for your manuscript play an important part in the discovery of your article after publication. Since many researchers rely on search engines such as Google to find content relevant to their field, the careful selection of keywords in all of these can have a large impact on the life of an article, extending from readership through citation.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Journal production - FAQs","pageBlurb":"If you have any questions on the production of your article check out our frequently asked questions.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"journal-production-faqs","introTitle":"Journal production - FAQs","introText":"Please click on a question below to go directly to the answer. If your question is not covered here, please direct it to your chosen journal’s editor or content manager.","contentBlocks":[],"areas":["authors"]}}],"column2Heading":"Journals (cont.)","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Author affiliations","pageBlurb":"Author affiliations FAQs. A guide to determining, submitting and the display of your affiliations as a Cambridge journal author.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"author-affiliations","introTitle":"Author affiliations","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Co-reviewing policy","pageBlurb":"In journals that allow co-reviewing, an invited reviewer can work with a more junior colleague to review a manuscript for the purpose of reviewer training.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"co-reviewing-policy","introTitle":"Co-reviewing policy","introText":"In journals that allow co-reviewing, an invited reviewer can work with a more junior colleague to review a manuscript for the purpose of reviewer training. This allows the co-reviewer to gain experience with the review process and become a viable reviewer for a journal.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Digital Author Publishing Agreement - FAQs","pageBlurb":"Find out more about our digital author publishing agreements by checking out our frequently asked questions. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"digital-author-publishing-agreement-faqs","introTitle":"Digital Author Publishing Agreement FAQs","introText":"Click on a question below to see the corresponding answer. If your question is not covered here, please direct it to our author contracts team.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Anonymising your manuscript","pageBlurb":"To maintain this anonymity, authors will need to remove any details that may reveal their identity from their manuscript, before it is reviewed.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"anonymising-your-manuscript","introTitle":"Anonymising your manuscript","introText":"Some of our journals use a double-anonymous peer review process, meaning neither the author nor the reviewers know the identity of each other. To maintain this anonymity, authors will need to remove any details that may reveal their identity from their manuscript, before it is reviewed.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Publishing open access","pageBlurb":"Publishing your research Gold Open Access (Gold OA) helps to advance discovery by allowing anyone, anywhere to find, read, and benefit from your research.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-open-access","introTitle":"Publishing open access","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Converting your article to open access","pageBlurb":"Find information about publishing your article as open access","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"converting-your-article-to-open-access","introTitle":"Converting your article to open access","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Publishing Open Access - webinars","pageBlurb":"This session focuses on the basics of Open Access (OA) and presents evidence of the increased impact of choosing this option.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"publishing-open-access-webinars","introTitle":"Publishing Open Access - webinars","contentBlocks":[],"areas":["authors"]}}],"column3Heading":"Journals (cont.)","column3StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Preparing and submitting your paper","pageBlurb":"Each journal published by Cambridge University Press is unique, pursuing a specific set of editorial aims.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"preparing-and-submitting-your-paper","introTitle":"Preparing and submitting your paper","introText":"Each journal published by Cambridge University Press is unique, pursuing a specific set of editorial aims. We strongly encourage authors to read the Instructions for Contributors document associated with their chosen journal as early as possible during the manuscript preparation process, to ensure full understanding of the journal's audience and scope. This document can be found via the homepage of each journal on Cambridge Core.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Publishing an accepted paper","pageBlurb":"Once a paper is accepted by a journal's editor(s) it will be sent to Cambridge University Press to be prepared for publication. Efficiency, accuracy and quality are at the heart of this process.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-an-accepted-paper","introTitle":"Publishing an accepted paper","introText":"Once a paper is accepted by a journal's editor(s) it will be sent to Cambridge University Press to be prepared for publication. Efficiency, accuracy and quality are at the heart of this process. Authors will have access to a dedicated content manager, who will be available to answer questions and offer support as their article progresses through the steps towards publication.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Promoting your published paper","pageBlurb":"Cambridge University Press is committed to making sure your paper reaches a broad international audience in order to maximise its scholarly impact. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"promoting-your-published-paper","introTitle":"Promoting your published paper","introText":"Cambridge University Press is committed to making sure your paper reaches a broad international audience in order to maximise its scholarly impact. Our marketing team runs regular promotions through a wide variety of marketing channels and Cambridge Core is designed to make your article easy to discover, access, read and cite.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Measuring impact","pageBlurb":"This page explains the most common metrics used within scholarly publishing to measure impact at a journal level.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"measuring-impact","introTitle":"Measuring impact","introText":"This page explains the most common metrics used within scholarly publishing to measure impact at a journal level.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Journals artwork guide","pageBlurb":"This guide will explain how to effectively prepare your artwork for electronic submission to our journals. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"journals-artwork-guide","introTitle":"Journals artwork guide","introText":"Submitting your illustrations, pictures and other artwork (such as multimedia and supplementary files) in an electronic format helps us produce your work to the best possible standards, ensuring accuracy, clarity, accessibility, and a high level of detail.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Using ORCID","pageBlurb":"ORCID is a not-for-profit organization governed by an elected board. It provides a persistent digital identifier (an ORCID ID) for individual researchers.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"using-orcid","introTitle":"Using ORCID","introText":"ORCID is a not-for-profit organization governed by an elected board. It provides a persistent digital identifier (an ORCID ID) for individual researchers, and a registry of those identifiers, to support automated linkages between researchers and their professional activities and affiliations, through integration in research workflows such as manuscript and grant submission.","contentBlocks":[],"areas":["authors"]}}],"column4Heading":"Books","column4StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Books","pageBlurb":"Cambridge University Press works closely with the global academic community to deliver the highest quality, peer-reviewed content, as well as a portfolio of innovative tools and services to advance learning and research.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"books","introTitle":"Books","introText":"Cambridge University Press works closely with the global academic community to deliver the highest quality, peer-reviewed content, as well as a portfolio of innovative tools and services to advance learning and research. Together with Cambridge University, our mission is to advance the work of researchers, students, lecturers, and librarians worldwide.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Marketing your book","pageBlurb":"Cambridge provides marketing support across all relevant channels and will work closely with your Editor and the Sales team to ensure that your book reaches its target audience.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"marketing-your-book","introTitle":"Marketing your book","introText":"You will be assigned a dedicated Marketing Specialist who will be in touch soon to discuss the marketing plans for your book. They will provide marketing support across all relevant channels and will work closely with your Editor and the Sales team to ensure that your book reaches its target audience.","contentBlocks":[],"areas":["authors"]}},{"name":"staticPage","value":{"pageTitle":"Author guides for Cambridge Elements","pageBlurb":"Cambridge Elements user guides. Author guides, FAQs and style guides: find all the information about writing Cambridge Elements here.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"elements-user-guides","introTitle":"User guides for Cambridge Elements","introText":"Cambridge Elements user guides. Author guides, FAQs and style guides: find all the information about writing Cambridge Elements here.","contentBlocks":[],"areas":["authors"]}}],"slug":"authors"}},{"name":"navigationBarArea","value":{"title":"Corporates","hubPage":{"name":"staticPage","value":{"pageTitle":"Services for corporates","pageBlurb":"Discover more about commercial reprints, advertising, sponsorship, special sales, foreign rights and permissions.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"services-for-corporates","introTitle":"Services for corporates","contentBlocks":[],"areas":[]}},"column1Heading":"Corporates","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Commercial reprints","pageBlurb":"Providing hard copy and electronic reprints of articles in the majority of our journals. Our commercial sales team can provide quotes for permission to translate articles and the reuse of our journal content for a corporate client.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"commercial-reprints","introTitle":"Commercial reprints","introText":"Cambridge University Press provides hard copy and electronic reprints of articles in the majority of our journals. Our commercial sales team can also provide quotes for permission to translate articles and any other permission that involves the reuse of our journal content for a corporate client.","contentBlocks":[],"areas":["corporates"]}},{"name":"staticPage","value":{"pageTitle":"Advertising","pageBlurb":"With over 400 titles to choose from, our extensive list of journals – spanning 45 subject areas – means you will always reach your target audience.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"advertising","introTitle":"Advertising","introText":"With over 400 titles to choose from, our extensive list of journals – spanning 45 subject areas – means you will always reach your target audience.","contentBlocks":[],"areas":["corporates"]}},{"name":"staticPage","value":{"pageTitle":"Sponsorship","pageBlurb":"Sponsorship of journals, supplements, special or themed issues, collections and questions allows your organisation to work with Cambridge University Press Publishing.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"sponsorship","introTitle":"Sponsorship","introText":"Sponsorship of journals, supplements, special or themed issues, collections and questions allows your organisation to work with Cambridge University Press Publishing.","contentBlocks":[],"areas":["corporates"]}},{"name":"staticPage","value":{"pageTitle":"Book special sales","pageBlurb":"We offer a range of flexible print and digital customisation options to suit your organization’s needs.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"book-special-sales","introTitle":"Book special sales","introText":"We offer a range of flexible print and digital customisation options to suit your organization’s needs, working with not-for-profit organisations, educational institutions, corporate customers and pharmaceutical companies to creating co-branded editions with our partners.","contentBlocks":[],"areas":["corporates"]}},{"name":"staticPage","value":{"pageTitle":"Contact us","pageBlurb":"Contact Cambridge University Press for Book special sales, advertising, sponsorship and commercial reprints.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"contact-us","introTitle":"Contact us","introText":"Contact Cambridge University Press for Book special sales, advertising, sponsorship and commercial reprints.","contentBlocks":[],"areas":["corporates"]}}],"slug":"corporates"}},{"name":"navigationBarArea","value":{"title":"Editors","hubPage":{"name":"staticPage","value":{"pageTitle":"Editors","pageBlurb":"EditorTo empower our editors as advocates we provide dedicated tools, support and training throughout the publishing process, to raise journal visibility and cement its reputation in the field.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"editors","introTitle":"Editors","introText":"Editors make important contributions to the research community and ensure their journals feature work of the highest calibre. To empower our editors as advocates we provide dedicated tools, support and training throughout the publishing process, to raise journal visibility and cement its reputation in the field.","contentBlocks":[],"areas":[]}},"column1Heading":"Information","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Journal development","pageBlurb":"The editor is the journal’s ambassador, here are some of the ways to support the journal's development.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"journal-development","introTitle":"Journal development","introText":"Supporting the journal. The editor is the journal’s ambassador, here are some of the ways to support the journal's development:","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Peer review for editors","pageBlurb":"An Online Peer Review System (OPRS), works to help editors carry out the peer review process. A submission is anonymised before being uploaded to allow for unbiased assessment. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"peer-review-for-editors","introTitle":"Peer review for editors","introText":"An Online Peer Review System (OPRS), works to help editors carry out the peer review process. A submission is anonymised before being uploaded to allow for unbiased assessment. ","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Open access for editors","pageBlurb":"Open access (OA) is an important way to make research findings freely available for anyone to access and view.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"open-access-for-editors","introTitle":"Open access for editors","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Policies and guidelines","pageBlurb":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount, and as our editors, you are our partners in supporting this mission. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"policies-and-guidelines","introTitle":"Policies and guidelines","introText":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount, and as our editors, you are our partners in supporting this mission. ","contentBlocks":[],"areas":["editors"]}}],"column2Heading":"Resources","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"The editor's role","pageBlurb":"Our editors ensure that their journals continue to evolve and have an impact on their research field.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"the-editors-role","introTitle":"The editor's role","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Open research for editors","pageBlurb":"Open research refers to a range of practices that are dramatically improving how researchers are publishing and sharing their work.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"open-research-for-editors","introTitle":"Open research for editors","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Engagement and promotion","pageBlurb":"There are a number of ways an editor can support the visibility and dissemination of their journal(s).","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"engagement-and-promotion","introTitle":"Engagement and promotion","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Blogging","pageBlurb":"Blogs are great for profiling new and exciting research or features in the journal. It is also a key way to attract search traffic.","linkImage":{"title":"A Guide to Blogging","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/5sLUiDKTGCXxU9GKcPcHSA/36bf1c9ca78ae1e2a47293d27fb32647/A_Guide_to_Blogging.png","details":{"size":56411,"image":{"width":1824,"height":894}},"fileName":"A Guide to Blogging.png","contentType":"image/png"}},"altText":"Editor Resources A Guide to Blogging","slug":"blogging","introTitle":"Blogging","introText":"Blogs are great for profiling new and exciting research or features in the journal. It is also a key way to attract search traffic.","contentBlocks":[],"areas":["editors"]}},{"name":"staticPage","value":{"pageTitle":"Social media","pageBlurb":"Social media is a great way to raise the profile and visibility of your journal. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"social-media","introTitle":"Social media","introText":"Social media is a great way to raise the profile and visibility of your journal.                    ","contentBlocks":[],"areas":["editors"]}}],"slug":"editors"}},{"name":"navigationBarArea","value":{"title":"Librarians","hubPage":{"name":"staticPage","value":{"pageTitle":"Librarians","pageBlurb":"Cambridge Core has primarily been designed to help your readers and researchers to make fast and easy journeys to valuable content.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"librarians","introTitle":"Librarians","contentBlocks":[],"areas":[]}},"column1Heading":"Information","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open Access for Librarians","pageBlurb":"Welcome to your librarian hub for Open Access at Cambridge University Press. Open Access (OA) can be quite daunting, so we've created this page to help you navigate our different resources.","linkImage":{"title":"OA Librarian Website Header Banner","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/6eyfpqZUi4KEeOt7lsGKJg/41ef6beae2eee1a8304a95313aa08b64/OA-Librarian-Website-Header-Banner-Image-1-1500x300.jpg","details":{"size":348959,"image":{"width":1500,"height":300}},"fileName":"OA-Librarian-Website-Header-Banner-Image-1-1500x300.jpg","contentType":"image/jpeg"}},"altText":"Open Access for Librarians: get all your information about OA from Cambridge","slug":"open-access-for-librarians","introTitle":"Open Access for Librarians","introText":"Welcome to your librarian hub for Open Access at Cambridge University Press. Open Access (OA) can be quite daunting, so we've created this page to help you navigate our different resources. Learn about our OA policies, what we publish OA and how you can support your institution when publishing OA journals, books, monographs and Elements.","contentBlocks":[],"areas":["librarians"]}},{"name":"url","value":{"title":"Transformative agreements","url":"https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements","ariaLabel":"Transformative agreements"}},{"name":"staticPage","value":{"pageTitle":"Transformative Agreements - FAQs","pageBlurb":"A transformative agreement is the combined provision of two services for one cost: OA publishing and reading access to all subscription journals.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"transformative-agreements-faqs","introTitle":"Transformative Agreements - FAQs","introText":"A transformative agreement is the combined provision of two services for one cost: OA publishing and reading access to all subscription journals.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Evidence based acquisition","pageBlurb":"Academic institutions can access Cambridge University Press ebook collections via an EBA - access more than 50,000 titles published by Cambridge and our publishing partners, before making decisions on which titles to buy in perpetuity. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"evidence-based-acquisition","introTitle":"Evidence based acquisition","introText":"Academic institutions can now access Cambridge University Press ebook collections via an EBA. Under this model, institutions are given access to an extensive online collection of more than 50,000 titles published by Cambridge and our publishing partners, before making decisions on which titles to buy in perpetuity. ","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"ebook news & updates","pageBlurb":"Welcome to your Cambridge University Press ebooks hub. This is your single reference page where you can learn the latest news on ebooks and Cambridge Core platform updates. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"ebook-news-and-updates","introTitle":"ebook news & updates","introText":"Welcome to your Cambridge University Press ebooks hub. This is your single reference page where you can learn the latest news on ebooks and Cambridge Core platform updates. ","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Cambridge libraries of the world podcast","pageBlurb":"A brand-new podcast series for librarians to listen to and learn about topics at the heart of academic discourse, via interviews conducted with professors, researchers, students, librarians, and Cambridge personnel.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"cambridge-libraries-of-the-world-podcast","introTitle":"Cambridge libraries of the world podcast","introText":"A brand-new podcast series for librarians to listen to and learn about topics at the heart of academic discourse, via interviews conducted with professors, researchers, students, librarians, and Cambridge personnel. ","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Purchasing models","pageBlurb":"Discover more about the purchasing options available to institutions for the content hosted on Cambridge Core.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"purchasing-models","introTitle":"Purchasing models","introText":"Below are a number of different purchasing options available to institutions for the content hosted on Cambridge Core.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Journals Publishing Updates","pageBlurb":"Another year of development for Cambridge University Press's journals publishing sees big changes coming for 2025.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"journals-publishing-updates","introTitle":"Journals Publishing Updates","introText":"Another year of development for Cambridge University Press's journals publishing sees big changes coming for 2025. Below is a preview of our list changes, including our new titles, titles that are converting to an open access publishing model, and details of our new series.","contentBlocks":[],"areas":["librarians"]}}],"column2Heading":"Products","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Cambridge frontlist","pageBlurb":"Our frontlist ebook collection contains the most recent, award-winning publishing from Cambridge University Press.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"cambridge-frontlist","introTitle":"Cambridge frontlist","introText":"Our frontlist ebook collection contains the most recent, award-winning publishing from Cambridge University Press.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Cambridge journals digital archive","pageBlurb":"The Cambridge journals digital archive provides instant online access to two centuries of academic excellence and publishing history from Cambridge University Press. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"cambridge-journals-digital-archive","introTitle":"Cambridge journals digital archive","introText":"The Cambridge journals digital archive provides instant online access to two centuries of academic excellence and publishing history from Cambridge University Press. Our constantly growing digital archive takes knowledge off the shelf and makes it readily available online, alongside our current Cambridge Core content.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Hot topics","pageBlurb":"Hot Topics from Cambridge are a series of bespoke ebook collections reflecting the latest research trends.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"hot-topics","introTitle":"Hot topics","introText":"Hot Topics from Cambridge are a series of bespoke ebook collections comprising titles from across a wide range of subjects, created to reflect the latest research trends. Drawing from the humanities and social sciences, in addition to the science, technology and medical disciplines, the collections highlight Cambridge’s depth of publishing.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Other digital products","pageBlurb":"Cambridge University Press publishes a wide variety of academic content online, some of which currently sits on other platforms.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"other-digital-products","introTitle":"Other digital products","introText":"Cambridge University Press publishes a wide variety of academic content online, some of which currently sits on other platforms.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Perpetual access products","pageBlurb":"Gain perpetual access to Cambridge University Press resources for a one-time payment. Details of products an offers can be found on this page.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"perpetual-access-products","introTitle":"Perpetual access products","introText":"Gain perpetual access to Cambridge University Press resources for a one-time payment. Details of products an offers can be found on this page.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Price list","pageBlurb":"Cambridge University Press 2025 price lists for journals, books and Elements. For subscription agents and academic institutions.","linkImage":{"title":"GettyImages-2148427272","description":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1BYMYdALXI4YZz9sJKegO1/0c50d6ede207418c796a9b5691b5c20a/GettyImages-2148427272.jpg","details":{"size":185617,"image":{"width":1536,"height":1024}},"fileName":"GettyImages-2148427272.jpg","contentType":"image/jpeg"}},"altText":"A row of books on a metal, library style, bookshelf. the books are view from the rear and a hand is reaching and picking a book.","slug":"price-list","introTitle":"Price list","introText":"Cambridge University Press 2025 price lists for journals, books and Elements. For subscription agents and academic institutions.","contentBlocks":[],"areas":["librarians","agents"]}},{"name":"staticPage","value":{"pageTitle":"Developing country programme","pageBlurb":"Cambridge University Press has been working to improve the lives of people living in less economically developed parts of the world for nearly 500 years.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"developing-country-programme","introTitle":"Developing country programme","introText":"Cambridge University Press has been working to improve the lives of people living in less economically developed parts of the world for nearly 500 years.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"New content","pageBlurb":"Discover new books and journals published by Cambridge University Press on Cambridge Core. Including information on titles moving to online only.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"new-content","introTitle":"New content","introText":"Discover new books and journals published by Cambridge University Press on Cambridge Core. Including information on titles moving to online only.","contentBlocks":[],"areas":["librarians"]}}],"column3Heading":"Tools","column3StaticPagesOrUrls":[{"name":"url","value":{"title":"Eligibility checker","url":"/core/eligibility-checker","ariaLabel":"Eligibility checker"}},{"name":"url","value":{"title":"Transformative agreements","url":"https://www.cambridge.org/core/services/open-access-policies/read-and-publish-agreements","ariaLabel":"Transformative agreements"}},{"name":"url","value":{"title":"KBART","url":"https://www.cambridge.org/core/services/librarians/kbart","ariaLabel":"KBART"}},{"name":"url","value":{"title":"MARC records","url":"https://www.cambridge.org/core/services/librarians/marc-records","ariaLabel":"MARC records"}},{"name":"staticPage","value":{"pageTitle":"Using MARCEdit for MARC records","pageBlurb":"Using MARCEdit for MARC records. To open/use MARC records from Cambridge Core, we recommend you use MARCEdit.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"using-marcedit-for-marc-records","introTitle":"Using MARCEdit for MARC records","introText":"Using MARCEdit for MARC records. To open/use MARC records from Cambridge Core, we recommend you use MARCEdit.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Inbound OpenURL specifications","pageBlurb":"Specifications to link directly to an article, book, chapter or journal page on Cambridge Core using an OpenURL link.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"inbound-openurl-specifications","introTitle":"Inbound OpenURL specifications","introText":"Use the following specifications to link directly to an article, book, chapter or journal page on Cambridge Core using an OpenURL link.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"COUNTER report types","pageBlurb":"A librarian guide to COUNTER reporting and report types, as well as turnaway reports for books and journals.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"counter-report-types","introTitle":"COUNTER report types","introText":"A librarian guide to COUNTER reporting and report types, as well as turnaway reports for books and journals.","contentBlocks":[],"areas":["librarians"]}}],"column4Heading":"Resources","column4StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Catalogues and resources","pageBlurb":"Discover resources that have been created to help you get the most value out of content on Cambridge Core. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"catalogues-and-resources","introTitle":"Catalogues and resources","introText":"These resources have been created to help you get the most value out of content on Cambridge Core. By promoting your Cambridge publications, you can increase usage and help readers discover new titles.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Making the most of your EBA","pageBlurb":"Evidence Based Acquisition (EBA) is an ebook purchase model that allows you to make informed decisions about which titles to own based on usage data.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"making-the-most-of-your-eba","introTitle":"Making the most of your EBA","introText":"Evidence Based Acquisition (EBA) is an ebook purchase model that allows you to make informed decisions about which titles to own based on usage data. We want to make sure that your EBA with Cambridge is a success, so we’ve put together a selection of resources to help with both raising awareness and ensuring discoverability of content.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Posters","pageBlurb":"Below are all posters for ebooks and journals available for you to download in two sizes - A3 and A4.","linkImage":{"title":"Posters","description":"An image showing the walkway between bookshelves in a library. The word \"Posters\" is superimposed over. ","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/3gpLCEAqNNQ89VoZWA6Aot/6ee8948bc1bd70553b19070431fc0818/Posters.jpg","details":{"size":64672,"image":{"width":424,"height":305}},"fileName":"Posters.jpg","contentType":"image/jpeg"}},"altText":"An image showing the walkway between bookshelves in a library. The word \"Posters\" is superimposed over. ","slug":"posters","introTitle":"Posters","introText":"Below are all posters for ebooks and journals available for you to download in two sizes - A3 and A4.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Leaflets and brochures","pageBlurb":"View and download our latest leaflets, brochures and catalogues, for products and resources, on Cambridge Core.","linkImage":{"title":"leaflets-and-brochures","description":"A close up of fingertips holding a ballpoint pen. The pen hovers about a notepad. Superimposed are the words \"Leaflets & Brochures\".","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/2vuFy7rj3ER6B7sH4iCAQW/f6ba1d6a1e00c2dbbf6e36e83a082e22/leaflets-and-brochures.jpg","details":{"size":42247,"image":{"width":424,"height":305}},"fileName":"leaflets-and-brochures.jpg","contentType":"image/jpeg"}},"altText":"A close up of fingertips holding a ballpoint pen. The pen hovers about a notepad. Superimposed are the words \"Leaflets & Brochures\".","slug":"leaflets-and-brochures","introTitle":"Leaflets and brochures","introText":"View and download our latest leaflets, brochures and catalogues, for products and resources, on Cambridge Core.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Additional resources","pageBlurb":"Download the Cambridge Core desktop background and logo to use on your library screens and in publicity material.","linkImage":{"title":"Additional-resources","description":"A close up of hands typing on a computer keyboard. Superimposed over the image are the words \"Additional Resources\". ","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7IT9biHnN2eHG5WfwB7zmU/5cf73c7ade25233f31c709e359b2f78c/Additional-resources.jpg","details":{"size":39706,"image":{"width":424,"height":305}},"fileName":"Additional-resources.jpg","contentType":"image/jpeg"}},"altText":"A close up of hands typing on a computer keyboard. Superimposed over the image are the words \"Additional Resources\". ","slug":"additional-resources","introTitle":"Additional resources","introText":"Download the Cambridge Core desktop background and logo to use on your library screens and in publicity material.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Find my sales contact","pageBlurb":"Please find your sales contacts below by the appropriate region. Alternatively you can send general enquiries.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"find-my-sales-contact","introTitle":"Find my sales contact","introText":"Please find your sales contacts below by the appropriate region. Alternatively you can send general enquiries.","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Webinars","pageBlurb":"The author publishing workflow and author outreach activities. How to approve articles using Rightslink Agreement Manager. How to analyse reporting from Cambridge University Press.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"webinars","introTitle":"Webinars","contentBlocks":[],"areas":["librarians"]}},{"name":"staticPage","value":{"pageTitle":"Read and publish resources","pageBlurb":"This page is dedicated to providing resources, messaging and training to help make your deal a success.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"read-and-publish-resources","introTitle":"Read and publish resources","contentBlocks":[],"areas":["librarians"]}}],"slug":"librarians"}},{"name":"navigationBarArea","value":{"title":"Peer review","hubPage":{"name":"staticPage","value":{"pageTitle":"Peer review","pageBlurb":"Peer review is the foundation of quality in research for both books and journals, ensuring that published research is rigorous, ethical and significant to the discipline in question.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"peer-review","introTitle":"Peer review","introText":"Peer review is the foundation of quality in research for both books and journals, ensuring that published research is rigorous, ethical and significant to the discipline in question. On these pages you'll find information about how to review both books and journal articles, as well as some frequently asked questions, notes on ethics in review, and news from Cambridge University Press relating to developments in review.","contentBlocks":[],"areas":[]}},"column1Heading":"Peer review","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"How to peer review journal articles","pageBlurb":"Cambridge University Press has created a guide to give a practical introduction to conducting effective peer reviews, especially for those who are new to the process. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"how-to-peer-review-journal-articles","introTitle":"How to peer review journal articles","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"How to peer review book proposals","pageBlurb":"Cambridge University Press has created a guide to give a practical introduction to conducting effective peer reviews, especially for those who are new to the process.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"how-to-peer-review-book-proposals","introTitle":"How to peer review book proposals","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"How to peer review Registered Reports","pageBlurb":"This page offers guidance for peer reviewers about Registered Reports, a publishing format where a research article is published in two stages, each stage undergoing a separate peer review process.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"how-to-peer-review-registered-reports","introTitle":"How to peer review Registered Reports","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"Peer review FAQs","pageBlurb":"Please note that these frequently asked questions have been written with journal articles in mind, though some may be applicable to reviewing book proposals.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"peer-review-faqs","introTitle":"Peer review FAQs","introText":"Please note that these frequently asked questions have been written with journal articles in mind, though some may be applicable to reviewing book proposals.","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"Ethics in peer review","pageBlurb":"Cambridge University Press is committed to publishing ethics in all areas. Discover more about the five main areas of ethical considerations in peer review.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"ethics-in-peer-review","introTitle":"Ethics in peer review","introText":"Cambridge University Press is committed to publishing ethics in all areas. Discover more about the five main areas of ethical considerations in peer review.","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"Online peer review systems","pageBlurb":"Where you complete and/or submit your review will vary depending on the journal. At Cambridge University Press, we use two online peer review systems called Scholar One and Editorial Manager, where you'll be expected to submit your review.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"online-peer-review-systems","introTitle":"Online peer review systems","introText":"Where you complete and/or submit your review will vary depending on the journal. At Cambridge University Press, we use two online peer review systems called Scholar One and Editorial Manager, where you'll be expected to submit your review. These systems are user-friendly and should walk you through the process of submitting your review.","contentBlocks":[],"areas":["peer-review"]}},{"name":"staticPage","value":{"pageTitle":"A guide to Publons","pageBlurb":"Cambridge University Press has created a guide to Publons to explain what it is and the benefits of using it as a reviewer.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"a-guide-to-publons","introTitle":"A guide to Publons","introText":"Publons has recently been rebranded as the Web of Science Reviewer Recognition Service during its integration with the Web of Science. ","contentBlocks":[],"areas":["peer-review"]}}],"slug":"peer-review"}},{"name":"navigationBarArea","value":{"title":"Publishing ethics","hubPage":{"name":"staticPage","value":{"pageTitle":"Publishing ethics","pageBlurb":"As a leading publisher of scholarly journals and books, Cambridge University Press is committed to meeting high standards of ethical behaviour at all stages of the publication process.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-ethics","introTitle":"Publishing ethics","introText":"As a leading publisher of scholarly journals and books, Cambridge University Press is committed to meeting high standards of ethical behaviour at all stages of the publication process. Our research publishing ethics guidelines outline the publishing ethics responsibilities of Cambridge University Press, authors, peer reviewers and editors.","contentBlocks":[],"areas":[]}},"column1Heading":"Journals ","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Publishing ethics guidelines for journals","pageBlurb":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount. These guidelines outline the best practice principles that we apply to our Academic content. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-ethics-guidelines-journals","introTitle":"Publishing ethics guidelines for journals","introText":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount. These guidelines outline the best practice principles that we apply to our Academic content. We hope these guidelines will be useful to many different groups, including authors, peer reviewers, editors within and outside of Cambridge University Press, societies, publishing partners and funders.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Core editorial policies for journals","pageBlurb":"This page contains information about our publishing principles, research integrity and academic freedom - as well as a number of other aspects of our editorial policies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"core-editorial-policies-journals","introTitle":"Core editorial policies for journals","introText":"This page contains information about our publishing principles, research integrity and academic freedom - as well as a number of other aspects of our editorial policies.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Authorship and contributorship for journals","pageBlurb":"We acknowledge that different disciplines and publication formats have different norms for who is listed as an author.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"authorship-and-contributorship-journals","introTitle":"Authorship and contributorship for journals","introText":"We acknowledge that different disciplines and publication formats have different norms for who is listed as an author. We expect all authors on any content submitted to Cambridge to be in agreement that the authors listed would all be considered authors according to disciplinary norms, and that no authors who would reasonably be considered an author have been excluded. ","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Affiliations for journals","pageBlurb":"Any affiliations should represent the institution(s) at which the research presented was conducted and/or supported and/or approved.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"affiliations-journals","introTitle":"Affiliations for journals","introText":"Any affiliations should represent the institution(s) at which the research presented was conducted and/or supported and/or approved. For non-research content, any affiliations should represent the institution(s) with which each author is currently affiliated.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Research ethics for journals","pageBlurb":"Cambridge University Press expects all contributors to align with ethics of research when researching and writing.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"research-ethics-journals","introTitle":"Research ethics for journals","introText":"Cambridge University Press expects all contributors to align with ethics of research when researching and writing.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Competing interests and funding for journals","pageBlurb":"Authors submitting a journal manuscript to Cambridge University Press, employees, the SAPC, editors and reviewers of Cambridge University Press publications, are required to declare any potential competing interests.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"competing-interests-and-funding-journals","introTitle":"Competing interests and funding for journals","introText":"Authors submitting a journal manuscript to Cambridge University Press, employees, the SAPC, editors and reviewers of Cambridge University Press publications, are required to declare any potential competing interests that could interfere with the objectivity or integrity of a publication. ","contentBlocks":[],"areas":["publishing-ethics"]}}],"column2Heading":"Journals (cont.)","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Data and supporting evidence for journals","pageBlurb":"We advocate for transparency and openness around data, code, and other materials associated with research, and we are a signatory of the Transparency and Openness Promotion (TOP) Guidelines. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"data-and-supporting-evidence-for-journals","introTitle":"Data and supporting evidence for journals","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Misconduct for journals","pageBlurb":"The principles of research integrity - honesty, transparency, accountability, care and respect - are encompassed by our core editorial policies described above.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"misconduct-journals","introTitle":"Misconduct for journals","introText":"The principles of research integrity - honesty, transparency, accountability, care and respect - are encompassed by our core editorial policies described above.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Corrections, retractions and removals for journals","pageBlurb":"Cambridge University Press policies and information regarding corrections, retractions and removals in published content.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"corrections-retractions-and-removals-journals","introTitle":"Corrections, retractions and removals for journals","introText":"Cambridge University Press policies and information regarding corrections, retractions and removals in published content.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Versions and adaptations for journals","pageBlurb":"Our publications are distributed in many different global, cultural, environmental and economic contexts. We may therefore issue different versions of some of our products in order to cater to these contexts. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"versions-and-adaptations-journals","introTitle":"Versions and adaptations for journals","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Libel, defamation and freedom of expression","pageBlurb":"Freedom of expression is critical to us as academic publishers, but we do not support publishing false statements that harm the reputation of individuals, groups or organisations. Our legal team will address allegations of libel in our publications.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"libel-defamation-and-freedom-of-expression","introTitle":"Libel, defamation and freedom of expression","introText":"Freedom of expression is critical to us as academic publishers, but we do not support publishing false statements that harm the reputation of individuals, groups or organisations. Our legal team will address allegations of libel in any of our publications.","areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Business ethics journals","pageBlurb":"Business ethics: fair access, censorship, marketing communication, advertising, sponsorship, PR / media and metrics, usage and reporting. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"business-ethics-journals","introTitle":"Business ethics journals","introText":"Business ethics: fair access, censorship, marketing communication, advertising, sponsorship, PR / media and metrics, usage and reporting. ","contentBlocks":[],"areas":["publishing-ethics"]}}],"column3Heading":"Books","column3StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Publishing ethics guidelines for books","pageBlurb":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount. These guidelines outline the best practice principles that we apply to our Academic content. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"publishing-ethics-guidelines-books","introTitle":"Publishing ethics guidelines for books","introText":"At Cambridge University Press, the integrity of our academic content and publishing process is paramount. These guidelines outline the best practice principles that we apply to our Academic content. We hope these guidelines will be useful to many different groups, including authors, peer reviewers, editors within and outside of Cambridge University Press, societies, publishing partners and funders. ","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Core editorial policies for books","pageBlurb":"This page contains information about our publishing principles, research integrity and academic freedom - as well as a number of other aspects of our editorial policies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"core-editorial-policies-books","introTitle":"Core editorial policies for books","introText":"This page contains infromation about our publishing principles, research integrity and academic freedom - as well as a number of other aspects of our editorial policies.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Authorship and contributorship for books","pageBlurb":"We acknowledge that different disciplines and publication formats have different norms for who is listed as an author.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"authorship-and-contributorship-books","introTitle":"Authorship and contributorship for books","introText":"We acknowledge that different disciplines and publication formats have different norms for who is listed as an author. We expect all authors on any content submitted to Cambridge to be in agreement that the authors listed would all be considered authors according to disciplinary norms, and that no authors who would reasonably be considered an author have been excluded.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Affiliations for books","pageBlurb":"Any affiliations should represent the institution(s) at which the research presented was conducted and/or supported and/or approved.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"affiliations-books","introTitle":"Affiliations for books","introText":"Any affiliations should represent the institution(s) at which the research presented was conducted and/or supported and/or approved. For non-research content, any affiliations should represent the institution(s) with which each author is currently affiliated.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Research ethics for books","pageBlurb":"Cambridge University Press expects all contributors to align with ethics of research when researching and writing.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"research-ethics-books","introTitle":"Research ethics for books","introText":"Cambridge University Press expects all contributors to align with ethics of research when researching and writing.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Competing interests and funding for books","pageBlurb":"Authors submitting a book or Element manuscript to Cambridge University Press, employees, the SAPC, editors and reviewers of Cambridge University Press publications, must declare any potential competing interests.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"competing-interests-and-funding-books","introTitle":"Competing interests and funding for books","introText":"Authors submitting a book or Element manuscript to Cambridge University Press, employees, the SAPC, editors and reviewers of Cambridge University Press publications, are required to declare any potential competing interests that could interfere with the objectivity or integrity of a publication. ","contentBlocks":[],"areas":["publishing-ethics"]}}],"column4Heading":"Books (cont.)","column4StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Data and supporting evidence for books","pageBlurb":"We advocate for transparency and openness around data, code, and other materials associated with research, and we are a signatory of the Transparency and Openness Promotion (TOP) Guidelines. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"data-and-supporting-evidence-books","introTitle":"Data and supporting evidence for books","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Misconduct for books","pageBlurb":"The principles of research integrity - honesty, transparency, accountability, care and respect - are encompassed by our core editorial policies described above.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"misconduct-books","introTitle":"Misconduct for books","introText":"The principles of research integrity - honesty, transparency, accountability, care and respect - are encompassed by our core editorial policies described above.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Corrections, retractions and removals for books","pageBlurb":"Cambridge University Press policies and information regarding corrections, retractions and removals in published content.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"corrections-retractions-and-removals-books","introTitle":"Corrections, retractions and removals for books","introText":"Cambridge University Press policies and information regarding corrections, retractions and removals in published content.","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Versions and adaptations for books","pageBlurb":"Our publications are distributed in many different global, cultural, environmental and economic contexts. We may therefore issue different versions of some of our products in order to cater to these contexts. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"versions-and-adaptations-books","introTitle":"Versions and adaptations for books","contentBlocks":[],"areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Libel, defamation and freedom of expression","pageBlurb":"Freedom of expression is critical to us as academic publishers, but we do not support publishing false statements that harm the reputation of individuals, groups or organisations. Our legal team will address allegations of libel in our publications.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"libel-defamation-and-freedom-of-expression","introTitle":"Libel, defamation and freedom of expression","introText":"Freedom of expression is critical to us as academic publishers, but we do not support publishing false statements that harm the reputation of individuals, groups or organisations. Our legal team will address allegations of libel in any of our publications.","areas":["publishing-ethics"]}},{"name":"staticPage","value":{"pageTitle":"Business ethics books","pageBlurb":"Business ethics: fair access, censorship, marketing communication, advertising, sponsorship, PR / media and metrics, usage and reporting. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"business-ethics-books","introTitle":"Business ethics books","introText":"Business ethics: fair access, censorship, marketing communication, advertising, sponsorship, PR / media and metrics, usage and reporting. ","contentBlocks":[],"areas":["publishing-ethics"]}}],"slug":"publishing-ethics"}},{"name":"navigationBarArea","value":{"title":"Publishing partners","hubPage":{"name":"staticPage","value":{"pageTitle":"Publishing partnerships","pageBlurb":"Over 220 world-leading societies and presses partner with us on publications including journals, book series, early research and e-books. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"publishing-partnerships","introTitle":"Publishing partnerships","contentBlocks":[],"areas":["publishing-partners"]}},"column1Heading":"Publishing partners","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Publishing partnerships","pageBlurb":"Over 220 world-leading societies and presses partner with us on publications including journals, book series, early research and e-books. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"publishing-partnerships","introTitle":"Publishing partnerships","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"Partner books","pageBlurb":"Books published for our society partners are given the same high quality production as our own publications, and are published throughout the world in print and electronic formats as appropriate.","linkImage":{"title":"PartnersBooks","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/2U1gsLlZ2xJxn8N97MOIAt/2604acf1f3e6274808a94cc3e0562509/PartnersBooks.jpg","details":{"size":32409,"image":{"width":500,"height":300}},"fileName":"PartnersBooks.jpg","contentType":"image/jpeg"}},"altText":"A background of dark blue with the word 'Books' in white text. The background has a pattern of dots forming circles.","slug":"partner-books","introTitle":"Partner books","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"eBook publishing partnerships","pageBlurb":"Publishing partner ebook titles are delivered to customers on Cambridge Core. The publisher logo also appears on each individual book page, to ensure it is always clear that the ebook is published by a publishing partners.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"ebook-publishing-partnerships","introTitle":"eBook publishing partnerships","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"Journal publishing partnerships","pageBlurb":"Cambridge University Press has a proud history of forming productive and enduring publishing relationships with learned societies, universities, and professional associations.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"journal-publishing-partnerships","introTitle":"Journal publishing partnerships","introText":"Cambridge University Press has a proud history of forming productive and enduring publishing relationships with learned societies, universities, and professional associations. Our mission is to unlock people's potential with the best learning and research solutions.","contentBlocks":[],"areas":["publishing-partners"]}}],"column2Heading":"Publishing partners (cont.)","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Journals publishing","pageBlurb":"We pride ourselves on offering a truly collaborative publishing service which has journal development at its core.","linkImage":{"title":"journals publishing","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/3pWJoXbWYEFDv5DOiBQs7I/8c4dedff525b7170cfe631f4f66b88e7/journals_publishing.JPG","details":{"size":7595,"image":{"width":177,"height":155}},"fileName":"journals publishing.JPG","contentType":"image/jpeg"}},"altText":"An image of the outline of a book or journal with grey arrows around it. This is on a white circle, which is against a blue background.","slug":"journals-publishing","introTitle":"Journals publishing ","introText":"We pride ourselves on offering a truly collaborative publishing service which has journal development at its core. The range of services we provide includes:","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"Customer support","pageBlurb":"Our Customer Services teams have units dedicated to library support, handling print fulfilment and claims, as well as online training and access","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, with stars and galaxies. The dark outline of the Cambridge shield is superimposed over the top. ","slug":"customer-support","introTitle":"Customer support","introText":"Our Customer Services teams have units dedicated to library support, handling print fulfilment and claims, as well as online training and access.","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"Membership Services","pageBlurb":"We provide a range of services to our society partners, including access to journal content, discounts on Cambridge University Press publications and customer service support.","linkImage":{"title":"membership services","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/74cDq9dHVmVPJDQwvqN9ed/00f3dfbab0c2b1f79dea2c40dabb47e6/membership_services.JPG","details":{"size":8177,"image":{"width":169,"height":156}},"fileName":"membership services.JPG","contentType":"image/jpeg"}},"altText":"Line image of a person in blue on a white circle background, against a blue square. Around the person is a curved circular arrow. ","slug":"membership-services","introTitle":"Membership Services","introText":"We provide a range of services to the members of our society partners, including access to journal content (current and archival), discounts on Cambridge University Press publications and customer service support to ensure that any queries or problems are resolved quickly.","contentBlocks":[],"areas":["publishing-partners"]}},{"name":"staticPage","value":{"pageTitle":"Our Team","pageBlurb":"If you would like to discuss a potential journal publishing collaboration, please contact us to discuss your requirements or ideas.","linkImage":{"title":"Our team","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/3DbyRtmwHXuXmdpmkQTTd4/fb83164a4b8328415057478c6a81fde7/Our_team.JPG","details":{"size":8368,"image":{"width":184,"height":157}},"fileName":"Our team.JPG","contentType":"image/jpeg"}},"altText":"Line drawing of three people. One is shaded grey and the other two are without fill on a white circle. The circle is against a blue background.","slug":"our-team","introTitle":"Our Team","introText":"If you would like to discuss a potential journal publishing collaboration, please contact us to discuss your requirements or ideas.","contentBlocks":[],"areas":["publishing-partners"]}}],"slug":"publishing-partners"}}]}},{"name":"navigationBarCategory","value":{"title":"Open research","navigationBarArea":[{"name":"navigationBarArea","value":{"title":"Open access policies","hubPage":{"name":"staticPage","value":{"pageTitle":"Open access policies","pageBlurb":"Open access (OA) has become an important way to make research findings freely available for anyone to access and view. Open access serves authors and the wider community by publishing high-quality, peer-reviewed OA content.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"open-access-policies","introTitle":"Open access policies","introText":"Open access (OA) has become an important way to make research findings freely available for anyone to access and view. Open access serves authors and the wider community by publishing high-quality, peer-reviewed OA content. We support and promote all forms of OA that are financially sustainable. Our introduction to open access page provides some essential information about the types of OA we offer.","contentBlocks":[],"areas":["open-research-policies"]}},"column1Heading":"Open access policies","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open research","pageBlurb":"The open research movement seeks to maximise the impact and benefits of research by prioritising barrier-free access to research findings, data and methodologies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"open-research","introTitle":"Open research","introText":"The open research movement seeks to maximise the impact and benefits of research by prioritising barrier-free access to research findings, data and methodologies. Open research reflects a fundamental belief that the pursuit of knowledge benefits directly from collaboration, transparency, rapid dissemination and accessibility.","contentBlocks":[],"areas":["open-access-publishing","open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Open access policies","pageBlurb":"Open access (OA) has become an important way to make research findings freely available for anyone to access and view. Open access serves authors and the wider community by publishing high-quality, peer-reviewed OA content.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"open-access-policies","introTitle":"Open access policies","introText":"Open access (OA) has become an important way to make research findings freely available for anyone to access and view. Open access serves authors and the wider community by publishing high-quality, peer-reviewed OA content. We support and promote all forms of OA that are financially sustainable. Our introduction to open access page provides some essential information about the types of OA we offer.","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Cambridge University Press and Plan S","pageBlurb":"Our goal to transform the journals we publish to open research depends on a continued, widespread move to funding for Open Access (OA) around the world.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"cambridge-university-press-and-plan-s","introTitle":"Cambridge University Press and Plan S","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Text and data mining","pageBlurb":"We believe that text and data mining (TDM) is an important and powerful research tool with incredible potential. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"text-and-data-mining","introTitle":"Text and data mining","introText":"We believe that text and data mining (TDM) is an important and powerful research tool with incredible potential. ","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Preprint policy","pageBlurb":"Discover full details of our Preprint policy at Cambridge University Press ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"preprint-policy","introTitle":"Preprint policy at Cambridge University Press","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Social sharing","pageBlurb":"Content sharing is a natural and vital part of research. It helps to disseminate and raise awareness about new findings and to stimulate discussion and further progress.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"social-sharing","introTitle":"Social Sharing","introText":"Many authors and readers share journal and book content with others. Some of the most common ways that content is shared are:","contentBlocks":[],"areas":["open-research-policies"]}}],"column2Heading":"Journals","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open access journals","pageBlurb":"We have a growing list of journals that publish Gold Open Access articles under Creative Commons licences.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"open-access-journals","introTitle":"Open access journals","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Gold Open Access journals","pageBlurb":"Many of our journals publish articles as Gold Open Access, under Creative commons (CC) licences, enabling readers to freely access and re-distribute their articles.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"gold-open-access-journals","introTitle":"Gold Open Access journals","introText":"Many of our journals publish articles as Gold Open Access, under Creative commons (CC) licences, enabling readers to freely access and re-distribute their articles. In some journals Gold OA is an option offered to authors, while in other journals, some or all article types are always published Gold OA.","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Transformative journals","pageBlurb":"The journals below have committed to transition to open research and meet the transformative journals requirements of Plan S. Download our 2021 and 2022 TJ progress reports to see each journal's progression to full OA.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"transformative-journals","introTitle":"Transformative journals","introText":"The journals below have committed to transition to open research and meet the transformative journals requirements of Plan S. Download our 2021 and 2022 TJ progress reports to see each journal's progression to full OA.","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Green Open Access policy for journals","pageBlurb":"Discover details of how our Green Open Access policy permits authors of journal articles published by Cambridge to share versions of their work online.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"green-open-access-policy-for-journals","introTitle":"Green Open Access policy for journals","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Transparent pricing policy for journals","pageBlurb":"We aim to price our journals fairly and transparently. In particular, our subscription prices should reflect the amount of subscription content in a journal.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"transparent-pricing-policy-for-journals","introTitle":"Transparent pricing policy for journals","introText":"We aim to price our journals fairly and transparently. In particular, our subscription prices should reflect the amount of subscription content in a journal.","contentBlocks":[],"areas":["open-research-policies"]}}],"column3Heading":"Books and Elements","column3StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open access books","pageBlurb":"Open access (OA) is emerging as a new model for book publishing. We support OA books in line with our mission to disseminate knowledge at the highest international levels of excellence.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"open-access-books","introTitle":"Open access books","introText":"Open access (OA) is emerging as a new model for book publishing. We support OA books in line with our commitment to exploring publishing models that are financially sustainable, scalable and in accordance with our mission to disseminate knowledge at the highest international levels of excellence.","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Gold open access books","pageBlurb":"We offer authors the option of publishing their work as open access. Discover more information about our gold open access books on our dedicated information page. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"gold-open-access-books","introTitle":"Gold open access books","introText":"We offer authors the option of publishing their work as open access to allow them to make their works freely available online without compromising any aspect of the publishing process. Typically, we offer this option only for monographs, which are books usually written by a single author to convey the results of their research and analysis in a particular field of study.","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Green Open Access policy for books","pageBlurb":"Discover details of how our Green Open Access policy permits authors of books published by Cambridge to share versions of their work online.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"green-open-access-policy-for-books","introTitle":"Green Open Access policy for books","contentBlocks":[],"areas":["open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Open access Elements","pageBlurb":"Discover information about how to publish an Element Gold Open Access, as well as details of our Green Open Access policy for Elements.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"open-access-elements","introTitle":"Open access Elements","introText":"We offer authors the option of publishing their Elements as open access to allow them to make their works freely available online without compromising any aspect of the publishing process.","contentBlocks":[],"areas":["open-research-policies"]}}],"slug":"open-research-policies"}},{"name":"navigationBarArea","value":{"title":"Open access publishing","hubPage":{"name":"staticPage","value":{"pageTitle":"What is open access?","pageBlurb":"Open access (OA) is an important way to make research findings freely available for anyone to access and view.","linkImage":{"title":"What is open access","description":"Dark yellow vertical stripes fill a shield shape, representative of the Cambridge University Press logo, on a light yellow background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7sEQ4K9gF64X6oCYQm5gHg/74f9d5bc0f37e8f4ffe68bd177a5118c/What_is_open_access.png","details":{"size":4791,"image":{"width":450,"height":270}},"fileName":"What is open access.png","contentType":"image/png"}},"altText":"Dark yellow vertical stripes fill a shield shape, representative of the Cambridge University Press logo, on a light yellow background.","slug":"open-access","introTitle":"What is open access?","introText":"Open access (OA) makes research findings freely available for anyone to access and view. Authors will benefit from the open availability of their research to others, leading to an increase in the visibility and usage of their work. Open access allows authors to comply with the Gold and Green OA requirements of major funders.","contentBlocks":[],"areas":["open-access-publishing"]}},"banner":{"title":"1","description":"a mountain range in mist. a red shield is superimposed on the right.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/tg5ltxgzVAAUpHiLcO8IM/8799f338869f552c0974163f621af29a/1.png","details":{"size":2466105,"image":{"width":2560,"height":1440}},"fileName":"1.png","contentType":"image/png"}},"column1Heading":"About open access","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open research","pageBlurb":"The open research movement seeks to maximise the impact and benefits of research by prioritising barrier-free access to research findings, data and methodologies.","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"open-research","introTitle":"Open research","introText":"The open research movement seeks to maximise the impact and benefits of research by prioritising barrier-free access to research findings, data and methodologies. Open research reflects a fundamental belief that the pursuit of knowledge benefits directly from collaboration, transparency, rapid dissemination and accessibility.","contentBlocks":[],"areas":["open-access-publishing","open-research-policies"]}},{"name":"staticPage","value":{"pageTitle":"Open Access Week","pageBlurb":"Open Access Week 2024 | Explore how we're working to build an open equitable future for all authors. We believe in the quality of your research rather than your ability to pay; so explore our routes to fund your open access publication.","linkImage":{"title":"Open Access Week","description":"Cambridge Blue vertical stripes fill a shield shape, representative of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/116LrrCNfUgNVzHbAtJ8Xj/b8f072a1ffc9713d156e6c96c91ef5d4/OA_Week.png","details":{"size":4883,"image":{"width":450,"height":270}},"fileName":"OA Week.png","contentType":"image/png"}},"altText":"Cambridge Blue vertical stripes fill a shield shape, representative of the Cambridge University Press logo.","slug":"open-access-week","introTitle":"Open Access Week","introText":"Welcome to Open Access Week 2024! We're working to build an open, equitable future for all authors around the world. Find out more about how we're supporting authors from low- and middle-income countries with our award-winning Cambridge Open Equity Initiative, our open access book-funding programme, Flip it Open, and more. ","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"What is open access?","pageBlurb":"Open access (OA) is an important way to make research findings freely available for anyone to access and view.","linkImage":{"title":"What is open access","description":"Dark yellow vertical stripes fill a shield shape, representative of the Cambridge University Press logo, on a light yellow background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7sEQ4K9gF64X6oCYQm5gHg/74f9d5bc0f37e8f4ffe68bd177a5118c/What_is_open_access.png","details":{"size":4791,"image":{"width":450,"height":270}},"fileName":"What is open access.png","contentType":"image/png"}},"altText":"Dark yellow vertical stripes fill a shield shape, representative of the Cambridge University Press logo, on a light yellow background.","slug":"open-access","introTitle":"What is open access?","introText":"Open access (OA) makes research findings freely available for anyone to access and view. Authors will benefit from the open availability of their research to others, leading to an increase in the visibility and usage of their work. Open access allows authors to comply with the Gold and Green OA requirements of major funders.","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Open access glossary","pageBlurb":"Open access has many unique terms, acronyms and additional information. Read our useful glossary to discover more.","linkImage":{"title":"Open access glossary","description":"Bright pink dot pattern fill a shield shape, representative of the Cambridge University Press logo, on a dark pink background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7cLxXpMkrUCEEHVu2Brt1b/69ca9bc044cd4698fce66e6ca8224fd7/OA_glossary.png","details":{"size":58518,"image":{"width":450,"height":270}},"fileName":"OA glossary.png","contentType":"image/png"}},"altText":"Bright pink dot pattern fill a shield shape, representative of the Cambridge University Press logo, on a dark pink background.","slug":"open-access-glossary","introTitle":"Open access glossary","introText":"Open access has many unique terms, acronyms and additional information. Read our useful glossary to discover more.","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Open access myths","pageBlurb":"Open access (OA) is an important way to make high-quality, peer reviewed content freely available for  anyone to access and view. ","linkImage":{"title":"Open access myths","description":"Bright orange dot pattern fills a shield shape, representative of the Cambridge University Press logo, on a dark orange background","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/6NIEmqNMzKKHvOHi9cLDUR/157e57a4457dce626d4c3469f15caff6/OA_Myths.png","details":{"size":50542,"image":{"width":450,"height":270}},"fileName":"OA Myths.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"open-access-myths","introTitle":"Open access myths","introText":"Open access is an important way to make high-quality, peer reviewed content freely available for anyone to access and view. ","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Hybrid Open Access FAQs","pageBlurb":"Discover move about what hybrid open access is and what's possible in our frequently asked questions","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"hybrid-open-access-faqs","introTitle":"Hybrid open access - Frequently asked questions","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"url","value":{"title":"Eligibility checker","url":"/core/eligibility-checker","ariaLabel":"Eligibility checker"}}],"column2Heading":"Open access resources","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open access resources","pageBlurb":"Find Open Access resources for authors, including; information on creative commons licences, funder mandates and policies and much more. ","linkImage":{"title":"Open Access - Resource for authors","description":"Open Access - Resource for authors image","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/nZUZJsiSdEQ92gahE9HVA/5ed0042cab7f26c29ce64460ccbbb273/Open-Access-button-v2.png","details":{"size":10991,"image":{"width":500,"height":300}},"fileName":"Open-Access-button-v2.png","contentType":"image/png"}},"altText":"A bright orange background, with light orange stripes has superimposed text in a white font that reads Open Access, resources for authors ","slug":"open-access-resources","introTitle":"Open access resources","introText":"Open access (OA) has become an important way to make research findings freely available for anyone to access and view. Open access serves authors and the wider community by publishing high-quality, peer-reviewed OA content. We support and promote all forms of OA that are financially sustainable. Our open access resources page provides some essential information about the types of OA we offer.","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Benefits of open access","pageBlurb":"Discover the benefits of open access on our dedicated page which covers; Discoverability and dissemination, Educational and other re-use and Public access and engagement.","linkImage":{"title":"Benefits of open access","description":"Pale yellow spot pattern fills a shield shape, representative of the Cambridge University Press logo, on a dark yellow background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/RzyZtGBB0v0qAeZdQG2lA/b986a003757ddb9089555b2388542208/benefits_of_open_access.png","details":{"size":53285,"image":{"width":450,"height":270}},"fileName":"benefits of open access.png","contentType":"image/png"}},"altText":"Pale yellow spot pattern fills a shield shape, representative of the Cambridge University Press logo, on a dark yellow background.","slug":"benefits-of-open-access","introTitle":"Benefits of open access","introText":"Publishing open access helps to advance discovery by allowing anyone, anywhere in the world with internet access to find, access, and benefit from your research. Open access plays an important part in allowing us to fulfil our mission of furthering the advancement of learning, knowledge and research worldwide.","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Creative commons licences","pageBlurb":"Creative Commons (CC) licences play an important role in facilitating Gold Open Access publishing. They provide a legal framework for giving anyone the ability to freely view, download and re-use content.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"A light pink outline of a shield with vertical gaps running through it, on a dark mauve background.","slug":"creative-commons-licenses","introTitle":"Creative Commons (CC) licences","introText":"Creative Commons (CC) licences play an important role in facilitating Gold Open Access publishing. They provide a legal framework for giving users the ability to freely view, download and distribute content.","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Funder policies and mandates","pageBlurb":"Many funders around the world are introducing policies and mandates concerning public access to the research they fund. The policies vary and researchers should check the terms and conditions of their research grants.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"funder-policies-and-mandates","introTitle":"Funder policies and mandates","introText":"Many funders around the world are introducing policies and mandates concerning public access to the research they fund. The policies vary and researchers should check the terms and conditions of their research grants in order to be aware of the particular conditions they must abide by.  ","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Article type definitions","pageBlurb":"Discover which article types are covered in transformative agreements via our dedicated article type definitions page","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"article-type-definitions","introTitle":"Article type definitions","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Convert your article to Open Access","pageBlurb":"If you would like to publish your article to Open Access, we ask that you select a creative commons licence. The CC licence you choose will determine how readers can use your content. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"convert-your-article-to-open-access","introTitle":"Publishing OA under a Read and Publish agreement","contentBlocks":[],"areas":["open-access-publishing"]}},{"name":"staticPage","value":{"pageTitle":"Open access video resources","pageBlurb":"Comprise of Open Access (OA) Advantage: Webinar, benefits, myths, flipping journal to OA, publishing OA with Cambridge","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"Comprise of Open Access (OA) Advantage: Webinar, benefits, myths, flipping journal to OA, publishing OA with Cambridge","slug":"open-access-video-resources","introTitle":"Open Access Video Resources","contentBlocks":[],"areas":["open-access-publishing"]}}],"slug":"open-access-publishing"}},{"name":"navigationBarArea","value":{"title":"Open research initiatives","column1Heading":"Research transparency","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Transparency and openness","pageBlurb":"Transparency and openness are core values of academic research and are essential if new observations and discoveries are to fully contribute to advances in global knowledge. ","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top.","slug":"transparency-and-openness","introTitle":"Transparency and openness","introText":"Transparency and openness are core values of academic research and are essential if new observations and discoveries are to fully contribute to advances in global knowledge. Research articles serve their readers best when they provide sufficient information for new assertions and findings to be properly evaluated and built upon.","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"Open Practice Badges","pageBlurb":"Open Practice Badges are incentives for researchers to share data, materials, or to preregister their work. They are designed to be displayed on published articles to show that authors have engaged in these open practices.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"open-practice-badges","introTitle":"Open Practice Badges","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"OA organisations, initiatives & directories","pageBlurb":"Discover more information on the Open Access organisations, initiatives and directories we are a member of.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"An image of the night sky, showing stars and galaxies. The dark outline of a shield has been superimposed over the top. ","slug":"oa-organisations-initiatives-and-directories","introTitle":"OA organisations, initiatives and directories","introText":"Cambridge University Press is a member of a number of organisations, initiatives and directories which we have provided information on below;","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"Registered Reports","pageBlurb":"Registered Reports are a publishing format developed by the Center for Open Science to incentivise and reward good research practices and study design. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"registered-reports","introTitle":"Registered Reports","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"Annotation for Transparent Inquiry (ATI)","pageBlurb":"Annotation for Transparent Inquiry (ATI) is a new tool designed to facilitate transparency in qualitative and mixed-methods research. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"annotation-for-transparent-inquiry-ati","introTitle":"Annotation for Transparent Inquiry (ATI)","introText":"Annotation for Transparent Inquiry (ATI) is a new tool designed to facilitate transparency in qualitative and mixed-methods research. ","contentBlocks":[],"areas":["open-research-initiatives"]}}],"column2Heading":"Journal flips","column2StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Open access journal flips","pageBlurb":"We are transforming the vast majority of research publishing in our journals to open access by 2025. 41 journals have flipped to open access for 2024 as part of this transformation, and we are pleased to announce a further 79 flips for 2025.","linkImage":{"title":"Open access journal flips","description":"Orange background with a stylized representation of a shield composed of vertical stripes.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/12DeiUx4ALihi8EgNJ6WB1/e8f5094d9645f9d58250abe35e1550a5/Flips_page.png","details":{"size":6919,"image":{"width":450,"height":270}},"fileName":"Flips page.png","contentType":"image/png"}},"altText":"Orange background with a stylized representation of a shield composed of vertical stripes.","slug":"open-access-journal-flips","introTitle":"Open access journal flips","introText":"We are transforming the vast majority of research publishing in our journals to open access by 2025. 41 journals have flipped to open access for 2024 as part of this transformation, and we are pleased to announce a further 79 flips for 2025.","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"OA Journal Flip FAQs","pageBlurb":"Find answers to questions about open access journal flips for authors, readers, librarians, society members, editorial board members and publishing partners.","linkImage":{"title":"Flips FAQ page","description":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/4PiPfW4LdMHTslTjTN19Di/86fdfe644104405c03a13f76a7d5a94e/Flips_FAQ_page.png","details":{"size":5367,"image":{"width":450,"height":270}},"fileName":"Flips FAQ page.png","contentType":"image/png"}},"altText":"A graphic of a purple shield with a striped pattern, consisting of vertical lines in a slightly lighter shade of purple.","slug":"oa-journal-flip-faqs","introTitle":"OA Journal Flip FAQs","introText":"Find answers to questions about open access journal flips for authors, readers, librarians, society members, editorial board members and publishing partners.","contentBlocks":[],"areas":["open-research-initiatives"]}}],"column3Heading":"Flip it Open","column3StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Flip it Open","pageBlurb":"Flip it Open is our new programme which aims to fund the open access publication of titles through typical purchasing habits. Once titles meet a set amount of revenue, we have committed to make them freely available as open access books.","linkImage":{"title":"flip it open","description":"Image of a door, slightly ajar, with large pink and green balls bouncing through, against a grey background wall.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1Iumh0y51Li1h1czm7xT3s/ffb2baaf34ed6fd6ec5ecea8721a2e17/flip_it_open.JPG","details":{"size":9674,"image":{"width":354,"height":199}},"fileName":"flip it open.JPG","contentType":"image/jpeg"}},"altText":"Image of a door, slightly ajar, with large pink and green balls bouncing through, against a grey background wall.","slug":"flip-it-open","introTitle":"Flip it Open","introText":"Flip it Open is our new programme which aims to fund the open access publication of 100 titles through typical purchasing habits. Once titles meet a set amount of revenue, we have committed to make them freely available as open access books here on Cambridge Core and also as an affordable paperback. Just another way we're building an open future.","contentBlocks":[],"areas":["open-research-initiatives"]}},{"name":"staticPage","value":{"pageTitle":"Flip it Open FAQs","pageBlurb":"Flip it Open is our new programme which aims to fund the open access publication of titles through typical purchasing habits. Once titles meet a set amount of revenue, we have committed to make them freely available as open access books.","linkImage":{"title":"flip it open","description":"Image of a door, slightly ajar, with large pink and green balls bouncing through, against a grey background wall.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1Iumh0y51Li1h1czm7xT3s/ffb2baaf34ed6fd6ec5ecea8721a2e17/flip_it_open.JPG","details":{"size":9674,"image":{"width":354,"height":199}},"fileName":"flip it open.JPG","contentType":"image/jpeg"}},"altText":"Image of a door, slightly ajar, with large pink and green balls bouncing through, against a grey background wall.","slug":"flip-it-open-faqs","introTitle":"Flip it Open FAQs","introText":"Flip it Open is our new programme which aims to fund the open access publication of 100 titles through typical purchasing habits. Once titles meet a set amount of revenue, we have committed to make them freely available as open access books here on Cambridge Core and also as an affordable paperback. Just another way we're building an open future.","contentBlocks":[],"areas":["open-research-initiatives"]}}],"slug":"open-research-initiatives"}},{"name":"navigationBarArea","value":{"title":"Open access funding","hubPage":{"name":"staticPage","value":{"pageTitle":"Funding open access publication","pageBlurb":"We believe that journals must publish articles based on the quality of the work rather than an author's ability to pay.","linkImage":{"title":"Funding open access publication","description":"Dark orange dot pattern fills a shield shape, representative of the Cambridge University Press logo, on a bright orange background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7te0Mjpty44biAUabJ5RPF/d6e37cd51d396f6ec99817b89b00b235/Funding_OA.png","details":{"size":53374,"image":{"width":450,"height":270}},"fileName":"Funding OA.png","contentType":"image/png"}},"altText":"Dark orange dot pattern fills a shield shape, representative of the Cambridge University Press logo, on a bright orange background.","slug":"funding-open-access-publication","introTitle":"Funding open access publication","introText":"We believe that journals must publish articles based on the quality of the work rather than an author's ability to pay. The editorial process – including the decision of whether or not to accept an article for publication – should be independent of the author’s decision to publish Gold Open Access (OA), in cases where this is optional, as well as being independent of how the publication costs for a Gold OA article are funded.","contentBlocks":[],"areas":["open-access-funding"]}},"banner":{"title":"3","description":"An image of prairie land and dusky skies with a green shield superimposed to the left.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7lbnCGEEZDq5LJSmPTzfqB/ff8d397038089070b71384ed0ce45028/3.jpg","details":{"size":313671,"image":{"width":2560,"height":1440}},"fileName":"3.jpg","contentType":"image/jpeg"}},"column1Heading":"Open access funding","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Funding open access publication","pageBlurb":"We believe that journals must publish articles based on the quality of the work rather than an author's ability to pay.","linkImage":{"title":"Funding open access publication","description":"Dark orange dot pattern fills a shield shape, representative of the Cambridge University Press logo, on a bright orange background.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/7te0Mjpty44biAUabJ5RPF/d6e37cd51d396f6ec99817b89b00b235/Funding_OA.png","details":{"size":53374,"image":{"width":450,"height":270}},"fileName":"Funding OA.png","contentType":"image/png"}},"altText":"Dark orange dot pattern fills a shield shape, representative of the Cambridge University Press logo, on a bright orange background.","slug":"funding-open-access-publication","introTitle":"Funding open access publication","introText":"We believe that journals must publish articles based on the quality of the work rather than an author's ability to pay. The editorial process – including the decision of whether or not to accept an article for publication – should be independent of the author’s decision to publish Gold Open Access (OA), in cases where this is optional, as well as being independent of how the publication costs for a Gold OA article are funded.","contentBlocks":[],"areas":["open-access-funding"]}},{"name":"staticPage","value":{"pageTitle":"Cambridge Open Equity Initiative","pageBlurb":"The Cambridge Open Equity Initiative is a new pilot designed to support authors in low- and middle-income countries who wish to publish their research open access in our journals but do not have access to funding.","linkImage":{"title":"Cambridge Open Equity Initiative","description":"Blue vertical stripes fill a shield shape, representative of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/6JjuLI91OqL5yWfpDCBabw/57656fcab1c004e52e2ac5221bf45ae3/COEI.png","details":{"size":7741,"image":{"width":450,"height":270}},"fileName":"COEI.png","contentType":"image/png"}},"altText":"Blue vertical stripes fill a shield shape, representative of the Cambridge University Press logo.","slug":"cambridge-open-equity-initiative","introTitle":"Cambridge Open Equity Initiative","introText":"The Cambridge Open Equity Initiative is designed to support authors in low- and middle-income countries who wish to publish their research open access in our journals but do not have access to funding.","contentBlocks":[],"areas":["open-access-funding"]}},{"name":"staticPage","value":{"pageTitle":"Completing a RightsLink (open access) transaction","pageBlurb":"Copyright Clearance Center (CCC) act on behalf of Cambridge to process open access article transactions using their secure platform, RightsLink. ","linkImage":{"title":"frontlist","description":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1yzIb3CIxMfQ2CTz8eswKt/8b72cfb68d75445b52d567fbe3573eb7/frontlist.JPG","details":{"size":20050,"image":{"width":334,"height":341}},"fileName":"frontlist.JPG","contentType":"image/jpeg"}},"altText":"A prism reflects a spectrum of light over a white shield on a purple background. The shield represents the shape of the Cambridge University Press logo.","slug":"completing-a-rightslink-open-access-transaction","introTitle":"Completing a RightsLink (open access) transaction","introText":"Copyright Clearance Center (CCC) act on behalf of Cambridge to process open access article transactions using their secure platform, RightsLink. ","contentBlocks":[],"areas":["open-access-funding"]}}],"slug":"open-access-funding"}},{"name":"navigationBarArea","value":{"title":" Cambridge Open Engage","hubPage":{"name":"staticPage","value":{"pageTitle":"Cambridge Open Engage","pageBlurb":"Cambridge Open Engage is the early research platform from Cambridge University Press. The site offers researchers a new space to rapidly disseminate early and open research, including preprints, posters, presentations and conference papers.","linkImage":{"title":"COE Banner for Core","description":"COE Banner for Core","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/11sHvd1kZ8Vow8Yew8U8Cb/5f69b2462388856cfdb9984c750c6d3c/COE-banner-for-Core-1200x175.jpg","details":{"size":30597,"image":{"width":1600,"height":233}},"fileName":"COE-banner-for-Core-1200x175.jpg","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage banner with its logo on the left side of the text and with a blue-green background color.","slug":"cambridge-open-engage","introTitle":"Cambridge Open Engage","contentBlocks":[],"areas":["cambridge-open-engage"]}},"column1Heading":"  Cambridge Open Engage","column1StaticPagesOrUrls":[{"name":"staticPage","value":{"pageTitle":"Cambridge Open Engage","pageBlurb":"Cambridge Open Engage is the early research platform from Cambridge University Press. The site offers researchers a new space to rapidly disseminate early and open research, including preprints, posters, presentations and conference papers.","linkImage":{"title":"COE Banner for Core","description":"COE Banner for Core","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/11sHvd1kZ8Vow8Yew8U8Cb/5f69b2462388856cfdb9984c750c6d3c/COE-banner-for-Core-1200x175.jpg","details":{"size":30597,"image":{"width":1600,"height":233}},"fileName":"COE-banner-for-Core-1200x175.jpg","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage banner with its logo on the left side of the text and with a blue-green background color.","slug":"cambridge-open-engage","introTitle":"Cambridge Open Engage","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"Partner With Us","pageBlurb":"We work with partners - such as learned societies, funders and departments or research centres within research institutions - in a range of ways.","linkImage":{"title":"Partner With Us","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/54Be88OUjKkNI6xbxmes8I/1196f9840ef75b6e7cf6aec599616505/partner-with-us.jpg","details":{"size":50015,"image":{"width":1250,"height":300}},"fileName":"partner-with-us.jpg","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage Partner with Us banner with a yellow background for the Cambridge Core website.","slug":"engage-partner-with-us","introTitle":"Partner With Us","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"Branded Hubs","pageBlurb":"Work with us to build your own branded space within Cambridge Open Engage. We offer partners such as learned societies, departments, and many more.","linkImage":{"title":"Branded Hubs","description":"Cambridge Open Engage logo with Branded Hubs banner with yellow background on Cambridge Core website.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1HYQfWBUMmFh5cxXT4ASk3/dd42f22e9034baad9e25666308f03055/Branded_hubs.JPG","details":{"size":13529,"image":{"width":318,"height":192}},"fileName":"Branded hubs.JPG","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage logo with Branded Hubs banner with yellow background on Cambridge Core website.","slug":"engage-branded-hubs","introTitle":"Branded Hubs","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"Event Workspaces","pageBlurb":"We offer branded event spaces where you can share outputs from events, offer commenting features allowing attendees to ask questions, and much more.","linkImage":{"title":"Event Workspaces","description":"Cambridge Open Engage with logo Event Workspaces banner with a yellow background on Cambridge Core website.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/1JNYmsm8HkoalS94741ud2/0299af5939154363dff989acb51ae3b3/Event_workspaces.JPG","details":{"size":13881,"image":{"width":320,"height":192}},"fileName":"Event workspaces.JPG","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage with logo Event Workspaces banner with a yellow background on Cambridge Core website.","slug":"engage-event-workspaces","introTitle":"Event Workspaces","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"Partner Resources","pageBlurb":"Spread the word to your researchers by sending them to Cambridge Open Engage, or contact us if you'd like to discuss tailored messaging for your community.","linkImage":{"title":"Partner Resources","description":"Cambridge Open Engage with logo Partner Resources banner with a yellow background on Cambridge Core website.","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/2NdKUR1NBikcoqYsjp1ePQ/0838ba4b5725badec595663d5ab08147/Partner_resources.JPG","details":{"size":13317,"image":{"width":316,"height":190}},"fileName":"Partner resources.JPG","contentType":"image/jpeg"}},"altText":"Cambridge Open Engage with logo Partner Resources banner with a yellow background on Cambridge Core website.","slug":"engage-partner-resources","introTitle":"Partner Resources","introText":"A selection of resources to aid partners with informing their members, staff, authors and editors about the benefits and functionality of Cambridge Open Engage.","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"APSA Preprints","pageBlurb":"APSA Preprints is a free-to-access pre-publication platform dedicated to early research outputs in political science, and developed through the collaboration of the American Political Science Association and Cambridge University Press.","linkImage":{"title":"APSA Preprints Banner","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/37QmpvOaJxxp9ksVaulObU/6ea66e49ff4f3d920eb5c43db27c02ce/APSA-Preprints.jpg","details":{"size":40928,"image":{"width":2000,"height":440}},"fileName":"APSA-Preprints.jpg","contentType":"image/jpeg"}},"altText":"APSA Preprints American Political Science Association logo banner with blue and green text color at the right and a cut circle at the left.","slug":"engage-apsa-preprints","introTitle":"APSA Preprints","contentBlocks":[],"areas":["cambridge-open-engage"]}},{"name":"staticPage","value":{"pageTitle":"APSA Preprints FAQs","pageBlurb":"If you have any questions about APSA Preprints that are not addressed by the FAQs or User Guide, you can contact preprints@apsanet.org.","linkImage":{"title":"APSA Preprints Banner","description":"","file":{"url":"//images.ctfassets.net/ulsp6w1o06p0/37QmpvOaJxxp9ksVaulObU/6ea66e49ff4f3d920eb5c43db27c02ce/APSA-Preprints.jpg","details":{"size":40928,"image":{"width":2000,"height":440}},"fileName":"APSA-Preprints.jpg","contentType":"image/jpeg"}},"altText":"APSA Preprints American Political Science Association logo banner with blue and green text color at the right and a cut circle at the left.","slug":"engage-apsa-preprints-faqs","introTitle":"APSA Preprints FAQs","contentBlocks":[],"areas":["cambridge-open-engage"]}}],"slug":"cambridge-open-engage"}}]}}],"youtube":{"name":"url","value":{"title":"Youtube","url":"https://www.youtube.com/playlist?list=PLTK8KRW19hUVucVRHbIx73oLKUro8HXt0","ariaLabel":"Visit Cambridge University Press Youtube account. Opens in a new tab."}},"xTwitter":{"name":"url","value":{"title":"xTwitter","url":"https://twitter.com/CambridgeCore","ariaLabel":"Visit Cambridge University Press X account. Opens in a new tab."}},"facebook":{"name":"url","value":{"title":"Facebook","url":"https://www.facebook.com/CambridgeCore","ariaLabel":"Visit Cambridge University Press Facebook account. Opens in a new tab."}},"instagram":{"name":"url","value":{"title":"Instagram","url":"https://www.instagram.com/cambridgeuniversitypress/","ariaLabel":"Visit Cambridge University Press Instagram account. Opens in a new tab."}},"linkedin":{"name":"url","value":{"title":"Linkedin","url":"https://www.linkedin.com/showcase/11096649","ariaLabel":"Visit Cambridge University Press Linkedin account. Opens in a new tab."}},"accessibility":{"name":"url","value":{"title":"Accessibility","url":"/core/accessibility","ariaLabel":"Cambridge Core accessibility page"}},"contactAndHelp":{"name":"url","value":{"title":"Contact & Help","url":"/core/help/FAQs","ariaLabel":"Cambridge Core contact & help page"}},"legalNotices":{"name":"url","value":{"title":"Legal notices","url":"/core/legal-notices/terms","ariaLabel":"Cambridge Core legal notices page"}}}},"locations":[{"id":"4cceb186-9e0f-4a5c-9cae-5fb7faf7c98e","alpha2Code":"AF","alpha3Code":"AFG","shortName":"Afghanistan","status":"officially-assigned","kkCountryId":"1","cupBranch":"D","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"7a8af489-758b-4a78-832d-dd9e754b5512","alpha2Code":"AX","alpha3Code":"ALA","shortName":"Aland Islands","status":"officially-assigned","kkCountryId":"240","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"6ec2aa7c-8144-4413-9a53-1e477c4c836a","alpha2Code":"AL","alpha3Code":"ALB","shortName":"Albania","status":"officially-assigned","kkCountryId":"2","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"522e3e12-6761-4d4d-bcaa-eb24f87aec05","alpha2Code":"DZ","alpha3Code":"DZA","shortName":"Algeria","status":"officially-assigned","kkCountryId":"3","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a01524a2-a472-44b6-9ebe-a61fd156973d","alpha2Code":"AS","alpha3Code":"ASM","shortName":"American Samoa","status":"officially-assigned","kkCountryId":"4","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"8317dd2d-d1f4-4009-ad3c-457ab2ba3a14","alpha2Code":"AD","alpha3Code":"AND","shortName":"Andorra","status":"officially-assigned","kkCountryId":"5","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":true},{"id":"1bf09008-eab9-4805-bdd4-2158a298f1f9","alpha2Code":"AO","alpha3Code":"AGO","shortName":"Angola","status":"officially-assigned","kkCountryId":"6","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"ea735233-adeb-4652-8f09-22420fc62809","alpha2Code":"AI","alpha3Code":"AIA","shortName":"Anguilla","status":"officially-assigned","kkCountryId":"7","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"20d19e9e-af37-4d13-a146-799796d8b6c0","alpha2Code":"AQ","alpha3Code":"ATA","shortName":"Antarctica","status":"officially-assigned","kkCountryId":"8","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"075b9cdb-8bcd-4afa-bea8-38fcb3a91d03","alpha2Code":"AG","alpha3Code":"ATG","shortName":"Antigua and Barbuda","status":"officially-assigned","kkCountryId":"9","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"5e891ca3-69c1-429c-9673-4b0ca191b8f4","alpha2Code":"AR","alpha3Code":"ARG","shortName":"Argentina","status":"officially-assigned","kkCountryId":"10","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"0ee9aa6f-ad1f-4760-95d4-4d233a39b6fe","alpha2Code":"AM","alpha3Code":"ARM","shortName":"Armenia","status":"officially-assigned","kkCountryId":"11","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"c01b6345-36af-4b5e-804b-81e4def241ff","alpha2Code":"AW","alpha3Code":"ABW","shortName":"Aruba","status":"officially-assigned","kkCountryId":"12","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"49d6f1dc-ca68-42f8-9f0f-3906decfe986","alpha2Code":"AU","alpha3Code":"AUS","shortName":"Australia","status":"officially-assigned","kkCountryId":"13","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"AUD","ecommLabel":"Total (incl. GST)","sensitiveTerritory":false},{"id":"f8c6231d-104b-45f9-b8bd-59d058c26617","alpha2Code":"AT","alpha3Code":"AUT","shortName":"Austria","status":"officially-assigned","kkCountryId":"14","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"e6898057-dce5-4267-9839-6f246dc56655","alpha2Code":"AZ","alpha3Code":"AZE","shortName":"Azerbaijan","status":"officially-assigned","kkCountryId":"15","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"5303e81f-50eb-474d-9de4-642d76633624","alpha2Code":"BS","alpha3Code":"BHS","shortName":"Bahamas","status":"officially-assigned","kkCountryId":"16","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"b39f1489-b2e0-4c8c-af10-6c1a1d1d51c8","alpha2Code":"BH","alpha3Code":"BHR","shortName":"Bahrain","status":"officially-assigned","kkCountryId":"17","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"813ffe7c-1bdc-43b7-a14c-ffe7d7c0da9d","alpha2Code":"BD","alpha3Code":"BGD","shortName":"Bangladesh","status":"officially-assigned","kkCountryId":"18","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"5e8eb00f-958a-440e-b98d-d3c0d7fc98c3","alpha2Code":"BB","alpha3Code":"BRB","shortName":"Barbados","status":"officially-assigned","kkCountryId":"19","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"3046521d-29d4-40f3-853e-3f401ead4807","alpha2Code":"BY","alpha3Code":"BLR","shortName":"Belarus","status":"officially-assigned","kkCountryId":"20","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f1e88a22-8adc-4a00-bbb2-a99ec1b6ec39","alpha2Code":"BE","alpha3Code":"BEL","shortName":"Belgium","status":"officially-assigned","kkCountryId":"21","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"1823ccfc-72e0-4af3-bba7-74ef903f39c5","alpha2Code":"BZ","alpha3Code":"BLZ","shortName":"Belize","status":"officially-assigned","kkCountryId":"22","cupBranch":"O","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"05e40998-408c-474d-9d60-bc9d29780ef6","alpha2Code":"BJ","alpha3Code":"BEN","shortName":"Benin","status":"officially-assigned","kkCountryId":"23","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"76d1cd2b-973d-4f95-ae34-60dae74043da","alpha2Code":"BM","alpha3Code":"BMU","shortName":"Bermuda","status":"officially-assigned","kkCountryId":"24","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"d9890163-5116-4417-aff7-73eee8cf767b","alpha2Code":"BT","alpha3Code":"BTN","shortName":"Bhutan","status":"officially-assigned","kkCountryId":"25","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"2c1cf90a-805b-461b-a568-d5123004356a","alpha2Code":"BO","alpha3Code":"BOL","shortName":"Bolivia","status":"officially-assigned","kkCountryId":"26","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"80dac76f-ee03-48f1-bbad-f2dd3a05b557","alpha2Code":"BA","alpha3Code":"BIH","shortName":"Bosnia and Herzegovina","status":"officially-assigned","kkCountryId":"241","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"83ded62f-d6fa-4269-8389-6bb6d3530551","alpha2Code":"BW","alpha3Code":"BWA","shortName":"Botswana","status":"officially-assigned","kkCountryId":"28","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"1f520814-af4f-4c4f-94ba-9d2f33b1c1f8","alpha2Code":"BV","alpha3Code":"BVT","shortName":"Bouvet Island","status":"officially-assigned","kkCountryId":"29","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"3f620225-dde7-4487-a6a1-bb59b383a939","alpha2Code":"BR","alpha3Code":"BRA","shortName":"Brazil","status":"officially-assigned","kkCountryId":"30","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"218df5a9-9145-46cd-927c-09d58d5f9dd5","alpha2Code":"IO","alpha3Code":"IOT","shortName":"British Indian Ocean Territory","status":"officially-assigned","kkCountryId":"31","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"874a057f-d7b0-413d-a102-b900fa63d3c7","alpha2Code":"BN","alpha3Code":"BRN","shortName":"Brunei Darussalam","status":"officially-assigned","kkCountryId":"32","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"f8fcfe8b-de4f-4249-82a9-b25c53f752a7","alpha2Code":"BG","alpha3Code":"BGR","shortName":"Bulgaria","status":"officially-assigned","kkCountryId":"33","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"825c267c-010c-4686-9e3a-8d9a102481d9","alpha2Code":"BF","alpha3Code":"BFA","shortName":"Burkina Faso","status":"officially-assigned","kkCountryId":"34","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"c64375a9-61b0-44d2-94de-43fc25389cfc","alpha2Code":"BI","alpha3Code":"BDI","shortName":"Burundi","status":"officially-assigned","kkCountryId":"35","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f571cf28-91cb-4c0d-8a4b-debd3047ab42","alpha2Code":"KH","alpha3Code":"KHM","shortName":"Cambodia","status":"officially-assigned","kkCountryId":"36","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"5ee10546-d2a4-40c8-b221-02baac2aceb7","alpha2Code":"CM","alpha3Code":"CMR","shortName":"Cameroon","status":"officially-assigned","kkCountryId":"37","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"df5651a0-e129-4f1b-b6ad-2f2dc1a61f70","alpha2Code":"CA","alpha3Code":"CAN","shortName":"Canada","status":"officially-assigned","kkCountryId":"38","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"fb011ad1-74ca-4235-a7ba-5003ba82e3de","alpha2Code":"CV","alpha3Code":"CPV","shortName":"Cape Verde","status":"officially-assigned","kkCountryId":"39","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f38f6d00-c96d-4362-b92f-8eb37b011f40","alpha2Code":"KY","alpha3Code":"CYM","shortName":"Cayman Islands","status":"officially-assigned","kkCountryId":"40","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"c0add3c3-c40b-4cda-b80f-acf3b8e0224e","alpha2Code":"CF","alpha3Code":"CAF","shortName":"Central African Republic","status":"officially-assigned","kkCountryId":"41","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"579966ba-d860-4886-94d0-e67d02f84a3e","alpha2Code":"TD","alpha3Code":"TCD","shortName":"Chad","status":"officially-assigned","kkCountryId":"42","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"b2179410-845b-4891-8e62-968eb7eb1b9e","alpha2Code":"IM","alpha3Code":"IMN","shortName":"Channel Islands, Isle of Man","status":"officially-assigned","kkCountryId":"243","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"621336c2-1da9-432a-9c7b-d3d5839ed6ab","alpha2Code":"CL","alpha3Code":"CHL","shortName":"Chile","status":"officially-assigned","kkCountryId":"43","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"296c1d80-767f-4036-912b-5f73422f1a22","alpha2Code":"CN","alpha3Code":"CHN","shortName":"China","status":"officially-assigned","kkCountryId":"44","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"eb60ae74-39b7-4400-bf55-0d65502a6c73","alpha2Code":"CX","alpha3Code":"CXR","shortName":"Christmas Island","status":"officially-assigned","kkCountryId":"45","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"048a93ca-514c-42cc-b460-cf620e540eb0","alpha2Code":"CC","alpha3Code":"CCK","shortName":"Cocos (Keeling) Islands","status":"officially-assigned","kkCountryId":"46","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"93f3e798-c2b9-4240-be44-9e1b60e837cc","alpha2Code":"CO","alpha3Code":"COL","shortName":"Colombia","status":"officially-assigned","kkCountryId":"47","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9b9db9aa-5411-4d1d-b663-f090006a0f07","alpha2Code":"KM","alpha3Code":"COM","shortName":"Comoros","status":"officially-assigned","kkCountryId":"48","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"893b2388-9613-4a21-b513-7cb3af28ca02","alpha2Code":"CG","alpha3Code":"COG","shortName":"Congo","status":"officially-assigned","kkCountryId":"49","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"1f635c2a-859c-45e7-9c82-6b985d327344","alpha2Code":"CD","alpha3Code":"COD","shortName":"Congo, The Democratic Republic of the","status":"officially-assigned","kkCountryId":"242","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e1b80aee-0209-4a3f-bbd1-eb7eaf5b8d4a","alpha2Code":"CK","alpha3Code":"COK","shortName":"Cook Islands","status":"officially-assigned","kkCountryId":"50","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"801c58d4-4ad9-42a6-867d-71e9aa0fa6fd","alpha2Code":"CR","alpha3Code":"CRI","shortName":"Costa Rica","status":"officially-assigned","kkCountryId":"51","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"7b6c730a-0a5a-4404-b790-98d5795ef876","alpha2Code":"CI","alpha3Code":"CIV","shortName":"Cote D'Ivoire","status":"officially-assigned","kkCountryId":"52","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e223963e-bacb-4397-b9d4-a2b6089ed2df","alpha2Code":"HR","alpha3Code":"HRV","shortName":"Croatia","status":"officially-assigned","kkCountryId":"53","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"2b7a9450-97e2-482d-b493-dec2c8d6271a","alpha2Code":"CU","alpha3Code":"CUB","shortName":"Cuba","status":"officially-assigned","kkCountryId":"54","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"9c1673be-ad0d-4ace-beb1-bc90aa7aaa81","alpha2Code":"CY","alpha3Code":"CYP","shortName":"Cyprus","status":"officially-assigned","kkCountryId":"55","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"a882aa2a-0c36-463c-8bcc-f7a39b4afc30","alpha2Code":"CZ","alpha3Code":"CZE","shortName":"Czech Republic","status":"officially-assigned","kkCountryId":"56","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"2c65e5f6-06d7-47f5-96b1-a4de391bd665","alpha2Code":"DK","alpha3Code":"DNK","shortName":"Denmark","status":"officially-assigned","kkCountryId":"57","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"27fc0d3d-d582-408c-aba4-405b9d47561f","alpha2Code":"DJ","alpha3Code":"DJI","shortName":"Djibouti","status":"officially-assigned","kkCountryId":"58","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0ca16a87-f6e1-4d66-b97a-a2b0d0c47a12","alpha2Code":"DM","alpha3Code":"DMA","shortName":"Dominica","status":"officially-assigned","kkCountryId":"59","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e81463fc-6dca-4065-b460-3788de92c400","alpha2Code":"DO","alpha3Code":"DOM","shortName":"Dominican Republic","status":"officially-assigned","kkCountryId":"60","cupBranch":"Q","cupRegion":"Q","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e36b7e41-87f5-4cf2-b989-e98cf2302c15","alpha2Code":"TP","alpha3Code":"TMP","shortName":"East Timor","status":"officially-assigned","kkCountryId":"61","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0a3f7221-8e0b-4544-87aa-39d757fea3b8","alpha2Code":"EC","alpha3Code":"ECU","shortName":"Ecuador","status":"officially-assigned","kkCountryId":"62","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"6f4e6491-3e03-4c7f-9150-7bfa50ee6432","alpha2Code":"EG","alpha3Code":"EGY","shortName":"Egypt","status":"officially-assigned","kkCountryId":"63","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"43351744-5158-4569-a9a8-946c9ac2245a","alpha2Code":"SV","alpha3Code":"SLV","shortName":"El Salvador","status":"officially-assigned","kkCountryId":"64","cupBranch":"O","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"7000fd0d-4dc6-4d1a-90fe-e5353a407af5","alpha2Code":"GQ","alpha3Code":"GNQ","shortName":"Equatorial Guinea","status":"officially-assigned","kkCountryId":"65","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"20a43d20-1135-4a18-8c6a-6bcd6f9793b1","alpha2Code":"ER","alpha3Code":"ERI","shortName":"Eritrea","status":"officially-assigned","kkCountryId":"66","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"be3990d4-098b-4294-ae25-5b5b5ad4079c","alpha2Code":"EE","alpha3Code":"EST","shortName":"Estonia","status":"officially-assigned","kkCountryId":"67","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"57314c79-55a8-4792-8365-6a2ba72a104a","alpha2Code":"ET","alpha3Code":"ETH","shortName":"Ethiopia","status":"officially-assigned","kkCountryId":"68","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"fa4b8ced-ff82-4ca9-9452-caad93f772fb","alpha2Code":"FK","alpha3Code":"FLK","shortName":"Falkland Islands (Malvinas)","status":"officially-assigned","kkCountryId":"69","cupBranch":"N","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"45c3a918-4f45-4640-b62e-5a41cb3a7f90","alpha2Code":"FO","alpha3Code":"FRO","shortName":"Faroe Islands","status":"officially-assigned","kkCountryId":"70","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a2f4e53e-19ba-4795-83c6-616f537e57b3","alpha2Code":"FJ","alpha3Code":"FJI","shortName":"Fiji","status":"officially-assigned","kkCountryId":"71","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"865d0154-f45c-44f1-93f0-f2ea86ea51fc","alpha2Code":"FI","alpha3Code":"FIN","shortName":"Finland","status":"officially-assigned","kkCountryId":"72","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"05dd7d5d-8a87-43e0-85c7-8c759916e772","alpha2Code":"FR","alpha3Code":"FRA","shortName":"France","status":"officially-assigned","kkCountryId":"73","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"03497ee5-48ee-4cdc-b775-0ade025586d4","alpha2Code":"GF","alpha3Code":"GUF","shortName":"French Guiana","status":"officially-assigned","kkCountryId":"75","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"791e2817-e292-4258-beb9-2a6c94bb5d69","alpha2Code":"PF","alpha3Code":"PYF","shortName":"French Polynesia","status":"officially-assigned","kkCountryId":"76","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"bb9290d9-fb8e-4529-baf9-b27e5528980d","alpha2Code":"TF","alpha3Code":"ATF","shortName":"French Southern Territories","status":"officially-assigned","kkCountryId":"77","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"afaa83a3-f2db-4be0-bdca-5453f35f1ae7","alpha2Code":"GA","alpha3Code":"GAB","shortName":"Gabon","status":"officially-assigned","kkCountryId":"78","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"68325bad-f70d-4ffd-b67c-5de5ebdfecbd","alpha2Code":"GM","alpha3Code":"GMB","shortName":"Gambia","status":"officially-assigned","kkCountryId":"79","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a9a68e5f-bb52-481d-bbe6-b652ac54f20d","alpha2Code":"GE","alpha3Code":"GEO","shortName":"Georgia","status":"officially-assigned","kkCountryId":"80","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"94230303-e27a-4c7e-83c0-59f3b65cb72b","alpha2Code":"DE","alpha3Code":"DEU","shortName":"Germany","status":"officially-assigned","kkCountryId":"81","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"40151893-12fb-4cc2-8564-33d43fcdb38b","alpha2Code":"GH","alpha3Code":"GHA","shortName":"Ghana","status":"officially-assigned","kkCountryId":"82","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"fcb90596-ccc1-4c0d-9c21-12786102ca92","alpha2Code":"GI","alpha3Code":"GIB","shortName":"Gibraltar","status":"officially-assigned","kkCountryId":"83","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"51d2507a-91a0-43cd-824b-a7cbd20bb1f6","alpha2Code":"GR","alpha3Code":"GRC","shortName":"Greece","status":"officially-assigned","kkCountryId":"84","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"30514745-a50b-448e-90d9-c3bf603f9218","alpha2Code":"GL","alpha3Code":"GRL","shortName":"Greenland","status":"officially-assigned","kkCountryId":"85","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"80e039ef-ccd8-4d9c-9651-01a75dbd2b18","alpha2Code":"GD","alpha3Code":"GRD","shortName":"Grenada","status":"officially-assigned","kkCountryId":"86","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"cb2cecc3-84ae-4a09-b0c5-1e2e94274264","alpha2Code":"GP","alpha3Code":"GLP","shortName":"Guadeloupe","status":"officially-assigned","kkCountryId":"87","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"187928f6-cd34-4669-b48a-077f7562c651","alpha2Code":"GU","alpha3Code":"GUM","shortName":"Guam","status":"officially-assigned","kkCountryId":"88","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"3cf99d09-1a9d-4e56-b0ff-a3f886a242dd","alpha2Code":"GT","alpha3Code":"GTM","shortName":"Guatemala","status":"officially-assigned","kkCountryId":"89","cupBranch":"O","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"0c0bd020-ee8f-458b-84ba-c4969b8f3d9e","alpha2Code":"GG","alpha3Code":"GGY","shortName":"Guernsey","status":"officially-assigned","kkCountryId":"256","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"05f3afe1-7c4f-4ca2-962d-0db4aa6b410c","alpha2Code":"GN","alpha3Code":"GIN","shortName":"Guinea","status":"officially-assigned","kkCountryId":"90","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"2f917b45-0301-4eca-bf2c-3644498bb848","alpha2Code":"GW","alpha3Code":"GNB","shortName":"Guinea-bissau","status":"officially-assigned","kkCountryId":"91","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"dcaf4941-7baa-41b4-9b00-3464cd91eff9","alpha2Code":"GY","alpha3Code":"GUY","shortName":"Guyana","status":"officially-assigned","kkCountryId":"92","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"c33a6cf0-d4fc-443d-b9e6-d42f630d1af2","alpha2Code":"HT","alpha3Code":"HTI","shortName":"Haiti","status":"officially-assigned","kkCountryId":"93","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e6179487-b4ca-4d8f-92ab-a7864b84f85d","alpha2Code":"HM","alpha3Code":"HMD","shortName":"Heard and Mc Donald Islands","status":"officially-assigned","kkCountryId":"94","cupBranch":"NULL","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"d11f91c5-5187-4d08-b48d-9077a0768435","alpha2Code":"HN","alpha3Code":"HND","shortName":"Honduras","status":"officially-assigned","kkCountryId":"95","cupBranch":"O","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"b67c3d03-f527-4dbd-a07d-01a82bef1107","alpha2Code":"HK","alpha3Code":"HKG","shortName":"Hong Kong","status":"officially-assigned","kkCountryId":"96","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"47b97900-a473-4bf5-981b-9de200e6b34f","alpha2Code":"HU","alpha3Code":"HUN","shortName":"Hungary","status":"officially-assigned","kkCountryId":"97","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"614cb0b5-e364-44d9-8c58-ff8d88a58156","alpha2Code":"IS","alpha3Code":"ISL","shortName":"Iceland","status":"officially-assigned","kkCountryId":"98","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"bd34cc15-3c6e-40a6-a0d4-e5d564a28536","alpha2Code":"IN","alpha3Code":"IND","shortName":"India","status":"officially-assigned","kkCountryId":"99","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"87f30ea0-a22f-40b0-aeb6-6a35d7f32bf3","alpha2Code":"ID","alpha3Code":"IDN","shortName":"Indonesia","status":"officially-assigned","kkCountryId":"100","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"c01d7696-a9ab-4703-9e1a-78e5607d9352","alpha2Code":"IR","alpha3Code":"IRN","shortName":"Iran, Islamic Republic of","status":"officially-assigned","kkCountryId":"244","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0ceb019d-6fbd-4deb-9f86-024807ca8aa2","alpha2Code":"IQ","alpha3Code":"IRQ","shortName":"Iraq","status":"officially-assigned","kkCountryId":"102","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"84740583-b333-4f4a-8a42-3a211eaa7af3","alpha2Code":"IE","alpha3Code":"IRL","shortName":"Ireland","status":"officially-assigned","kkCountryId":"103","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"979019bc-315b-4619-9a23-9eb96fa30b42","alpha2Code":"IL","alpha3Code":"ISR","shortName":"Israel","status":"officially-assigned","kkCountryId":"104","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a0301abd-eeaf-4e34-b578-328b434dce05","alpha2Code":"IT","alpha3Code":"ITA","shortName":"Italy","status":"officially-assigned","kkCountryId":"105","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"a082a2fa-fece-4cda-978a-af61bb51f297","alpha2Code":"JM","alpha3Code":"JAM","shortName":"Jamaica","status":"officially-assigned","kkCountryId":"106","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e0d7d043-51c6-4e34-8609-0f26b56bbaa3","alpha2Code":"JP","alpha3Code":"JPN","shortName":"Japan","status":"officially-assigned","kkCountryId":"107","cupBranch":"F","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"cf6cbfe5-435d-4f16-b202-b75935b9bfd1","alpha2Code":"JE","alpha3Code":"JEY","shortName":"Jersey","status":"officially-assigned","kkCountryId":"254","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"301a18e0-a4ff-4454-b99a-1f7ddaf960fc","alpha2Code":"JO","alpha3Code":"JOR","shortName":"Jordan","status":"officially-assigned","kkCountryId":"108","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"a2be4056-1959-41fd-b83b-a1a6e6790668","alpha2Code":"KZ","alpha3Code":"KAZ","shortName":"Kazakhstan","status":"officially-assigned","kkCountryId":"109","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"8a09cfd7-9e32-461f-9814-51cf3d205afa","alpha2Code":"KE","alpha3Code":"KEN","shortName":"Kenya","status":"officially-assigned","kkCountryId":"110","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0e97bf1b-5058-4bcf-9e56-e3e59ec097bf","alpha2Code":"KI","alpha3Code":"KIR","shortName":"Kiribati","status":"officially-assigned","kkCountryId":"111","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"f78723d2-da0d-4208-b398-d64225f3db40","alpha2Code":"KP","alpha3Code":"PRK","shortName":"Korea, Democratic People's Republic of","status":"officially-assigned","kkCountryId":"112","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"77f81edb-1401-4d4a-a221-6e17780dcc64","alpha2Code":"KR","alpha3Code":"KOR","shortName":"Korea, Republic of","status":"officially-assigned","kkCountryId":"113","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e4f2006174dd3a2b0174f952ffbb141b","alpha2Code":"XK","alpha3Code":"UNK","shortName":"Kosovo","status":"officially-assigned","kkCountryId":"0","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"d309f426-ed06-491e-af88-ae18beb738d3","alpha2Code":"KW","alpha3Code":"KWT","shortName":"Kuwait","status":"officially-assigned","kkCountryId":"114","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"8be878e2-8c54-4b7c-b0b4-45f440cc39a4","alpha2Code":"KG","alpha3Code":"KGZ","shortName":"Kyrgyzstan","status":"officially-assigned","kkCountryId":"115","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"22b8d6c8-a08f-4ce2-b547-c075e7402d5d","alpha2Code":"LA","alpha3Code":"LAO","shortName":"Lao People's Democratic Republic","status":"officially-assigned","kkCountryId":"116","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"66842dc2-54f5-4545-8fe9-10825844cd0b","alpha2Code":"LV","alpha3Code":"LVA","shortName":"Latvia","status":"officially-assigned","kkCountryId":"117","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"f2344dfd-803e-438b-b83b-2d83411344f5","alpha2Code":"LB","alpha3Code":"LBN","shortName":"Lebanon","status":"officially-assigned","kkCountryId":"118","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"cf97eb3b-a38a-4d0a-a86d-3c62af7cc102","alpha2Code":"LS","alpha3Code":"LSO","shortName":"Lesotho","status":"officially-assigned","kkCountryId":"119","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0d696510-0141-46c3-a9dd-73032d1dc369","alpha2Code":"LR","alpha3Code":"LBR","shortName":"Liberia","status":"officially-assigned","kkCountryId":"120","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e8da5281-6f13-4500-844c-af6e50a1817d","alpha2Code":"LY","alpha3Code":"LBY","shortName":"Libyan Arab Jamahiriya","status":"officially-assigned","kkCountryId":"121","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9e30228c-05f0-465e-8227-9deb4392ec80","alpha2Code":"LI","alpha3Code":"LIE","shortName":"Liechtenstein","status":"officially-assigned","kkCountryId":"122","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"f18fcd58-f002-4a29-bddd-88ab368390e4","alpha2Code":"LT","alpha3Code":"LTU","shortName":"Lithuania","status":"officially-assigned","kkCountryId":"123","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"bf550d77-9986-49c0-bda6-f8df4a33731b","alpha2Code":"LU","alpha3Code":"LUX","shortName":"Luxembourg","status":"officially-assigned","kkCountryId":"124","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"6142574e-44d7-4df3-b80f-11e0a1ffa8e1","alpha2Code":"MO","alpha3Code":"MAC","shortName":"Macau","status":"officially-assigned","kkCountryId":"125","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"b61d957a-4d6a-46a1-9df6-eafbd496bcc4","alpha2Code":"MK","alpha3Code":"MKD","shortName":"Macedonia","status":"officially-assigned","kkCountryId":"247","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f9c8f20a-0224-4405-a2b4-2dddeb899216","alpha2Code":"MG","alpha3Code":"MDG","shortName":"Madagascar","status":"officially-assigned","kkCountryId":"127","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a1e24326-20ed-4e87-a4b5-852bcba55fc7","alpha2Code":"MW","alpha3Code":"MWI","shortName":"Malawi","status":"officially-assigned","kkCountryId":"128","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"55e049f1-34d1-4a72-84db-6b20eb50be32","alpha2Code":"MY","alpha3Code":"MYS","shortName":"Malaysia","status":"officially-assigned","kkCountryId":"129","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"b43dce9c-06f7-489e-b21b-1b581fd1b376","alpha2Code":"MV","alpha3Code":"MDV","shortName":"Maldives","status":"officially-assigned","kkCountryId":"130","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f2186aa4-a2e6-472e-b953-e2cff11e723a","alpha2Code":"ML","alpha3Code":"MLI","shortName":"Mali","status":"officially-assigned","kkCountryId":"131","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"7a0ab8e8-39b3-4db9-bf76-fc6788ab0791","alpha2Code":"MT","alpha3Code":"MLT","shortName":"Malta","status":"officially-assigned","kkCountryId":"132","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"851a02c7-636d-4906-9178-375d5dca7138","alpha2Code":"MH","alpha3Code":"MHL","shortName":"Marshall Islands","status":"officially-assigned","kkCountryId":"133","cupBranch":"A","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"33938e2e-84d6-47f9-af01-6544c2556a5f","alpha2Code":"MQ","alpha3Code":"MTQ","shortName":"Martinique","status":"officially-assigned","kkCountryId":"134","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":true},{"id":"ee4f5569-bdd1-407b-957d-d1242217c485","alpha2Code":"MR","alpha3Code":"MRT","shortName":"Mauritania","status":"officially-assigned","kkCountryId":"135","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"9eab61a4-b4e2-4a36-a057-f5db49aea119","alpha2Code":"MU","alpha3Code":"MUS","shortName":"Mauritius","status":"officially-assigned","kkCountryId":"136","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"68048470-cbf1-46b1-8f48-69d4c29c111e","alpha2Code":"YT","alpha3Code":"MYT","shortName":"Mayotte","status":"officially-assigned","kkCountryId":"137","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"2870e1d0-f8d8-44fc-8b14-f7edb8503b86","alpha2Code":"MX","alpha3Code":"MEX","shortName":"Mexico","status":"officially-assigned","kkCountryId":"138","cupBranch":"O","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"7c3a61f7-61e4-4675-af2d-0e42f46cf477","alpha2Code":"FM","alpha3Code":"FSM","shortName":"Micronesia, Federated States of","status":"officially-assigned","kkCountryId":"139","cupBranch":"A","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"d6389849-d545-4980-99a2-541aa9b9ee17","alpha2Code":"MD","alpha3Code":"MDA","shortName":"Moldova, Republic of","status":"officially-assigned","kkCountryId":"140","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"379a1832-9838-44d1-b577-184ca5f5daff","alpha2Code":"MC","alpha3Code":"MCO","shortName":"Monaco","status":"officially-assigned","kkCountryId":"141","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"cc8ff54a-a9e7-41f4-b3c6-f63599d081e1","alpha2Code":"MN","alpha3Code":"MNG","shortName":"Mongolia","status":"officially-assigned","kkCountryId":"142","cupBranch":"C","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"6d6467a5-7349-4779-b335-631dda710c21","alpha2Code":"ME","alpha3Code":"MNE","shortName":"Montenegro","status":"officially-assigned","kkCountryId":"246","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a53aa839-afc4-4570-9d12-3733f9afd851","alpha2Code":"MS","alpha3Code":"MSR","shortName":"Montserrat","status":"officially-assigned","kkCountryId":"143","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"6db8c37d-c5a0-43db-812f-539c8ff3dd8a","alpha2Code":"MA","alpha3Code":"MAR","shortName":"Morocco","status":"officially-assigned","kkCountryId":"144","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"730b89e1-8ee5-47cf-a292-28758e3a7f20","alpha2Code":"MZ","alpha3Code":"MOZ","shortName":"Mozambique","status":"officially-assigned","kkCountryId":"145","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"5ce552e1-eedc-4350-a4c6-f6a2898def41","alpha2Code":"MM","alpha3Code":"MMR","shortName":"Myanmar","status":"officially-assigned","kkCountryId":"146","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"63b3ee95-fe51-4901-a7ae-fc0bc2cd81df","alpha2Code":"NA","alpha3Code":"NAM","shortName":"Namibia","status":"officially-assigned","kkCountryId":"147","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"d3ae3784-58d2-48ee-a6f1-9749a339c902","alpha2Code":"NR","alpha3Code":"NRU","shortName":"Nauru","status":"officially-assigned","kkCountryId":"148","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"042203fb-c380-4c2b-8d90-f97239823d9f","alpha2Code":"NP","alpha3Code":"NPL","shortName":"Nepal","status":"officially-assigned","kkCountryId":"149","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"8623257e-9ce6-4fd7-aff5-1858ca8c1545","alpha2Code":"NL","alpha3Code":"NLD","shortName":"Netherlands","status":"officially-assigned","kkCountryId":"150","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"97a6b88a-5cea-4541-ad90-ea14531d1e89","alpha2Code":"AN","alpha3Code":"ANT","shortName":"Netherlands Antilles","status":"officially-assigned","kkCountryId":"151","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"8129284e-e369-4380-be57-644541694140","alpha2Code":"NC","alpha3Code":"NCL","shortName":"New Caledonia","status":"officially-assigned","kkCountryId":"152","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"fa967094-29de-42a2-99e4-2a8deb0eecc3","alpha2Code":"NZ","alpha3Code":"NZL","shortName":"New Zealand","status":"officially-assigned","kkCountryId":"153","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":"Total (incl. GST)","sensitiveTerritory":false},{"id":"20c4ce92-62d7-447a-98e7-48722dbc7f8c","alpha2Code":"NI","alpha3Code":"NIC","shortName":"Nicaragua","status":"officially-assigned","kkCountryId":"154","cupBranch":"O","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"39d2607b-d95e-48bc-8722-9af51474faac","alpha2Code":"NE","alpha3Code":"NER","shortName":"Niger","status":"officially-assigned","kkCountryId":"155","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0e6848ae-cbc9-4e95-8c06-193a6025a20a","alpha2Code":"NG","alpha3Code":"NGA","shortName":"Nigeria","status":"officially-assigned","kkCountryId":"156","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"825aee1c-2546-432c-a8ce-fe8533971658","alpha2Code":"NU","alpha3Code":"NIU","shortName":"Niue","status":"officially-assigned","kkCountryId":"157","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"798cfed5-ffd1-40d0-bd63-6ea5438b0511","alpha2Code":"NF","alpha3Code":"NFK","shortName":"Norfolk Island","status":"officially-assigned","kkCountryId":"158","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e0ea495c-ad0d-4de3-a171-896216b1406a","alpha2Code":"MP","alpha3Code":"MNP","shortName":"Northern Mariana Islands","status":"officially-assigned","kkCountryId":"159","cupBranch":"NULL","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"acd087bf-0c83-4c18-83a7-b8d08cdfd5ac","alpha2Code":"NO","alpha3Code":"NOR","shortName":"Norway","status":"officially-assigned","kkCountryId":"160","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"6cafeb12-f2c3-46ef-b9e2-3e5eb010f605","alpha2Code":"OM","alpha3Code":"OMN","shortName":"Oman","status":"officially-assigned","kkCountryId":"161","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"92af6c5d-aa6e-4f19-8260-7a9965860a59","alpha2Code":"PK","alpha3Code":"PAK","shortName":"Pakistan","status":"officially-assigned","kkCountryId":"162","cupBranch":"D","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"cfbd5050-98fe-4def-a217-98cece6f0237","alpha2Code":"PW","alpha3Code":"PLW","shortName":"Palau","status":"officially-assigned","kkCountryId":"163","cupBranch":"NULL","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"bb3310cf-b407-416e-b3d9-523e2ce1d794","alpha2Code":"PS","alpha3Code":"PSE","shortName":"Palestinian Territory, Occupied","status":"officially-assigned","kkCountryId":"248","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e2f744c2-e5d0-491e-aa88-1421e18615bc","alpha2Code":"PA","alpha3Code":"PAN","shortName":"Panama","status":"officially-assigned","kkCountryId":"164","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"2688a966-22e0-4d18-b5f0-c1e2e2ad65d0","alpha2Code":"PG","alpha3Code":"PNG","shortName":"Papua New Guinea","status":"officially-assigned","kkCountryId":"165","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9f7b77ce-3572-417d-bbbf-22d90f13f704","alpha2Code":"PY","alpha3Code":"PRY","shortName":"Paraguay","status":"officially-assigned","kkCountryId":"166","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"4da3ee97-6df0-470d-98cf-562df8c8d164","alpha2Code":"PE","alpha3Code":"PER","shortName":"Peru","status":"officially-assigned","kkCountryId":"167","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"0f106ec3-9de4-4f85-bb09-26d7830bbdf4","alpha2Code":"PH","alpha3Code":"PHL","shortName":"Philippines","status":"officially-assigned","kkCountryId":"168","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9c5b6ddf-210d-44ff-82b6-9ad51f915b83","alpha2Code":"PN","alpha3Code":"PCN","shortName":"Pitcairn","status":"officially-assigned","kkCountryId":"169","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"927b1ada-bf62-47c5-bbc0-df990fb2f818","alpha2Code":"PL","alpha3Code":"POL","shortName":"Poland","status":"officially-assigned","kkCountryId":"170","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"9b7febfc-d8ae-417c-9d43-e48a46dc4da8","alpha2Code":"PT","alpha3Code":"PRT","shortName":"Portugal","status":"officially-assigned","kkCountryId":"171","cupBranch":"I","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"7bdc01da-9217-4e89-9976-fa3c171419bd","alpha2Code":"PR","alpha3Code":"PRI","shortName":"Puerto Rico","status":"officially-assigned","kkCountryId":"172","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"988553d3-7531-425e-9de8-94e45fdd9d18","alpha2Code":"QA","alpha3Code":"QAT","shortName":"Qatar","status":"officially-assigned","kkCountryId":"173","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"3afbb3fb-e115-43d4-b08e-cf0924bd652e","alpha2Code":"RE","alpha3Code":"REU","shortName":"Reunion","status":"officially-assigned","kkCountryId":"174","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"a7590f7b-5da8-4887-aeee-f62ac89e7d73","alpha2Code":"RO","alpha3Code":"ROM","shortName":"Romania","status":"officially-assigned","kkCountryId":"175","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"927c85b7-545c-4c47-bf16-dece1a52bf76","alpha2Code":"RU","alpha3Code":"RUS","shortName":"Russian Federation","status":"officially-assigned","kkCountryId":"176","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"0142ee9b-ef89-4a7f-8e5d-1560f14a849d","alpha2Code":"RW","alpha3Code":"RWA","shortName":"Rwanda","status":"officially-assigned","kkCountryId":"177","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"26ae3d8f-411c-4617-9e9f-e0240920ccff","alpha2Code":"KN","alpha3Code":"KNA","shortName":"Saint Kitts and Nevis","status":"officially-assigned","kkCountryId":"178","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"30aacdef-10e4-43bb-bc98-4a6764cb1636","alpha2Code":"LC","alpha3Code":"LCA","shortName":"Saint Lucia","status":"officially-assigned","kkCountryId":"179","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"8a4e3f48-2458-4cd1-ade0-36e6a51d3508","alpha2Code":"VC","alpha3Code":"VCT","shortName":"Saint Vincent and the Grenadines","status":"officially-assigned","kkCountryId":"180","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"40904289-77be-48b6-94fc-962f1f4af37d","alpha2Code":"WS","alpha3Code":"WSM","shortName":"Samoa","status":"officially-assigned","kkCountryId":"181","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"697b639c-700f-4829-aafd-0aef39d0b2ae","alpha2Code":"SM","alpha3Code":"SMR","shortName":"San Marino","status":"officially-assigned","kkCountryId":"182","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"620b7524-57af-4fc6-a18d-84983311633b","alpha2Code":"ST","alpha3Code":"STP","shortName":"Sao Tome and Principe","status":"officially-assigned","kkCountryId":"183","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"831a8227-0d4c-4bef-991b-d81f06aafbf4","alpha2Code":"SA","alpha3Code":"SAU","shortName":"Saudi Arabia","status":"officially-assigned","kkCountryId":"184","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"0927ed95-6d83-450f-b038-f2960f2dad79","alpha2Code":"SN","alpha3Code":"SEN","shortName":"Senegal","status":"officially-assigned","kkCountryId":"185","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"d99ef40c-1a85-4077-881d-2f4c7c6f3046","alpha2Code":"RS","alpha3Code":"SRB","shortName":"Serbia","status":"officially-assigned","kkCountryId":"249","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"7bb7c2ce-5a84-4f0b-9ed4-d054ff932a0d","alpha2Code":"SC","alpha3Code":"SYC","shortName":"Seychelles","status":"officially-assigned","kkCountryId":"186","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"5ae5bac7-79d4-4073-9f1f-7e9961b85b52","alpha2Code":"SL","alpha3Code":"SLE","shortName":"Sierra Leone","status":"officially-assigned","kkCountryId":"187","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"bc064ea8-099f-4a8a-a386-48431dabd194","alpha2Code":"SG","alpha3Code":"SGP","shortName":"Singapore","status":"officially-assigned","kkCountryId":"188","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"84e588f5-00f9-4992-a722-8bd05a47a68a","alpha2Code":"SK","alpha3Code":"SVK","shortName":"Slovakia","status":"officially-assigned","kkCountryId":"250","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"ec5419a0-4f83-46e5-9f14-653580687402","alpha2Code":"SI","alpha3Code":"SVN","shortName":"Slovenia","status":"officially-assigned","kkCountryId":"190","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"e034000d-6fb7-4420-a23e-09af7c2ef1a5","alpha2Code":"SB","alpha3Code":"SLB","shortName":"Solomon Islands","status":"officially-assigned","kkCountryId":"191","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"b4e18b13-68f5-4680-b634-c03793a9287b","alpha2Code":"SO","alpha3Code":"SOM","shortName":"Somalia","status":"officially-assigned","kkCountryId":"192","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"11227fdf-051a-4520-89ef-315a5a39ec16","alpha2Code":"ZA","alpha3Code":"ZAF","shortName":"South Africa","status":"officially-assigned","kkCountryId":"193","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"2605c280-574a-4670-bb14-93f0ee29063f","alpha2Code":"GS","alpha3Code":"SGS","shortName":"South Georgia and the South Sandwich Islands","status":"officially-assigned","kkCountryId":"194","cupBranch":"N","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e4f2006174dd3a2b0174f94f00cc140b","alpha2Code":"SS","alpha3Code":"SSD","shortName":"South Sudan","status":"officially-assigned","kkCountryId":"0","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"27f58fe2-7c39-486f-ac6a-26da3b68225b","alpha2Code":"ES","alpha3Code":"ESP","shortName":"Spain","status":"officially-assigned","kkCountryId":"195","cupBranch":"I","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"9fe0fd0a-27d6-4723-8ceb-eaf40b45f064","alpha2Code":"LK","alpha3Code":"LKA","shortName":"Sri Lanka","status":"officially-assigned","kkCountryId":"196","cupBranch":"D","cupRegion":"D","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"8740a5ed-79e0-42a5-9769-6e6f035ff78b","alpha2Code":"SH","alpha3Code":"SHN","shortName":"St. Helena","status":"officially-assigned","kkCountryId":"197","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"16b75816-519b-43d4-81a4-10292ea39d1a","alpha2Code":"PM","alpha3Code":"SPM","shortName":"St. Pierre and Miquelon","status":"officially-assigned","kkCountryId":"198","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"5d27adfe-e461-4a2e-8703-5ec9ac6c1241","alpha2Code":"SD","alpha3Code":"SDN","shortName":"Sudan","status":"officially-assigned","kkCountryId":"199","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"6a7a0bf9-1b4d-4cc9-bf4f-fcef994b6716","alpha2Code":"SR","alpha3Code":"SUR","shortName":"Suriname","status":"officially-assigned","kkCountryId":"200","cupBranch":"Q","cupRegion":"Q","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"874d22e5-1e92-4125-87ab-3471711ee562","alpha2Code":"SJ","alpha3Code":"SJM","shortName":"Svalbard and Jan Mayen Islands","status":"officially-assigned","kkCountryId":"201","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"233630f3-d0d7-46df-836b-cef65edd0705","alpha2Code":"SZ","alpha3Code":"SWZ","shortName":"Swaziland","status":"officially-assigned","kkCountryId":"202","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"161e0c12-5539-472d-8681-47d12e0f7c87","alpha2Code":"SE","alpha3Code":"SWE","shortName":"Sweden","status":"officially-assigned","kkCountryId":"203","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"e418e58e-6977-42f0-90f9-1edeb74c9620","alpha2Code":"CH","alpha3Code":"CHE","shortName":"Switzerland","status":"officially-assigned","kkCountryId":"204","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"970c958c-d257-4243-996c-5cf54a892450","alpha2Code":"SY","alpha3Code":"SYR","shortName":"Syrian Arab Republic","status":"officially-assigned","kkCountryId":"205","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":1,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"bb572853-7dfb-45b9-8d72-5d65f614e2d1","alpha2Code":"TW","alpha3Code":"TWN","shortName":"Taiwan","status":"officially-assigned","kkCountryId":"206","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":true},{"id":"115100c9-86b0-481e-9960-b19253485425","alpha2Code":"TJ","alpha3Code":"TJK","shortName":"Tajikistan","status":"officially-assigned","kkCountryId":"207","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"43093a64-4dfb-49a0-ac35-a81d3ba8680d","alpha2Code":"TZ","alpha3Code":"TZA","shortName":"Tanzania, United Republic of","status":"officially-assigned","kkCountryId":"208","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"9de26d08-cbd0-46dd-95c3-d26c11b91ad9","alpha2Code":"TH","alpha3Code":"THA","shortName":"Thailand","status":"officially-assigned","kkCountryId":"209","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"e4f2006174dd3a2b0174f938e23413f1","alpha2Code":"TL","alpha3Code":"TLS","shortName":"Timor-Leste","status":"officially-assigned","kkCountryId":"0","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"396a394b-48bd-491d-8783-8e92c49cbf9c","alpha2Code":"TG","alpha3Code":"TGO","shortName":"Togo","status":"officially-assigned","kkCountryId":"210","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"c0ea7925-a0ec-4827-b13d-d7dc3666132a","alpha2Code":"TK","alpha3Code":"TKL","shortName":"Tokelau","status":"officially-assigned","kkCountryId":"211","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"7f008c68-9ed3-444d-9fb7-fdcd6def32e8","alpha2Code":"TO","alpha3Code":"TON","shortName":"Tonga","status":"officially-assigned","kkCountryId":"212","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"6cc3a480-5cc3-4aaa-b464-3a4638710353","alpha2Code":"TT","alpha3Code":"TTO","shortName":"Trinidad and Tobago","status":"officially-assigned","kkCountryId":"213","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"f8df523a-eb8b-4971-b5c0-d46bfc28d25e","alpha2Code":"TN","alpha3Code":"TUN","shortName":"Tunisia","status":"officially-assigned","kkCountryId":"214","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"c92bc644-fdae-4111-9393-78a5d845a49d","alpha2Code":"TR","alpha3Code":"TUR","shortName":"Türkiye","status":"officially-assigned","kkCountryId":"215","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"63efbcff-1e4b-4821-99c1-d67174df50db","alpha2Code":"TM","alpha3Code":"TKM","shortName":"Turkmenistan","status":"officially-assigned","kkCountryId":"216","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"560a128b-dabd-411b-ab88-2c3718176995","alpha2Code":"TC","alpha3Code":"TCA","shortName":"Turks and Caicos Islands","status":"officially-assigned","kkCountryId":"217","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"48917d67-0131-4a0d-ba34-34b7844e4f99","alpha2Code":"TV","alpha3Code":"TUV","shortName":"Tuvalu","status":"officially-assigned","kkCountryId":"218","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"63c2a03c-2ef3-427d-8fc4-1c5fdd81cb28","alpha2Code":"UG","alpha3Code":"UGA","shortName":"Uganda","status":"officially-assigned","kkCountryId":"219","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a25acd1f-1576-4eed-99f3-e690fa8fcdaf","alpha2Code":"UA","alpha3Code":"UKR","shortName":"Ukraine","status":"officially-assigned","kkCountryId":"220","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"5b7988cb-c044-4d74-9eda-3262b689088f","alpha2Code":"AE","alpha3Code":"ARE","shortName":"United Arab Emirates","status":"officially-assigned","kkCountryId":"221","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"ba985020-38b1-45d7-89bd-fc95a625e0c7","alpha2Code":"GB","alpha3Code":"GBR","shortName":"United Kingdom","status":"officially-assigned","kkCountryId":"222","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"a22b6cd6-8a7d-404a-bfc3-f28fe1cc9df1","alpha2Code":"US","alpha3Code":"USA","shortName":"United States","status":"officially-assigned","kkCountryId":"223","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"230a425e-b19e-47ae-b07f-e6418807972b","alpha2Code":"UM","alpha3Code":"UMI","shortName":"United States Minor Outlying Islands","status":"officially-assigned","kkCountryId":"224","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9379fd2c-ce6c-458c-b61d-8a554732ce5c","alpha2Code":"VI","alpha3Code":"VIR","shortName":"United States Virgin Islands","status":"officially-assigned","kkCountryId":"253","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"d282e488-3ecc-45ab-b72c-81dc0c885956","alpha2Code":"UY","alpha3Code":"URY","shortName":"Uruguay","status":"officially-assigned","kkCountryId":"225","cupBranch":"N","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"183122de-4fd8-4b1d-821d-ab89ddd09811","alpha2Code":"UZ","alpha3Code":"UZB","shortName":"Uzbekistan","status":"officially-assigned","kkCountryId":"226","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"9971d148-7a5b-454e-ac18-3c19822988b3","alpha2Code":"VU","alpha3Code":"VUT","shortName":"Vanuatu","status":"officially-assigned","kkCountryId":"227","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"6475d530-8e6d-4abd-a7f0-f4b382cb6db2","alpha2Code":"VA","alpha3Code":"VAT","shortName":"Vatican City","status":"officially-assigned","kkCountryId":"251","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"EUR","ecommLabel":null,"sensitiveTerritory":false},{"id":"9a5b0169-d1c8-4c10-8fa2-32b4ecac4987","alpha2Code":"VE","alpha3Code":"VEN","shortName":"Venezuela","status":"officially-assigned","kkCountryId":"229","cupBranch":"Q","cupRegion":"N","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"745aab4b-9973-41c0-917f-0638e51f3b57","alpha2Code":"VN","alpha3Code":"VNM","shortName":"Vietnam","status":"officially-assigned","kkCountryId":"252","cupBranch":"E","cupRegion":"E","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"9db772a9-22f5-43a9-b935-68ceb56a5a1c","alpha2Code":"VG","alpha3Code":"VGB","shortName":"Virgin Islands (British)","status":"officially-assigned","kkCountryId":"231","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"7bb4a17e-0a63-4141-9e19-b7851eb72937","alpha2Code":"WF","alpha3Code":"WLF","shortName":"Wallis and Futuna Islands","status":"officially-assigned","kkCountryId":"233","cupBranch":"A","cupRegion":"A","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"4a6f26bc-75ad-4d14-a9b8-689a4ec2c55d","alpha2Code":"EH","alpha3Code":"ESH","shortName":"Western Sahara","status":"officially-assigned","kkCountryId":"234","cupBranch":"NULL","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false},{"id":"9e2e344d-f7e3-4ae6-b10c-7f9acd4e7d16","alpha2Code":"YE","alpha3Code":"YEM","shortName":"Yemen","status":"officially-assigned","kkCountryId":"235","cupBranch":"C","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"USD","ecommLabel":null,"sensitiveTerritory":false},{"id":"d0a991ff-3d63-4e93-9f2f-09261a2a874c","alpha2Code":"ZM","alpha3Code":"ZMB","shortName":"Zambia","status":"officially-assigned","kkCountryId":"238","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":0,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":true},{"id":"1f6641ed-117b-4cc6-8880-09b4eb72ae4c","alpha2Code":"ZW","alpha3Code":"ZWE","shortName":"Zimbabwe","status":"officially-assigned","kkCountryId":"239","cupBranch":"R","cupRegion":"C","regionCodes":[],"embargoedCountryDigital":0,"embargoedCountryPhysical":1,"currency":"GBP","ecommLabel":null,"sensitiveTerritory":false}]}</script>
  </div>
  <platform-footer
    id='platform-footer-wc'
    platform='core'
    env='prod'
    class='platform-footer-wc'
    is-preview='false'
    style="display: none"
  ></platform-footer>
  
  <script>
    const platformFooter = $('#platform-footer');
    const platformFooterWc = $('#platform-footer-wc');
    platformFooterWc.prop('initialData', window.__PLATFORM_FOOTER_DATA__);
  
    platformFooterWc.on('initialized', function () {
      platformFooter.hide();
      platformFooterWc.show();
    });
  
    platformFooterWc.on('update-location', function (event) {
      $.post(AOP.baseUrl + '/services/country/override', {
        countryCode: event.originalEvent.detail[0].alpha2Code
      }, function () {
        window.location.reload();
        window.scrollTo(0, document.body.scrollHeight); // scroll the bottom of the page
      })
    })
  </script>
    <script src="/core/system/public/bower_components/foundation/js/foundation.js"></script>
    <script src="/core/cambridge-core/public/bower_components/jquery-lazy/jquery.lazy.min.js"></script>

    <script src="/core/cambridge-core/public/js/plugins/jquery.autocomplete.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/slick.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/jquery.qtip.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/foggy.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/jquery.cookie.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/jquery.textresizer.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/chosen.jquery.min.js"></script>
    <script src="/core/cambridge-core/public/js/plugins/jquery.sticky.js"></script>

    <script type="text/javascript">
      var AOP = AOP || {};
    </script>

    <!-- TODO CHECK -->
    <!-- system footer start -->
    <script type="text/javascript" language="javascript">
      window.AOP = window.AOP || {};
      window.AOP.pageId = '';
      window.AOP.baseUrl = '/core';
      window.AOP.env = 'prod';
      window.AOP.shouldUseCitationTool = true;
      
      
    </script>
    <script src="/core/system/public/js/foundationUtils.js" language="javascript" type="text/javascript"></script>
    <script src="/core/system/public/js/confirmModal.js" language="javascript" type="text/javascript"></script>
    <script src="/core/system/public/js/trimWhitespace.js" language="javascript" type="text/javascript"></script>
    <script src="/core/system/public/js/loadScript.js" language="javascript" type="text/javascript"></script>
    <script src="/core/system/public/js/isArray.js"></script>
    <script src="/core/system/public/js/parseQuerystring.js"></script>
    <script src="/core/system/public/js/createAlertBox.js"></script>
    <script src="/core/system/public/js/countryPicker.js"></script>
    
    <div id="confirmModalWrapper">
        <div id="confirm-modal" class="reveal-modal small" data-reveal role="dialog" aria-labelledby="confirmModalHeader">
            <div class="wrapper">
                <h1 class="title" id="confirmModalHeader"></h1>
                <div class="row">
                    <div class="large-12 columns">
                        <div class="panel callout message">
                        </div>
                    </div>
                </div>
    
                <div class="row margin-top">
                    <div class="small-12 large-6 columns"><a href="#" class="right small button radius transparent cancel">Cancel</a></div>
                    <div class="small-12 large-6 columns"><a href="#" class="left small radius button blue confirm">Confirm</a></div>
                </div>
            </div>
            <a href="#" class="close-reveal-modal"><span aria-hidden="true">×</span></a>
        </div>
    </div><!-- CMS Includes -->
    
    
      <script src="/aca/shared-elements/platform-header-footer.umd.js" language="javascript"></script>
      <script src="/core/cambridge-core/public/js/ecommerce/add-to-basket.js?v&#x3D;v7.403.1" language="javascript"></script>
    
    <script src="/core/system/public/js/validation.js" language="javascript" type="text/javascript"></script>
    <script src="/core/system/public/js/revealModal.js" language="javascript" type="text/javascript"></script>
    
    <!-- system footer finish -->
    
    <!-- Build:    unreadable -->
    
    

    <script type="text/javascript" language="javascript">
      window.AOP = window.AOP || {};
      window.AOP.webTrendsFpcdom = '';
      window.AOP.webtrendsSourceId = '';
      window.AOP.oracleInfinityAccountId = '';
      window.AOP.isInternalTraffic = false;
    </script>
    <script src="/core/cambridge-core/public/js/jquery-extend.js?v=v7.403.1"></script>
    <script src="/core/cambridge-core/public/js/additional.js?v=v7.403.1"></script>

    <script src="/core/cambridge-core/public/js/multilingual.js?v=v7.403.1"></script>
    <script src="/core/cambridge-core/public/js/analytics/cup-events.js?v=v7.403.1"></script>
    <script src="/core/cambridge-core/public/js/app.js?v=v7.403.1"></script>
    <script src="/core/cambridge-core/public/js/accessibility.js?v=v7.403.1"></script>
    <script src="/core/cambridge-core/public/js/analytics/listeners/counter.js?v=v7.403.1"></script>
      <script src="/core/cambridge-core/public/js/ga-events.js?v=v7.403.1"></script>

    <script type="text/javascript">
      $(document).ready(function () {
        if (typeof $(document).cupEvent === 'function') {
          $(document).cupEvent();
        }

        AOP.initCounterReporter({
          apiKey: 'qzD90fb0r257ZKVtGSFnG3UucyyMdgvu8u250Fa0',
          apiUrl: 'https://usage.prod.aop.cambridge.org/v1/events',
          applicationId: '8a94020952a738f80152b223c992000b',
          identities: [],
          authenticationMethods: [],
          sessionId: 'NIDEA_w_FWO3Id-E_cLEaxCDTqOiJhW7',
          eventContext: '/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30'
        })
      });
    </script>

<script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'91b2a66c8aae3063',t:'MTc0MTEwNDY5NC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
</html>
<div id="kindleModal" class="reveal-modal large" data-reveal>
  <div class="header">
    <h1 class="heading_07">Save article to Kindle</h1>
  </div>

  <div class="panel">
    <div class="row">
      <div class="large-12 columns margin-bottom">
          
            <p>To send this article to your Kindle, first ensure no-reply@cambridge.org is added to your Approved Personal Document E-mail List under your Personal Document Settings on the Manage Your Content and Devices page of your Amazon account. Then enter the ‘name’ part of your Kindle email address below. Find out more about sending to your Kindle.
              <a href="/core/help">Find out more about saving to your Kindle</a>.
            </p>
            <p>
              Note you can select to save to either the @free.kindle.com or @kindle.com variations. ‘@free.kindle.com’ emails are free but can only be saved to your device when it is connected to wi-fi. ‘@kindle.com’ emails can be delivered even when you are not connected to wi-fi, but note that service fees apply.
            </p>
            <p>
              Find out more about the <a href="https://www.amazon.com/gp/help/customer/display.html/ref=kinw_myk_wl_ln?ie=UTF8&nodeId=200767340#fees" target="_blank">Kindle Personal Document Service.</a>
            </p>
      </div>
    </div>

        <div class="row book-title">
          <div class="small-12 columns">
            <div class="heading_08"><div class="title">Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?</div></div>
          </div>
        </div>
        <div class="row book-details book-overview">
          <div class="small-12 columns">
            <ul>
                <li class="volume-issue"></li>
                    <li class="author">
                        <a href="/core/search?filters%5BauthorTerms%5D=Vasudevan Nedumpozhimana&eventCode=SE-AU" class="more-by-this-author ">Vasudevan Nedumpozhimana</a><a target="_blank" class="orcid-logo " href="https://orcid.org/0000-0001-5161-8925" aria-label="Open the ORCID record for Vasudevan Nedumpozhimana in new tab/window"></a> <sup data-affiliation-id="a1" class="">(a1)</sup><span class="separator "> and </span><a href="/core/search?filters%5BauthorTerms%5D=John D. Kelleher&eventCode=SE-AU" class="more-by-this-author ">John D. Kelleher</a> <sup data-affiliation-id="a2" class="">(a2)</sup>
                    </li>
                    <li class="meta-info">DOI: <a href="https://doi.org/10.1017/nlp.2024.43" target="_blank" class="url">https://doi.org/10.1017/nlp.2024.43</a></li>
            </ul>
          </div>
        </div>  </div>

  <div class="row wrapper no-padding-top">
    <div class="small-12 columns">
      <div class="row margin-top">
        <div class="large-12 columns">
          <form id="sendTokindleForm" action="/core/services/aop-cambridge-core/sendto" method="GET">

              <fieldset>
                <legend>
                    <label for="kindleEmailAddress">Your Kindle email address</label>
                </legend>
                <div class="row">
                  <div class="large-6 columns">
                        <input name="kindleEmailAddress" id="kindleEmailAddress" type="text" value="" required="required"/>
                    <small class="error kindleEmailAddress" style="display:none">Please provide your Kindle email.</small>
                  </div>
                  <div class="large-6 columns">
                    <label class="dInlineBlock"><input name="kindleEmailDomain" type="radio" checked="checked" value="free.kindle.com" />@free.kindle.com</label>
                    <label class="dInlineBlock"><input name="kindleEmailDomain" type="radio" value="kindle.com" />@kindle.com (<a href="https://www.amazon.com/gp/help/customer/display.html/ref=kinw_myk_wl_ln?ie=UTF8&nodeId=200767340#fees" target="_blank">service fees apply</a>)</label>
                  </div>
                </div>
              </fieldset>
            <div class="row">
              <div class="large-12 columns">
                <fieldset>
                  <legend>Available formats</legend>
                
                    <label>
                      <input type="checkbox" name="formats" class="formats" value="pdf" required /> PDF
                    </label>
                
                
                
                  <small class="error formats" style="display:none">Please select a format to save.</small>
                </fieldset>              </div>
            </div>

            <div class="row">
              <div class="large-12 columns">
                <label for="usagePolicykindle">
                  <input type="checkbox" name="usagePolicykindle" id="usagePolicykindle" value="Usage" required/>
                  By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services
                  <small class="error usagePolicy" style="display:none">Please confirm that you accept the terms of use.</small>
                </label>
              </div>
            </div>

            <input type="hidden" name="suppressTrackingEvent" value="false" />
            <input type="hidden" name="service" value="kindle" />
            <input type="hidden" name="documents" value="0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="finalReturn" value="/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="_csrf" value="otkuUNF9-3Nb3vQmWJfKACI-gz-gsr4cHgfc">

          </form>

          <!-- Submit button start -->
          <div class="row margin-top">
            <div class="small-6 large-6 columns">
              <button href="#" class="button radius transparent right closeModal">Cancel</button>
            </div>
            <div class="small-6 large-6 columns">
              <button type="submit" onClick="$('#sendTokindleForm').submit();" class="button blue radius left">
                Save
              </button>
            </div>
          </div>
          <!-- Submit button end  -->
        </div>
      </div>
    </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<script type="text/javascript">
  $('#sendTokindleForm').on('submit', validatekindleForm);

  var _formkindle = $('#sendTokindleForm');

  function resetValidationkindleForm() {
    var _form = _formkindle;
    _form.find('.error.formats').hide();
    _form.find('.error.usagePolicy').hide();
  }

  function validatekindleFormats(e) {
    var _form = _formkindle;
    var errors = false;
    var formats = _form.find('input[name="formats"]:checked');
    if (formats.length < 1) {
      _form.find('.error.formats').show();
      errors = true;
    } else {
      _form.find('.error.formats').hide();
    }
    return errors;
  }

  function validatekindleUsagePolicy(e) {
    var _form = _formkindle;
    var usagePolicy = _form.find('input[name="usagePolicykindle"]');
    var errors = false;
    if (!usagePolicy.is(':checked')) {
      _form.find('.error.usagePolicy').show();
      errors = true;
    } else {
      _form.find('.error.usagePolicy').hide();
    }
    return errors;
  }

  function validateKindleEmail(e) {
    var _form = _formkindle;
    var kindleEmailAddress = _form.find('input[name="kindleEmailAddress"]');
    var errors = false;

    if (!kindleEmailAddress.val().trim()) {
      _form.find('.error.kindleEmailAddress').show();
      kindleEmailAddress.addClass('error');
      errors = true;
    } else {
      _form.find('.error.kindleEmailAddress').hide();
      kindleEmailAddress.removeClass('error');
    }
    return errors;
  }


  function validatekindleForm(e) {
    var _form = _formkindle;
    var resultPolicy = validatekindleUsagePolicy();
    var resultFormats = validatekindleFormats();

    if (resultPolicy) {
      $('#usagePolicykindle', _form).one('change', validatekindleUsagePolicy);
    }
    if (resultFormats) {
      $('.formats', _form).one('change', validatekindleFormats);
    }

    var resultKindleEmail = validateKindleEmail(e);
    if (resultKindleEmail) {
      $('input[name="kindleEmailAddress"]', _form).one('change', validateKindleEmail);
    }

    return !resultPolicy && !resultFormats  && !resultKindleEmail;
  }


  $(document).ready(function () {
    // reset validation when the modal opens
    $(document).on('open.fndtn.reveal', '[data-reveal]', function () {
      resetValidationkindleForm();

      // automatically select if there's only one format
      if ($('#sendTokindleForm input[name="formats"]').length === 1) {
        $('#sendTokindleForm input[name="formats"]').prop('checked', true);
      }
    });
  });

</script><div id="dropboxModal" class="reveal-modal large" data-reveal>
  <div class="header">
    <h1 class="heading_07">Save article to Dropbox</h1>
  </div>

  <div class="panel">
    <div class="row">
      <div class="large-12 columns margin-bottom">
          <p>
              To save this article to your Dropbox account, please select one or more formats and confirm that you agree to abide by our usage policies. If this is the first time you used this feature, you will be asked to authorise Cambridge Core to connect with your Dropbox account.
              <a href="/core/help">Find out more about saving content to Dropbox</a>.
          </p>      </div>
    </div>

        <div class="row book-title">
          <div class="small-12 columns">
            <div class="heading_08"><div class="title">Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?</div></div>
          </div>
        </div>
        <div class="row book-details book-overview">
          <div class="small-12 columns">
            <ul>
                <li class="volume-issue"></li>
                    <li class="author">
                        <a href="/core/search?filters%5BauthorTerms%5D=Vasudevan Nedumpozhimana&eventCode=SE-AU" class="more-by-this-author ">Vasudevan Nedumpozhimana</a><a target="_blank" class="orcid-logo " href="https://orcid.org/0000-0001-5161-8925" aria-label="Open the ORCID record for Vasudevan Nedumpozhimana in new tab/window"></a> <sup data-affiliation-id="a1" class="">(a1)</sup><span class="separator "> and </span><a href="/core/search?filters%5BauthorTerms%5D=John D. Kelleher&eventCode=SE-AU" class="more-by-this-author ">John D. Kelleher</a> <sup data-affiliation-id="a2" class="">(a2)</sup>
                    </li>
                    <li class="meta-info">DOI: <a href="https://doi.org/10.1017/nlp.2024.43" target="_blank" class="url">https://doi.org/10.1017/nlp.2024.43</a></li>
            </ul>
          </div>
        </div>  </div>

  <div class="row wrapper no-padding-top">
    <div class="small-12 columns">
      <div class="row margin-top">
        <div class="large-12 columns">
          <form id="sendTodropboxForm" action="/core/services/aop-cambridge-core/sendto" method="GET">


            <div class="row">
              <div class="large-12 columns">
                <fieldset>
                  <legend>Available formats</legend>
                
                    <label>
                      <input type="checkbox" name="formats" class="formats" value="pdf" required /> PDF
                    </label>
                
                
                
                  <small class="error formats" style="display:none">Please select a format to save.</small>
                </fieldset>              </div>
            </div>

            <div class="row">
              <div class="large-12 columns">
                <label for="usagePolicydropbox">
                  <input type="checkbox" name="usagePolicydropbox" id="usagePolicydropbox" value="Usage" required/>
                  By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services
                  <small class="error usagePolicy" style="display:none">Please confirm that you accept the terms of use.</small>
                </label>
              </div>
            </div>

            <input type="hidden" name="suppressTrackingEvent" value="false" />
            <input type="hidden" name="service" value="dropbox" />
            <input type="hidden" name="documents" value="0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="finalReturn" value="/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="_csrf" value="otkuUNF9-3Nb3vQmWJfKACI-gz-gsr4cHgfc">

          </form>

          <!-- Submit button start -->
          <div class="row margin-top">
            <div class="small-6 large-6 columns">
              <button href="#" class="button radius transparent right closeModal">Cancel</button>
            </div>
            <div class="small-6 large-6 columns">
              <button type="submit" onClick="$('#sendTodropboxForm').submit();" class="button blue radius left">
                Save
              </button>
            </div>
          </div>
          <!-- Submit button end  -->
        </div>
      </div>
    </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<script type="text/javascript">
  $('#sendTodropboxForm').on('submit', validatedropboxForm);

  var _formdropbox = $('#sendTodropboxForm');

  function resetValidationdropboxForm() {
    var _form = _formdropbox;
    _form.find('.error.formats').hide();
    _form.find('.error.usagePolicy').hide();
  }

  function validatedropboxFormats(e) {
    var _form = _formdropbox;
    var errors = false;
    var formats = _form.find('input[name="formats"]:checked');
    if (formats.length < 1) {
      _form.find('.error.formats').show();
      errors = true;
    } else {
      _form.find('.error.formats').hide();
    }
    return errors;
  }

  function validatedropboxUsagePolicy(e) {
    var _form = _formdropbox;
    var usagePolicy = _form.find('input[name="usagePolicydropbox"]');
    var errors = false;
    if (!usagePolicy.is(':checked')) {
      _form.find('.error.usagePolicy').show();
      errors = true;
    } else {
      _form.find('.error.usagePolicy').hide();
    }
    return errors;
  }



  function validatedropboxForm(e) {
    var _form = _formdropbox;
    var resultPolicy = validatedropboxUsagePolicy();
    var resultFormats = validatedropboxFormats();

    if (resultPolicy) {
      $('#usagePolicydropbox', _form).one('change', validatedropboxUsagePolicy);
    }
    if (resultFormats) {
      $('.formats', _form).one('change', validatedropboxFormats);
    }


    return !resultPolicy && !resultFormats ;
  }


  $(document).ready(function () {
    // reset validation when the modal opens
    $(document).on('open.fndtn.reveal', '[data-reveal]', function () {
      resetValidationdropboxForm();

      // automatically select if there's only one format
      if ($('#sendTodropboxForm input[name="formats"]').length === 1) {
        $('#sendTodropboxForm input[name="formats"]').prop('checked', true);
      }
    });
  });

</script><div id="googleDriveModal" class="reveal-modal large" data-reveal>
  <div class="header">
    <h1 class="heading_07">Save article to Google Drive</h1>
  </div>

  <div class="panel">
    <div class="row">
      <div class="large-12 columns margin-bottom">
          <p>
              To save this article to your Google Drive account, please select one or more formats and confirm that you agree to abide by our usage policies. If this is the first time you used this feature, you will be asked to authorise Cambridge Core to connect with your Google Drive account.
              <a href="/core/help">Find out more about saving content to Google Drive</a>.
          </p>      </div>
    </div>

        <div class="row book-title">
          <div class="small-12 columns">
            <div class="heading_08"><div class="title">Topic aware probing: From sentence length prediction to idiom identification how reliant are neural language models on topic?</div></div>
          </div>
        </div>
        <div class="row book-details book-overview">
          <div class="small-12 columns">
            <ul>
                <li class="volume-issue"></li>
                    <li class="author">
                        <a href="/core/search?filters%5BauthorTerms%5D=Vasudevan Nedumpozhimana&eventCode=SE-AU" class="more-by-this-author ">Vasudevan Nedumpozhimana</a><a target="_blank" class="orcid-logo " href="https://orcid.org/0000-0001-5161-8925" aria-label="Open the ORCID record for Vasudevan Nedumpozhimana in new tab/window"></a> <sup data-affiliation-id="a1" class="">(a1)</sup><span class="separator "> and </span><a href="/core/search?filters%5BauthorTerms%5D=John D. Kelleher&eventCode=SE-AU" class="more-by-this-author ">John D. Kelleher</a> <sup data-affiliation-id="a2" class="">(a2)</sup>
                    </li>
                    <li class="meta-info">DOI: <a href="https://doi.org/10.1017/nlp.2024.43" target="_blank" class="url">https://doi.org/10.1017/nlp.2024.43</a></li>
            </ul>
          </div>
        </div>  </div>

  <div class="row wrapper no-padding-top">
    <div class="small-12 columns">
      <div class="row margin-top">
        <div class="large-12 columns">
          <form id="sendTogoogleDriveForm" action="/core/services/aop-cambridge-core/sendto" method="GET">


            <div class="row">
              <div class="large-12 columns">
                <fieldset>
                  <legend>Available formats</legend>
                
                    <label>
                      <input type="checkbox" name="formats" class="formats" value="pdf" required /> PDF
                    </label>
                
                
                
                  <small class="error formats" style="display:none">Please select a format to save.</small>
                </fieldset>              </div>
            </div>

            <div class="row">
              <div class="large-12 columns">
                <label for="usagePolicygoogleDrive">
                  <input type="checkbox" name="usagePolicygoogleDrive" id="usagePolicygoogleDrive" value="Usage" required/>
                  By using this service, you agree that you will only keep content for personal use, and will not openly distribute them via Dropbox, Google Drive or other file sharing services
                  <small class="error usagePolicy" style="display:none">Please confirm that you accept the terms of use.</small>
                </label>
              </div>
            </div>

            <input type="hidden" name="suppressTrackingEvent" value="false" />
            <input type="hidden" name="service" value="googleDrive" />
            <input type="hidden" name="documents" value="0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="finalReturn" value="/core/journals/natural-language-processing/article/topic-aware-probing-from-sentence-length-prediction-to-idiom-identification-how-reliant-are-neural-language-models-on-topic/0FFF33B18E284DAB8FE8DCF69A963A30" />
            <input type="hidden" name="_csrf" value="otkuUNF9-3Nb3vQmWJfKACI-gz-gsr4cHgfc">

          </form>

          <!-- Submit button start -->
          <div class="row margin-top">
            <div class="small-6 large-6 columns">
              <button href="#" class="button radius transparent right closeModal">Cancel</button>
            </div>
            <div class="small-6 large-6 columns">
              <button type="submit" onClick="$('#sendTogoogleDriveForm').submit();" class="button blue radius left">
                Save
              </button>
            </div>
          </div>
          <!-- Submit button end  -->
        </div>
      </div>
    </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<script type="text/javascript">
  $('#sendTogoogleDriveForm').on('submit', validategoogleDriveForm);

  var _formgoogleDrive = $('#sendTogoogleDriveForm');

  function resetValidationgoogleDriveForm() {
    var _form = _formgoogleDrive;
    _form.find('.error.formats').hide();
    _form.find('.error.usagePolicy').hide();
  }

  function validategoogleDriveFormats(e) {
    var _form = _formgoogleDrive;
    var errors = false;
    var formats = _form.find('input[name="formats"]:checked');
    if (formats.length < 1) {
      _form.find('.error.formats').show();
      errors = true;
    } else {
      _form.find('.error.formats').hide();
    }
    return errors;
  }

  function validategoogleDriveUsagePolicy(e) {
    var _form = _formgoogleDrive;
    var usagePolicy = _form.find('input[name="usagePolicygoogleDrive"]');
    var errors = false;
    if (!usagePolicy.is(':checked')) {
      _form.find('.error.usagePolicy').show();
      errors = true;
    } else {
      _form.find('.error.usagePolicy').hide();
    }
    return errors;
  }



  function validategoogleDriveForm(e) {
    var _form = _formgoogleDrive;
    var resultPolicy = validategoogleDriveUsagePolicy();
    var resultFormats = validategoogleDriveFormats();

    if (resultPolicy) {
      $('#usagePolicygoogleDrive', _form).one('change', validategoogleDriveUsagePolicy);
    }
    if (resultFormats) {
      $('.formats', _form).one('change', validategoogleDriveFormats);
    }


    return !resultPolicy && !resultFormats ;
  }


  $(document).ready(function () {
    // reset validation when the modal opens
    $(document).on('open.fndtn.reveal', '[data-reveal]', function () {
      resetValidationgoogleDriveForm();

      // automatically select if there's only one format
      if ($('#sendTogoogleDriveForm input[name="formats"]').length === 1) {
        $('#sendTogoogleDriveForm input[name="formats"]').prop('checked', true);
      }
    });
  });

</script><div id="post-comments-modal" class="reveal-modal data-reveal medium"  data-reveal >

  <a class="close-reveal-modal">&#215;</a>

  <section>

    <div class="title-underline">
      <h4 class="heading_03">
        <span class="reply-only">Reply to:</span>
        <span class="reply-only" id="reply-comment-title"></span>
        <span class="comments-only">Submit a response</span>
      </h4>
      <div class="border"></div>
    </div>

    <form id="postCommentsForm" autocomplete="off" action="/core/services/aop-cambridge-core/comments/submit" method="POST" data-abide="">

      <input type="hidden" name="_csrf" value="otkuUNF9-3Nb3vQmWJfKACI-gz-gsr4cHgfc">
      <input type="hidden" name="_pid" id="pid" value="">
      <input type="hidden" name="_cid" id="cid" value="">

      <div class="row">

        <div class="row comments-only">
          <div class="small-12 columns">
            <label for="title">Title *
              <input name="title" id="title" type="text" maxlength="250" required="required" />
              <small class="error">Please enter a title for your response.</small>
            </label>
          </div>
        </div>

        <div class="row">
          <div class="small-12 columns">
            <label class="inline" for="comment">Contents *</label>
            <span >
              
              <a href="#" data-dropdown="comment-tip" aria-expanded="false" aria-controls="comment-tip" class="icon info tooltip-icon info-icon"
                 >
                    <span class="sr-only">Contents help</span>
                  
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 37 37" aria-hidden="true" focusable="false">
                      <circle fill="#FFF" stroke="#0072CF" stroke-miterlimit="10" cx="18.5" cy="18.5" r="17.5"/>
                      <path fill="#0072CF" d="M20.4 25.3V15.2h-4.9v2.2h1.1v7.9h-1.1v2.2h6v-2.2h-1.1zm-1.9-11.4c1 0 1.9-1 1.9-2.2 0-1.2-.8-2.2-1.9-2.2s-1.9 1-1.9 2.2c0 1.2.9 2.2 1.9 2.2z"/>
                    </svg>
              </a>
            </span>
            
              <div id="comment-tip" data-dropdown-content role="dialog" aria-modal="true" aria-hidden="true" tabindex="-1"
                class="f-dropdown content medium" data-remove-focus="true" aria-label="Contents information">
                <div class="close-container">
                  <a href="#" class="button small transparent-no-border radius tooltip-close-btn" id="tooltip-close-link-comment-tip">
                    <span class="custom-tooltip-button-remove">
                        <span class="sr-only">Close Contents help</span>
                      <svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false">
                        <defs>
                          <path d="M9.986 0C4.479 0 0 4.434 0 9.916c0 5.469 4.465 9.916 9.986 9.916 5.507 0 9.986-4.433 9.986-9.916C19.958 4.434 15.493 0 9.986 0z" id="a-comment-tip"/>
                          <mask id="b-comment-tip" x="0" y="0" width="19.972" height="19.832" fill="#fff"><use xlink:href="#a-comment-tip"/></mask>
                        </defs>
                        <g fill="none" fill-rule="evenodd">
                          <use stroke="#436FCC" mask="url(#b-comment-tip)" stroke-width="2" xlink:href="#a-comment-tip"/>
                          <path d="M14.778 13.325a.983.983 0 0 1 0 1.385.982.982 0 0 1-.704.28c-.254 0-.507-.098-.704-.28l-3.352-3.329-3.353 3.329a.973.973 0 0 1-.69.28 1.01 1.01 0 0 1-.69-.28.983.983 0 0 1 0-1.385l3.352-3.328-3.352-3.33a.983.983 0 0 1 0-1.384 1 1 0 0 1 1.395 0l3.352 3.329 3.352-3.329a1 1 0 0 1 1.394 0 .983.983 0 0 1 0 1.385l-3.352 3.329 3.352 3.328z" fill="#436FCC"/>
                        </g>
                      </svg>        </span>
                  </a>
                </div>
                <p id="info-comment-tip" class="text-left">- No HTML tags allowed<br />- Web page URLs will display as text only<br />- Lines and paragraphs break automatically<br />- Attachments, images or tables are not permitted</p>
              </div>
            <textarea name="comment" id="comment" rows="5" maxlength="60000" required="required"></textarea>
            <small class="error">Please enter your response.</small>
          </div>
        </div>
      </div>
      <hr/>
      <div class="title-underline">
        <h4 class="heading_03">Your details</h4>
        <div class="border"></div>
      </div>
      <section id="contributor-section">

        <div id="contributor-row_0" class="contributor-row">
        
          
        
          <div class="large-6 medium-12 small-12 columns left-col">
        
            <div>
              <label for="firstname_0">First name *
                <input name="firstname_0" id="firstname_0" type="text" required="required" maxlength="100" placeholder="Enter your first name" />
                <small class="error">Please enter your first name.</small>
              </label>
            </div>
        
            <div>
              <label for="lastName_0">Last name *
                <input name="lastName_0" id="lastName_0" type="text" required="required" maxlength="100" placeholder="Enter your last name" />
                <small class="error">Please enter your last name.</small>
              </label>
            </div>
        
            <div>
              <label for="emailAddress_0" class="inline-tooltip">Email *</label>
              <span >
                
                <a href="#" data-dropdown="comment-email-tip_0" aria-expanded="false" aria-controls="comment-email-tip_0" class="icon info tooltip-icon info-icon"
                   >
                      <span class="sr-only">Email help</span>
                    
                      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 37 37" aria-hidden="true" focusable="false">
                        <circle fill="#FFF" stroke="#0072CF" stroke-miterlimit="10" cx="18.5" cy="18.5" r="17.5"/>
                        <path fill="#0072CF" d="M20.4 25.3V15.2h-4.9v2.2h1.1v7.9h-1.1v2.2h6v-2.2h-1.1zm-1.9-11.4c1 0 1.9-1 1.9-2.2 0-1.2-.8-2.2-1.9-2.2s-1.9 1-1.9 2.2c0 1.2.9 2.2 1.9 2.2z"/>
                      </svg>
                </a>
              </span>
              
                <div id="comment-email-tip_0" data-dropdown-content role="dialog" aria-modal="true" aria-hidden="true" tabindex="-1"
                  class="f-dropdown content medium" data-remove-focus="true" aria-label="Email information">
                  <div class="close-container">
                    <a href="#" class="button small transparent-no-border radius tooltip-close-btn" id="tooltip-close-link-comment-email-tip_0">
                      <span class="custom-tooltip-button-remove">
                          <span class="sr-only">Close Email help</span>
                        <svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false">
                          <defs>
                            <path d="M9.986 0C4.479 0 0 4.434 0 9.916c0 5.469 4.465 9.916 9.986 9.916 5.507 0 9.986-4.433 9.986-9.916C19.958 4.434 15.493 0 9.986 0z" id="a-comment-email-tip_0"/>
                            <mask id="b-comment-email-tip_0" x="0" y="0" width="19.972" height="19.832" fill="#fff"><use xlink:href="#a-comment-email-tip_0"/></mask>
                          </defs>
                          <g fill="none" fill-rule="evenodd">
                            <use stroke="#436FCC" mask="url(#b-comment-email-tip_0)" stroke-width="2" xlink:href="#a-comment-email-tip_0"/>
                            <path d="M14.778 13.325a.983.983 0 0 1 0 1.385.982.982 0 0 1-.704.28c-.254 0-.507-.098-.704-.28l-3.352-3.329-3.353 3.329a.973.973 0 0 1-.69.28 1.01 1.01 0 0 1-.69-.28.983.983 0 0 1 0-1.385l3.352-3.328-3.352-3.33a.983.983 0 0 1 0-1.384 1 1 0 0 1 1.395 0l3.352 3.329 3.352-3.329a1 1 0 0 1 1.394 0 .983.983 0 0 1 0 1.385l-3.352 3.329 3.352 3.328z" fill="#436FCC"/>
                          </g>
                        </svg>        </span>
                    </a>
                  </div>
                  <p id="info-comment-email-tip_0" class="text-left">Your email address will be used in order to notify you when your comment has been reviewed by the moderator and in case the author(s) of the article or the moderator need to contact you directly.</p>
                </div>
              <input name="emailAddress_0" id="emailAddress_0" type="text" required="required" maxlength="100" pattern="email" placeholder="Enter your email" />
              <small class="error">Please enter a valid email address.</small>
            </div>
        
          </div>
        
          <div class="large-6 medium-12 small-12 columns">
        
            <div class="row">
              <label for="occupation_0">Occupation
                <input name="occupation_0" id="occupation_0" type="text" maxlength="100" placeholder="Enter your role and/or occupation" />
                <small class="error">Please enter your occupation.</small>
              </label>
            </div>
        
            <div class="row">
              <label for="organisation_0">Affiliation
                <input name="organisation_0" id="organisation_0" type="text" maxlength="100" placeholder="Enter your organisation or institution name" />
                <small class="error">Please enter any affiliation.</small>
              </label>
            </div>
        
          </div>
        
        </div>
      </section>
      <div class="contributor-btn">
        <input id="add-contributor-btn" type="button" class="blue small button radius add-contributor" value="Add contributor">
      </div>

      <div class="add-contributor-limit-reached">
        <hr>
        <h5 class="heading_05">You have entered the maximum number of contributors</h5>
      </div>
      <hr/>
      <div class="title-underline">
        <h4 class="heading_03">Conflicting interests</h4>
        <div class="border"></div>
      </div>
      <div class="large-6 medium-12 small-12 columns">

        <p>
          <span >
            Do you have any conflicting interests? *
            <a href="#" data-dropdown="comment-conflict-tip" aria-expanded="false" aria-controls="comment-conflict-tip" class="icon info tooltip-icon info-icon"
               >
                  <span class="sr-only">Conflicting interests help</span>
                
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 37 37" aria-hidden="true" focusable="false">
                    <circle fill="#FFF" stroke="#0072CF" stroke-miterlimit="10" cx="18.5" cy="18.5" r="17.5"/>
                    <path fill="#0072CF" d="M20.4 25.3V15.2h-4.9v2.2h1.1v7.9h-1.1v2.2h6v-2.2h-1.1zm-1.9-11.4c1 0 1.9-1 1.9-2.2 0-1.2-.8-2.2-1.9-2.2s-1.9 1-1.9 2.2c0 1.2.9 2.2 1.9 2.2z"/>
                  </svg>
            </a>
          </span>
          
            <div id="comment-conflict-tip" data-dropdown-content role="dialog" aria-modal="true" aria-hidden="true" tabindex="-1"
              class="f-dropdown content medium" data-remove-focus="true" aria-label="Conflicting interests information">
              <div class="close-container">
                <a href="#" class="button small transparent-no-border radius tooltip-close-btn" id="tooltip-close-link-comment-conflict-tip">
                  <span class="custom-tooltip-button-remove">
                      <span class="sr-only">Close Conflicting interests help</span>
                    <svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false">
                      <defs>
                        <path d="M9.986 0C4.479 0 0 4.434 0 9.916c0 5.469 4.465 9.916 9.986 9.916 5.507 0 9.986-4.433 9.986-9.916C19.958 4.434 15.493 0 9.986 0z" id="a-comment-conflict-tip"/>
                        <mask id="b-comment-conflict-tip" x="0" y="0" width="19.972" height="19.832" fill="#fff"><use xlink:href="#a-comment-conflict-tip"/></mask>
                      </defs>
                      <g fill="none" fill-rule="evenodd">
                        <use stroke="#436FCC" mask="url(#b-comment-conflict-tip)" stroke-width="2" xlink:href="#a-comment-conflict-tip"/>
                        <path d="M14.778 13.325a.983.983 0 0 1 0 1.385.982.982 0 0 1-.704.28c-.254 0-.507-.098-.704-.28l-3.352-3.329-3.353 3.329a.973.973 0 0 1-.69.28 1.01 1.01 0 0 1-.69-.28.983.983 0 0 1 0-1.385l3.352-3.328-3.352-3.33a.983.983 0 0 1 0-1.384 1 1 0 0 1 1.395 0l3.352 3.329 3.352-3.329a1 1 0 0 1 1.394 0 .983.983 0 0 1 0 1.385l-3.352 3.329 3.352 3.328z" fill="#436FCC"/>
                      </g>
                    </svg>        </span>
                </a>
              </div>
              <p id="info-comment-conflict-tip" class="text-left">Please list any fees and grants from, employment by, consultancy for, shared ownership in or any close relationship with, at any time over the preceding 36 months, any organisation whose interests may be affected by the publication of the response. Please also list any non-financial associations or interests (personal, professional, political, institutional, religious or other) that a reasonable reader would want to know about in relation to the submitted work. This pertains to all the authors of the piece, their spouses or partners.</p>
            </div>
        </p>

        <label>
          <input type="radio" name="conflictInterest" id="frm-conflict-on" value="true"> Yes
        </label>
        <label>
          <input type="radio" name="conflictInterest" id="frm-conflict-off" checked="checked" value="false"> No
        </label>
      </div>
      <div class="large-6 medium-12 small-12 columns comment-more-info">

        <label>More information *
          <textarea name="conflictInfo" id="conflictInfo" rows="3" maxlength="500" placeholder="Enter details of your conflicting interests" disabled="false" required="required"></textarea>
          <small class="error">Please enter details of the conflict of interest or select &#x27;No&#x27;.</small>
        </label>

      </div>

      <hr />

      <div class="row margin-top">
        <div class="large-12 columns">
          <label class="paragraph_05">
            <input name="agreeToTerms" type="checkbox" value="yes" id="agreeToTerms" required="required" />
          &nbsp; Please tick the box to confirm you agree to our&nbsp;<a href="/core/legal-notices/terms" target="_blank">Terms of use</a>. *<br /><br />
            <small class="error">Please accept terms of use.</small>
          </label>
        </div>
      </div>

      <div class="row margin-top">
        <div class="large-12 columns">
          <label class="paragraph_05">
            <input name="agreePrint" type="checkbox" value="yes" id="agreePrint" required="required" />
          &nbsp; Please tick the box to confirm you agree that your name, comment and conflicts of interest (if accepted) will be visible on the website and your comment may be printed in the journal at the Editor’s discretion. *<br /><br />
            <small class="error">Please confirm you agree that your details will be displayed.</small>
          </label>
        </div>
      </div>
      <hr />
      <div class="submit-btn">
        <input type="submit" class="blue small button radius" value=Submit>
      </div>
    </form>
  </section>
</div>
<script type="text/template" data-template="contributorsTemplate">
  <div id="contributor-row_--x--" class="contributor-row contributor-row-hide">
  
    <hr />
  
    <div class="large-6 medium-12 small-12 columns left-col">
  
      <div>
        <label for="firstname_--x--">First name *
          <input name="firstname_--x--" id="firstname_--x--" type="text" required="required" maxlength="100" placeholder="Enter contributor first name" />
          <small class="error">Please enter your first name.</small>
        </label>
      </div>
  
      <div>
        <label for="lastName_--x--">Last name *
          <input name="lastName_--x--" id="lastName_--x--" type="text" required="required" maxlength="100" placeholder="Enter contributor last name" />
          <small class="error">Please enter your last name.</small>
        </label>
      </div>
  
      <div>
        <label for="emailAddress_--x--" class="inline-tooltip">Email *</label>
        <span >
          
          <a href="#" data-dropdown="comment-email-tip_--x--" aria-expanded="false" aria-controls="comment-email-tip_--x--" class="icon info tooltip-icon info-icon"
             >
                <span class="sr-only">Email help</span>
              
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 37 37" aria-hidden="true" focusable="false">
                  <circle fill="#FFF" stroke="#0072CF" stroke-miterlimit="10" cx="18.5" cy="18.5" r="17.5"/>
                  <path fill="#0072CF" d="M20.4 25.3V15.2h-4.9v2.2h1.1v7.9h-1.1v2.2h6v-2.2h-1.1zm-1.9-11.4c1 0 1.9-1 1.9-2.2 0-1.2-.8-2.2-1.9-2.2s-1.9 1-1.9 2.2c0 1.2.9 2.2 1.9 2.2z"/>
                </svg>
          </a>
        </span>
        
          <div id="comment-email-tip_--x--" data-dropdown-content role="dialog" aria-modal="true" aria-hidden="true" tabindex="-1"
            class="f-dropdown content medium" data-remove-focus="true" aria-label="Email information">
            <div class="close-container">
              <a href="#" class="button small transparent-no-border radius tooltip-close-btn" id="tooltip-close-link-comment-email-tip_--x--">
                <span class="custom-tooltip-button-remove">
                    <span class="sr-only">Close Email help</span>
                  <svg width="20" height="20" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" focusable="false">
                    <defs>
                      <path d="M9.986 0C4.479 0 0 4.434 0 9.916c0 5.469 4.465 9.916 9.986 9.916 5.507 0 9.986-4.433 9.986-9.916C19.958 4.434 15.493 0 9.986 0z" id="a-comment-email-tip_--x--"/>
                      <mask id="b-comment-email-tip_--x--" x="0" y="0" width="19.972" height="19.832" fill="#fff"><use xlink:href="#a-comment-email-tip_--x--"/></mask>
                    </defs>
                    <g fill="none" fill-rule="evenodd">
                      <use stroke="#436FCC" mask="url(#b-comment-email-tip_--x--)" stroke-width="2" xlink:href="#a-comment-email-tip_--x--"/>
                      <path d="M14.778 13.325a.983.983 0 0 1 0 1.385.982.982 0 0 1-.704.28c-.254 0-.507-.098-.704-.28l-3.352-3.329-3.353 3.329a.973.973 0 0 1-.69.28 1.01 1.01 0 0 1-.69-.28.983.983 0 0 1 0-1.385l3.352-3.328-3.352-3.33a.983.983 0 0 1 0-1.384 1 1 0 0 1 1.395 0l3.352 3.329 3.352-3.329a1 1 0 0 1 1.394 0 .983.983 0 0 1 0 1.385l-3.352 3.329 3.352 3.328z" fill="#436FCC"/>
                    </g>
                  </svg>        </span>
              </a>
            </div>
            <p id="info-comment-email-tip_--x--" class="text-left">Your email address will be used in order to notify you when your comment has been reviewed by the moderator and in case the author(s) of the article or the moderator need to contact you directly.</p>
          </div>
        <input name="emailAddress_--x--" id="emailAddress_--x--" type="text" required="required" maxlength="100" pattern="email" placeholder="Enter contributor email" />
        <small class="error">Please enter a valid email address.</small>
      </div>
  
    </div>
  
    <div class="large-6 medium-12 small-12 columns">
  
      <div class="row">
        <label for="occupation_--x--">Occupation
          <input name="occupation_--x--" id="occupation_--x--" type="text" maxlength="100" placeholder="Enter contributor role and/or occupation" />
          <small class="error">Please enter your occupation.</small>
        </label>
      </div>
  
      <div class="row">
        <label for="organisation_0">Affiliation
          <input name="organisation_--x--" id="organisation_--x--" type="text" maxlength="100" placeholder="Enter contributor organisation or institution name" />
          <small class="error">Please enter any affiliation.</small>
        </label>
      </div>
  
        <input type="button" id="remove-contributor_--x--" class="small button alert radius remove-contributor" value="Remove contributor">
    </div>
  
  </div></script>

<script>
  window.AOP.basket.currency = 'USD';
  window.AOP.basket.isEnabled = true;
</script>
<script>
  /* style checkboxes and radio inputs */
  AOP.styleInputElements = function (container) {
    var selector;

    // Use '.no-style' on the input element to exclude checkboxes from styling
    if (container) {
      selector = $('input[type=checkbox], input[type=radio]', container).not('.styled,.no-style');
    } else {
      selector = $('input[type=checkbox], input[type=radio]').not('.styled,.no-style');
    }

    selector.each(function (i, el) {
      $(this).addClass('styled');
      el = $(el)[0];
      var doneAlready = (el && el.nextElementSibling && el.nextElementSibling.outerHTML === '<span></span>');

      if (!doneAlready) {
        $(this).after($('<span></span>'));
      }
    });
  };

  $(document).ready(function () {
    AOP.styleInputElements();
  });
</script>