from typing import List, Dict, Any, Optional, Union
import os
import json
import requests
import re
import arxiv
from bs4 import BeautifulSoup
import hashlib

from .serpapi_scholar import ScholarSearchTool
from .openalex_api import OpenAlexTool
from utils.logger import logger

class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, title: str, authors: List[str], abstract: str, 
                 url: str, published_date: str = "", 
                 doi: str = "", pdf_url: str = "",
                 citation_count: Optional[int] = None,
                 source: str = ""):
        self.title = title
        self.authors = authors
        self.abstract = abstract
        self.url = url
        self.published_date = published_date
        self.doi = doi
        self.pdf_url = pdf_url
        self.citation_count = citation_count
        self.source = source
    
    def to_dict(self) -> Dict[str, Any]:
        """ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "title": self.title,
            "authors": self.authors,
            "abstract": self.abstract,
            "url": self.url,
            "published_date": self.published_date,
            "doi": self.doi,
            "pdf_url": self.pdf_url,
            "citation_count": self.citation_count,
            "source": self.source
        }


class AcademicSearchManager:
    """
    ë‹¤ì–‘í•œ í•™ìˆ  ê²€ìƒ‰ APIë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.scholar_tool = ScholarSearchTool()
        self.openalex_tool = OpenAlexTool()
        
        # Google API í‚¤ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")
        
        # SerpApi í‚¤
        self.serpapi_key = os.getenv("SerpApi_key")
    
    def search(self, 
              query: str, 
              source: str = "all", 
              limit: int = 10,
              year_start: Optional[int] = None,
              year_end: Optional[int] = None) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ í•™ìˆ  ê²€ìƒ‰ ì†ŒìŠ¤ì—ì„œ ë™ì‹œì— ê²€ìƒ‰ì„ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            source: ê²€ìƒ‰ ì†ŒìŠ¤ ('scholar', 'openalex', 'google', 'arxiv', 'crossref', 'all')
            limit: ê° ì†ŒìŠ¤ë‹¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            year_start: ì‹œì‘ ì—°ë„ (ì„ íƒ ì‚¬í•­)
            year_end: ì¢…ë£Œ ì—°ë„ (ì„ íƒ ì‚¬í•­)
            
        Returns:
            í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        results = {
            "query": query,
            "sources_used": [],
            "results": []
        }
        
        # Google Scholar ê²€ìƒ‰ (SerpApi)
        if source.lower() in ["scholar", "all"]:
            try:
                scholar_results = self.scholar_tool.search_scholar(
                    query=query,
                    num_results=limit,
                    year_start=year_start,
                    year_end=year_end
                )
                
                if scholar_results:
                    results["sources_used"].append("google_scholar")
                    
                    # í˜•ì‹ í†µì¼
                    for item in scholar_results:
                        results["results"].append({
                            "source": "google_scholar",
                            "title": item.get("title", ""),
                            "link": item.get("link", ""),
                            "abstract": item.get("snippet", ""),
                            "authors": item.get("publication_info", {}).get("authors", []),
                            "year": item.get("publication_info", {}).get("year", ""),
                            "raw_data": item  # ì›ë³¸ ë°ì´í„° ë³´ì¡´
                        })
            except Exception as e:
                logger.error(f"Google Scholar ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # OpenAlex ê²€ìƒ‰
        if source.lower() in ["openalex", "all"]:
            try:
                filter_options = {}
                if year_start and year_end:
                    filter_options["from_publication_date"] = f"{year_start}-01-01"
                    filter_options["to_publication_date"] = f"{year_end}-12-31"
                
                openalex_data = self.openalex_tool.search_works(
                    query=query,
                    limit=limit,
                    filter_options=filter_options
                )
                
                if openalex_data and "results" in openalex_data and openalex_data["results"]:
                    results["sources_used"].append("openalex")
                    
                    # í˜•ì‹ í†µì¼
                    for item in openalex_data["results"]:
                        # ì €ì ì •ë³´ ì¶”ì¶œ
                        authors = []
                        if "authorships" in item:
                            for authorship in item["authorships"]:
                                if "author" in authorship and "display_name" in authorship["author"]:
                                    authors.append(authorship["author"]["display_name"])
                        
                        # ê²°ê³¼ ì¶”ê°€
                        results["results"].append({
                            "source": "openalex",
                            "title": item.get("title", ""),
                            "link": item.get("doi", ""),
                            "abstract": self._extract_abstract_from_openalex(item),
                            "authors": authors,
                            "year": item.get("publication_year", ""),
                            "journal": item.get("primary_location", {}).get("source", {}).get("display_name", ""),
                            "raw_data": item  # ì›ë³¸ ë°ì´í„° ë³´ì¡´
                        })
            except Exception as e:
                logger.error(f"OpenAlex ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # êµ¬ê¸€ ê²€ìƒ‰ (ì¼ë°˜)
        if source.lower() in ["google", "all"]:
            try:
                google_results = self.google_search(
                    query=query,
                    num_results=limit
                )
                
                if google_results:
                    results["sources_used"].append("google")
                    
                    for item in google_results:
                        results["results"].append({
                            "source": "google",
                            "title": item.get("title", ""),
                            "link": item.get("link", ""),
                            "abstract": item.get("snippet", ""),
                            "authors": [],
                            "year": "",
                            "raw_data": item
                        })
            except Exception as e:
                logger.error(f"Google ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # arXiv ê²€ìƒ‰
        if source.lower() in ["arxiv", "all"]:
            try:
                arxiv_results = self.search_arxiv(
                    query=query,
                    max_results=limit
                )
                
                if arxiv_results:
                    results["sources_used"].append("arxiv")
                    
                    for item in arxiv_results:
                        results["results"].append({
                            "source": "arxiv",
                            "title": item.get("title", ""),
                            "link": item.get("url", ""),
                            "abstract": item.get("abstract", ""),
                            "authors": item.get("authors", []),
                            "year": item.get("published_date", "")[:4] if item.get("published_date") else "",
                            "pdf_url": item.get("pdf_url", ""),
                            "raw_data": item
                        })
            except Exception as e:
                logger.error(f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # Crossref ê²€ìƒ‰
        if source.lower() in ["crossref", "all"]:
            try:
                crossref_results = self.search_crossref(
                    query=query,
                    max_results=limit
                )
                
                if crossref_results:
                    results["sources_used"].append("crossref")
                    
                    for item in crossref_results:
                        results["results"].append({
                            "source": "crossref",
                            "title": item.get("title", ""),
                            "link": item.get("url", ""),
                            "abstract": item.get("abstract", ""),
                            "authors": item.get("authors", "").split(", ") if isinstance(item.get("authors"), str) else [],
                            "year": item.get("published_date", "")[:4] if item.get("published_date") else "",
                            "doi": item.get("doi", ""),
                            "raw_data": item
                        })
            except Exception as e:
                logger.error(f"Crossref ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _extract_abstract_from_openalex(self, item):
        """OpenAlex ê²°ê³¼ì—ì„œ ì´ˆë¡ ì¶”ì¶œ"""
        if "abstract_inverted_index" in item and item["abstract_inverted_index"]:
            # OpenAlexëŠ” inverted_index í˜•ì‹ìœ¼ë¡œ ì´ˆë¡ì„ ì œê³µ
            # ê°„ë‹¨í•œ ë³€í™˜ë§Œ ìˆ˜í–‰
            return "ì´ˆë¡ ìˆìŒ (ì¶”ê°€ ì²˜ë¦¬ í•„ìš”)"
        return ""
    
    def format_search_results(self, results: Dict[str, Any]) -> str:
        """
        í†µí•© ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        if not results or not results.get("results"):
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = f"ê²€ìƒ‰ì–´: {results['query']}\n"
        formatted += f"ì‚¬ìš©ëœ ì†ŒìŠ¤: {', '.join(results['sources_used'])}\n\n"
        
        for i, item in enumerate(results["results"], 1):
            # ì†ŒìŠ¤ë³„ ì•„ì´ì½˜
            source_icon = {
                "google_scholar": "ğŸ”",
                "openalex": "ğŸ“š",
                "google": "ğŸŒ",
                "arxiv": "ğŸ“‘",
                "crossref": "ğŸ”—"
            }.get(item["source"], "ğŸ“„")
            
            formatted += f"{i}. {source_icon} {item['title']}\n"
            
            # ì €ì
            if item.get("authors"):
                if isinstance(item["authors"], list):
                    formatted += f"   ì €ì: {', '.join(item['authors'])}\n"
                else:
                    formatted += f"   ì €ì: {item['authors']}\n"
            
            # ì¶œíŒ ì—°ë„
            if item.get("year"):
                formatted += f"   ì¶œíŒ ì—°ë„: {item['year']}\n"
            
            # ì €ë„
            if item.get("journal"):
                formatted += f"   ì €ë„: {item['journal']}\n"
            
            # ë§í¬
            if item.get("link"):
                formatted += f"   ë§í¬: {item['link']}\n"
            
            # DOI
            if item.get("doi"):
                formatted += f"   DOI: {item['doi']}\n"
            
            # ì´ˆë¡/ìŠ¤ë‹ˆí«
            if item.get("abstract"):
                # ì´ˆë¡ì´ ê¸¸ ê²½ìš° ì¤„ì„
                abstract = item["abstract"]
                if len(abstract) > 200:
                    abstract = abstract[:200] + "..."
                formatted += f"   ìš”ì•½: {abstract}\n"
            
            formatted += "\n"
        
        return formatted
    
    def get_citations_for_rag(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¸ìš© í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ë³€í™˜
        """
        citations = []
        
        for item in results.get("results", []):
            citation = {
                "title": item.get("title", ""),
                "authors": ", ".join(item.get("authors", [])) if isinstance(item.get("authors", []), list) else item.get("authors", ""),
                "year": str(item.get("year", "")),
                "source": item.get("journal", "") or item.get("source", ""),
                "link": item.get("link", ""),
                "snippet": item.get("abstract", ""),
                "citation_text": ""
            }
            
            # APA í˜•ì‹ì˜ ì¸ìš© í…ìŠ¤íŠ¸ ìƒì„±
            authors_text = citation["authors"]
            if authors_text:
                if "," in authors_text:  # ì—¬ëŸ¬ ì €ì
                    authors_text = authors_text.split(", ")[0] + " et al."
            
            citation["citation_text"] = f"{authors_text or 'Unknown'} ({citation['year'] or 'n.d.'}). {citation['title']}. {citation['source']}."
            
            citations.append(citation)
            
        return citations
    
    # ê¸°ì¡´ search.py ë° search_utils.pyì˜ í•¨ìˆ˜ë“¤ í†µí•©
    
    def google_search(self, query: str, num_results: int = 10, language: str = 'en') -> List[Dict[str, Any]]:
        """
        Google ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        """
        if not self.google_api_key or not self.google_cse_id:
            logger.warning("Google API í‚¤ ë˜ëŠ” CSE IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ê²°ê³¼ ìˆ˜ ì œí•œ (API ì œí•œ)
        num_results = min(num_results, 10)
        
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": self.google_api_key,
            "cx": self.google_cse_id,
            "q": query,
            "num": num_results,
            "lr": f'lang_{language}'
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            results = response.json()
            
            if "items" not in results:
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {query}")
                return []
            
            formatted_results = []
            for item in results["items"]:
                formatted_results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "source": "google"
                })
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """
        arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
        """
        logger.info(f"arXiv ê²€ìƒ‰: '{query}', ìµœëŒ€ {max_results}ê°œ ê²°ê³¼")
        
        try:
            # arXiv API ê²€ìƒ‰
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            results = []
            for paper in search.results():
                authors = [author.name for author in paper.authors]
                
                result = {
                    'title': paper.title,
                    'authors': authors,
                    'abstract': paper.summary,
                    'url': paper.entry_id,
                    'pdf_url': paper.pdf_url,
                    'published_date': paper.published.strftime('%Y-%m-%d'),
                    'source': 'arXiv'
                }
                
                results.append(result)
            
            logger.info(f"arXiv ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸ ì°¾ìŒ")
            return results
            
        except Exception as e:
            logger.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def search_crossref(self, query: str, max_results: int = 10, filter: str = None) -> List[Dict[str, Any]]:
        """
        Crossref APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
        """
        logger.info(f"Crossref ê²€ìƒ‰: '{query}', ìµœëŒ€ {max_results}ê°œ ê²°ê³¼")
        
        try:
            # Crossref API ê²€ìƒ‰
            url = "https://api.crossref.org/works"
            params = {
                "query": query,
                "rows": max_results,
                "sort": "relevance",
                "select": "DOI,title,author,abstract,URL,published-print,published-online"
            }
            
            if filter:
                params["filter"] = filter
            
            # ì‚¬ìš©ì ì—ì´ì „íŠ¸ í—¤ë” ì¶”ê°€ (Crossref ê¶Œì¥)
            headers = {"User-Agent": "ResearchAgent/1.0 (mailto:example@example.com)"}
            
            response = requests.get(url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            results = []
            if "message" in data and "items" in data["message"]:
                for item in data["message"]["items"]:
                    # ë°ì´í„° ì¶”ì¶œ ë¡œì§
                    title = item.get("title", [""])[0] if "title" in item and len(item["title"]) > 0 else ""
                    
                    # ì €ì ì •ë³´ ì¶”ì¶œ
                    authors = []
                    if "author" in item:
                        for author in item["author"]:
                            name_parts = []
                            if "given" in author:
                                name_parts.append(author["given"])
                            if "family" in author:
                                name_parts.append(author["family"])
                            
                            if name_parts:
                                authors.append(" ".join(name_parts))
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    published_date = ""
                    if "published-print" in item and "date-parts" in item["published-print"]:
                        date_parts = item["published-print"]["date-parts"][0]
                        if len(date_parts) >= 1:
                            published_date = str(date_parts[0])
                    elif "published-online" in item and "date-parts" in item["published-online"]:
                        date_parts = item["published-online"]["date-parts"][0]
                        if len(date_parts) >= 1:
                            published_date = str(date_parts[0])
                    
                    # URL ë° DOI ì¶”ì¶œ
                    url = item.get("URL", "")
                    doi = item.get("DOI", "")
                    
                    # PDF URL ì¶”ì •
                    pdf_url = f"https://doi.org/{doi}" if doi else url
                    
                    # ì¶”ìƒ ì¶”ì¶œ
                    abstract = ""
                    if "abstract" in item:
                        abstract = item["abstract"]
                    
                    result = {
                        'title': title,
                        'authors': ', '.join(authors),
                        'abstract': abstract,
                        'url': url,
                        'doi': doi,
                        'published_date': published_date,
                        'pdf_url': pdf_url,
                        'source': 'Crossref'
                    }
                    
                    results.append(result)
            
            logger.info(f"Crossref ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸ ì°¾ìŒ")
            return results
            
        except Exception as e:
            logger.error(f"Crossref ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def format_results_as_markdown(self, results: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        """
        if not results or not results.get("results"):
            return "# ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        markdown = f"# '{results['query']}' ê²€ìƒ‰ ê²°ê³¼\n\n"
        markdown += f"**ì‚¬ìš©ëœ ì†ŒìŠ¤**: {', '.join(results['sources_used'])}\n\n"
        
        for i, item in enumerate(results["results"], 1):
            source_name = {
                "google_scholar": "Google Scholar",
                "openalex": "OpenAlex",
                "google": "Google",
                "arxiv": "arXiv",
                "crossref": "Crossref"
            }.get(item["source"], item["source"])
            
            markdown += f"## {i}. {item['title']}\n\n"
            markdown += f"**ì¶œì²˜**: {source_name}\n\n"
            
            if item.get("authors"):
                if isinstance(item["authors"], list):
                    markdown += f"**ì €ì**: {', '.join(item['authors'])}\n\n"
                else:
                    markdown += f"**ì €ì**: {item['authors']}\n\n"
            
            if item.get("year"):
                markdown += f"**ì¶œíŒ ì—°ë„**: {item['year']}\n\n"
            
            if item.get("journal"):
                markdown += f"**ì €ë„**: {item['journal']}\n\n"
            
            if item.get("abstract"):
                markdown += f"**ìš”ì•½**:\n\n> {item['abstract']}\n\n"
            
            if item.get("link"):
                markdown += f"**ë§í¬**: [{item['link']}]({item['link']})\n\n"
            
            markdown += "---\n\n"
        
        return markdown
    
    # search.pyì˜ test_academic_search í•¨ìˆ˜ë„ í†µí•©
    def test_academic_search(self, query: str, max_results: int = 5) -> str:
        """
        í•™ìˆ  ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            str: í¬ë§·íŒ…ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        # í†µí•© ê²€ìƒ‰ ì‹¤í–‰
        results = self.search(
            query=query,
            source="all",
            limit=max_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ… (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        formatted_results = self.format_results_as_markdown(results)
        
        return formatted_results

    # ê¸°ì¡´ ë©”ì„œë“œ ì™¸ì— í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ ì¶”ê°€
    def google_search(self, query: str, num_results: int = 10, language: str = 'en') -> list:
        """
        êµ¬ê¸€ ê²€ìƒ‰ì„ ìˆ˜í–‰ (í˜¸í™˜ì„± ìœ ì§€ìš©)
        """
        # ë‚´ë¶€ì ìœ¼ë¡œ search ë©”ì„œë“œ ì‚¬ìš©
        results = self.search(
            query=query,
            source="google",
            limit=num_results
        )
        
        # ì›ë˜ ë°˜í™˜ í˜•ì‹ì— ë§ê²Œ ì¡°ì •
        return results.get("results", [])

    def search_with_fallback(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """
        SerpAPI ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ê²€ìƒ‰ ë°©ë²•ì„ ì‚¬ìš©
        """
        try:
            # ë¨¼ì € ëª¨ë“  ì†ŒìŠ¤ë¡œ ê²€ìƒ‰ ì‹œë„
            results = self.search(query=query, source="all", limit=limit)
            if not results.get("results"):
                # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Google Scholar ì œì™¸í•˜ê³  ë‹¤ì‹œ ì‹œë„
                logger.info("ì „ì²´ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, OpenAlexë§Œ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„")
                return self.search(query=query, source="openalex", limit=limit)
            return results
        except Exception as e:
            logger.warning(f"í†µí•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}, OpenAlexë§Œ ì‚¬ìš©í•˜ì—¬ ì¬ì‹œë„")
            return self.search(query=query, source="openalex", limit=limit) 