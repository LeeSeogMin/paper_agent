from typing import List, Dict, Any, Optional, Union
import os
import json
import requests
import re
import arxiv
from bs4 import BeautifulSoup
import hashlib
import time
import traceback

from .serpapi_scholar import ScholarSearchTool
from .openalex_api import OpenAlexTool
from utils.logger import logger

class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, title: str, authors: List[str], abstract: str, 
                 url: str, published_date: str = "", 
                 doi: str = "", pdf_url: str = "",
                 citation_count: Optional[int] = None,
                 source: str = ""):
        self.title = title
        self.authors = authors
        self.abstract = abstract
        self.url = url
        self.published_date = published_date
        self.doi = doi
        self.pdf_url = pdf_url
        self.citation_count = citation_count
        self.source = source
    
    def to_dict(self) -> Dict[str, Any]:
        """ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "title": self.title,
            "authors": self.authors,
            "abstract": self.abstract,
            "url": self.url,
            "published_date": self.published_date,
            "doi": self.doi,
            "pdf_url": self.pdf_url,
            "citation_count": self.citation_count,
            "source": self.source
        }


class AcademicSearchManager:
    """
    ë‹¤ì–‘í•œ í•™ìˆ  ê²€ìƒ‰ APIë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.scholar_tool = ScholarSearchTool()
        self.openalex_tool = OpenAlexTool()
        
        # Google API í‚¤ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")
        
        # SerpApi í‚¤
        self.serpapi_key = os.getenv("SerpApi_key")
    
    def search(self, 
              query: str, 
              source: str = "all", 
              limit: int = 10,
              year_start: Optional[int] = None,
              year_end: Optional[int] = None) -> Dict[str, Any]:
        """
        í†µí•© í•™ìˆ  ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            source: ê²€ìƒ‰ ì†ŒìŠ¤ ('scholar', 'openalex', 'google', 'arxiv', 'crossref', 'all')
            limit: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            year_start: ê²€ìƒ‰ ì‹œì‘ ì—°ë„ (ì„ íƒ ì‚¬í•­)
            year_end: ê²€ìƒ‰ ì¢…ë£Œ ì—°ë„ (ì„ íƒ ì‚¬í•­)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ (ì†ŒìŠ¤ë³„ë¡œ êµ¬ë¶„)
        """
        logger.info(f"search ë©”ì„œë“œ í˜¸ì¶œ: query={query}, source={source}, limit={limit}")
        
        # í˜¸ì¶œ ìŠ¤íƒ ì¶”ì ì„ ìœ„í•œ ë¡œê¹…
        import traceback
        call_stack = traceback.format_stack()
        logger.debug(f"search ë©”ì„œë“œ í˜¸ì¶œ ìŠ¤íƒ:\n{''.join(call_stack)}")
        
        # ì¿¼ë¦¬ ìµœì í™” (íŠ¹íˆ OpenAlex ê²€ìƒ‰ìš©)
        optimized_query = self._optimize_query(query)
        
        # ê²°ê³¼ ì´ˆê¸°í™”
        results = {
            "query": query,
            "sources_used": [],
            "results": []
        }
        
        # ê²€ìƒ‰í•  ì†ŒìŠ¤ ê²°ì •
        sources_to_search = []
        if source.lower() == "all":
            sources_to_search = ["scholar", "openalex", "google", "arxiv", "crossref"]
        else:
            sources_to_search = [source.lower()]
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤í–‰
        for search_source in sources_to_search:
            try:
                if search_source == "scholar":
                    # Google Scholar ê²€ìƒ‰ (SerpApi)
                    try:
                        scholar_results = self.scholar_tool.search_scholar(
                            query=optimized_query,
                            num_results=limit,
                            year_start=year_start,
                            year_end=year_end
                        )
                        
                        # ê²°ê³¼ ìœ íš¨ì„± ê²€ì‚¬
                        if scholar_results and isinstance(scholar_results, list):
                            results["sources_used"].append("google_scholar")
                            
                            # í˜•ì‹ í†µì¼
                            for item in scholar_results:
                                if not isinstance(item, dict):
                                    logger.warning(f"Google Scholar ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(item)}")
                                    continue
                                        
                                results["results"].append({
                                    "source": "google_scholar",
                                    "title": item.get("title", ""),
                                    "link": item.get("link", ""),
                                    "abstract": item.get("snippet", ""),
                                    "authors": item.get("publication_info", {}).get("authors", []) if isinstance(item.get("publication_info"), dict) else [],
                                    "year": item.get("publication_info", {}).get("year", "") if isinstance(item.get("publication_info"), dict) else "",
                                    "raw_data": item  # ì›ë³¸ ë°ì´í„° ë³´ì¡´
                                })
                        elif scholar_results:
                            logger.warning(f"Google Scholar ê²€ìƒ‰ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(scholar_results)}")
                    except Exception as e:
                        logger.error(f"Google Scholar ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                
                elif search_source == "openalex":
                    # OpenAlex ê²€ìƒ‰
                    try:
                        filter_options = {}
                        if year_start and year_end:
                            filter_options["from_publication_date"] = f"{year_start}-01-01"
                            filter_options["to_publication_date"] = f"{year_end}-12-31"
                        
                        openalex_data = self.openalex_tool.search_works(
                            query=optimized_query,
                            limit=limit,
                            filter_options=filter_options
                        )
                        
                        # OpenAlex ì‘ë‹µ ê²€ì¦
                        if not openalex_data:
                            logger.warning(f"OpenAlex ê²€ìƒ‰ ê²°ê³¼ê°€ None ë˜ëŠ” ë¹ˆ ê°’: {query}")
                            continue
                        
                        if isinstance(openalex_data, dict):
                            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°, í•„ìš”í•œ ë°ì´í„°ê°€ ìˆëŠ” í‚¤ í™•ì¸
                            if "results" in openalex_data:
                                # "results" í‚¤ì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                                openalex_data = openalex_data["results"]
                                logger.info(f"OpenAlex ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
                            else:
                                # í•„ìš”í•œ í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                                logger.warning(f"OpenAlex ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ì— 'results' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                openalex_data = []
                        
                        # ì´ì œ openalex_dataëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³´ì¥ë¨
                        if not isinstance(openalex_data, list):
                            logger.warning(f"OpenAlex ê²€ìƒ‰ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜: {type(openalex_data)}")
                            continue
                        
                        results["sources_used"].append("openalex")
                        
                        # ê²°ê³¼ ì²˜ë¦¬
                        for item in openalex_data:
                            if not isinstance(item, dict):
                                logger.warning(f"OpenAlex ê²€ìƒ‰ ê²°ê³¼ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(item)}")
                                continue
                            
                            # ì €ì ì •ë³´ ì¶”ì¶œ
                            authors = []
                            authorships = item.get("authorships", [])
                            if authorships and isinstance(authorships, list):
                                for authorship in authorships:
                                    if isinstance(authorship, dict) and "author" in authorship:
                                        author = authorship.get("author", {})
                                        display_name = author.get("display_name", "")
                                        if display_name:
                                            authors.append(display_name)
                            
                            # ê²°ê³¼ ì¶”ê°€
                            primary_location = item.get("primary_location") or {}
                            source_info = {}
                            if primary_location and isinstance(primary_location, dict):
                                source_info = primary_location.get("source") or {}
                                if not isinstance(source_info, dict):
                                    source_info = {}
                            
                            results["results"].append({
                                "source": "openalex",
                                "title": item.get("title", ""),
                                "link": item.get("doi", ""),
                                "abstract": self._extract_abstract_from_openalex(item),
                                "authors": authors,
                                "year": item.get("publication_year", ""),
                                "journal": source_info.get("display_name", ""),
                                "raw_data": item  # ì›ë³¸ ë°ì´í„° ë³´ì¡´
                            })
                    except Exception as e:
                        logger.error(f"OpenAlex ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                
                elif search_source == "google":
                    # êµ¬ê¸€ ê²€ìƒ‰ (ì¼ë°˜)
                    try:
                        google_results = self.google_search(
                            query=optimized_query,
                            num_results=limit
                        )
                        
                        if google_results:
                            results["sources_used"].append("google")
                            
                            for item in google_results:
                                results["results"].append({
                                    "source": "google",
                                    "title": item.get("title", ""),
                                    "link": item.get("link", ""),
                                    "abstract": item.get("snippet", ""),
                                    "raw_data": item
                                })
                    except Exception as e:
                        logger.error(f"Google ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                
                elif search_source == "arxiv":
                    # arXiv ê²€ìƒ‰
                    try:
                        arxiv_results = self.search_arxiv(
                            query=optimized_query,
                            max_results=limit
                        )
                        
                        if arxiv_results:
                            results["sources_used"].append("arxiv")
                            
                            for item in arxiv_results:
                                results["results"].append({
                                    "source": "arxiv",
                                    "title": item.get("title", ""),
                                    "link": item.get("url", ""),
                                    "abstract": item.get("abstract", ""),
                                    "authors": item.get("authors", []),
                                    "year": item.get("published_date", "")[:4] if item.get("published_date") else "",
                                    "pdf_url": item.get("pdf_url", ""),
                                    "raw_data": item
                                })
                    except Exception as e:
                        logger.error(f"arXiv ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
                
                elif search_source == "crossref":
                    # Crossref ê²€ìƒ‰
                    try:
                        crossref_results = self.search_crossref(
                            query=optimized_query,
                            max_results=limit
                        )
                        
                        if crossref_results:
                            results["sources_used"].append("crossref")
                            
                            for item in crossref_results:
                                results["results"].append({
                                    "source": "crossref",
                                    "title": item.get("title", ""),
                                    "link": item.get("url", "") or item.get("doi", ""),
                                    "abstract": item.get("abstract", ""),
                                    "authors": item.get("authors", []),
                                    "year": item.get("published_date", "")[:4] if item.get("published_date") else "",
                                    "pdf_url": item.get("pdf_url", ""),
                                    "doi": item.get("doi", ""),
                                    "raw_data": item
                                })
                    except Exception as e:
                        logger.error(f"Crossref ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
            
            except Exception as e:
                logger.error(f"{search_source} ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
        
        return results
    
    def _extract_abstract_from_openalex(self, item):
        """OpenAlex ê²°ê³¼ì—ì„œ ì´ˆë¡ ì¶”ì¶œ"""
        if not item:
            logger.debug("_extract_abstract_from_openalex: itemì´ None ë˜ëŠ” ë¹ˆ ê°’")
            return ""
        
        if not isinstance(item, dict):
            logger.warning(f"_extract_abstract_from_openalex: itemì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(item)}")
            return ""
        
        abstract_inverted_index = item.get("abstract_inverted_index")
        if not abstract_inverted_index:
            return ""
        
        if not isinstance(abstract_inverted_index, dict):
            logger.warning(f"_extract_abstract_from_openalex: abstract_inverted_indexê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {type(abstract_inverted_index)}")
            return ""
        
        try:
            # inverted_indexì—ì„œ ë‹¨ì–´ì™€ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
            words = []
            positions = {}
            
            # ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ ìˆ˜ì§‘
            for word, indices in abstract_inverted_index.items():
                if not isinstance(indices, list):
                    continue
                    
                for position in indices:
                    if not isinstance(position, int):
                        continue
                    positions[position] = word
            
            # ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ë‹¨ì–´ ë°°ì—´
            for i in sorted(positions.keys()):
                words.append(positions[i])
            
            # ë‹¨ì–´ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„±
            abstract_text = " ".join(words)
            return abstract_text
        except Exception as e:
            logger.error(f"ì´ˆë¡ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return "ì´ˆë¡ ì¶”ì¶œ ì˜¤ë¥˜"
    
    def format_search_results(self, results: Dict[str, Any]) -> str:
        """
        í†µí•© ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        if not results or not results.get("results"):
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = f"ê²€ìƒ‰ì–´: {results['query']}\n"
        formatted += f"ì‚¬ìš©ëœ ì†ŒìŠ¤: {', '.join(results['sources_used'])}\n\n"
        
        for i, item in enumerate(results["results"], 1):
            # ì†ŒìŠ¤ë³„ ì•„ì´ì½˜
            source_icon = {
                "google_scholar": "ğŸ”",
                "openalex": "ğŸ“š",
                "google": "ğŸŒ",
                "arxiv": "ğŸ“‘",
                "crossref": "ğŸ”—"
            }.get(item["source"], "ğŸ“„")
            
            formatted += f"{i}. {source_icon} {item['title']}\n"
            
            # ì €ì
            if item.get("authors"):
                if isinstance(item["authors"], list):
                    formatted += f"   ì €ì: {', '.join(item['authors'])}\n"
                else:
                    formatted += f"   ì €ì: {item['authors']}\n"
            
            # ì¶œíŒ ì—°ë„
            if item.get("year"):
                formatted += f"   ì¶œíŒ ì—°ë„: {item['year']}\n"
            
            # ì €ë„
            if item.get("journal"):
                formatted += f"   ì €ë„: {item['journal']}\n"
            
            # ë§í¬
            if item.get("link"):
                formatted += f"   ë§í¬: {item['link']}\n"
            
            # DOI
            if item.get("doi"):
                formatted += f"   DOI: {item['doi']}\n"
            
            # ì´ˆë¡/ìŠ¤ë‹ˆí«
            if item.get("abstract"):
                # ì´ˆë¡ì´ ê¸¸ ê²½ìš° ì¤„ì„
                abstract = item["abstract"]
                if len(abstract) > 200:
                    abstract = abstract[:200] + "..."
                formatted += f"   ìš”ì•½: {abstract}\n"
            
            formatted += "\n"
        
        return formatted
    
    def get_citations_for_rag(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¸ìš© í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ë³€í™˜
        """
        citations = []
        
        for item in results.get("results", []):
            citation = {
                "title": item.get("title", ""),
                "authors": ", ".join(item.get("authors", [])) if isinstance(item.get("authors", []), list) else item.get("authors", ""),
                "year": str(item.get("year", "")),
                "source": item.get("journal", "") or item.get("source", ""),
                "link": item.get("link", ""),
                "snippet": item.get("abstract", ""),
                "citation_text": ""
            }
            
            # APA í˜•ì‹ì˜ ì¸ìš© í…ìŠ¤íŠ¸ ìƒì„±
            authors_text = citation["authors"]
            if authors_text:
                if "," in authors_text:  # ì—¬ëŸ¬ ì €ì
                    authors_text = authors_text.split(", ")[0] + " et al."
            
            citation["citation_text"] = f"{authors_text or 'Unknown'} ({citation['year'] or 'n.d.'}). {citation['title']}. {citation['source']}."
            
            citations.append(citation)
            
        return citations
    
    # ê¸°ì¡´ search.py ë° search_utils.pyì˜ í•¨ìˆ˜ë“¤ í†µí•©
    
    def google_search(self, query: str, num_results: int = 10, language: str = 'en') -> List[Dict[str, Any]]:
        """
        Google ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        """
        logger.info(f"google_search ë©”ì„œë“œ í˜¸ì¶œ: query={query}, num_results={num_results}, language={language}")
        
        # í˜¸ì¶œ ìŠ¤íƒ ì¶”ì ì„ ìœ„í•œ ë¡œê¹…
        import traceback
        call_stack = traceback.format_stack()
        logger.debug(f"google_search í˜¸ì¶œ ìŠ¤íƒ:\n{''.join(call_stack)}")
        
        if not self.google_api_key or not self.google_cse_id:
            logger.warning("Google API í‚¤ ë˜ëŠ” CSE IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ê²°ê³¼ ìˆ˜ ì œí•œ (API ì œí•œ)
        num_results = min(num_results, 10)
        
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": self.google_api_key,
            "cx": self.google_cse_id,
            "q": query,
            "num": num_results,
            "lr": f'lang_{language}'
        }
        
        try:
            logger.info(f"Google API í˜¸ì¶œ: {url}, ë§¤ê°œë³€ìˆ˜: {params}")
            response = requests.get(url, params=params)
            response.raise_for_status()
            
            results = response.json()
            
            if "items" not in results:
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤: {query}")
                return []
            
            formatted_results = []
            for item in results["items"]:
                formatted_results.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "source": "google"
                })
            
            logger.info(f"Google ê²€ìƒ‰ ê²°ê³¼: {len(formatted_results)}ê°œ í•­ëª© ë°˜í™˜")
            return formatted_results
        
        except Exception as e:
            logger.error(f"Google ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
            return []
    
    def search_arxiv(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """
        arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
        """
        logger.info(f"arXiv ê²€ìƒ‰: '{query}', ìµœëŒ€ {max_results}ê°œ ê²°ê³¼")
        
        try:
            # arXiv API ê²€ìƒ‰
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            results = []
            for paper in search.results():
                authors = [author.name for author in paper.authors]
                
                result = {
                    'title': paper.title,
                    'authors': authors,
                    'abstract': paper.summary,
                    'url': paper.entry_id,
                    'pdf_url': paper.pdf_url,
                    'published_date': paper.published.strftime('%Y-%m-%d'),
                    'source': 'arXiv'
                }
                
                results.append(result)
            
            logger.info(f"arXiv ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸ ì°¾ìŒ")
            return results
            
        except Exception as e:
            logger.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def search_crossref(self, query: str, max_results: int = 10, filter: str = None) -> List[Dict[str, Any]]:
        """
        Crossref APIë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            filter: ì¶”ê°€ í•„í„° ì˜µì…˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
        """
        logger.info(f"Crossref ê²€ìƒ‰: '{query}', ìµœëŒ€ {max_results}ê°œ ê²°ê³¼")
        
        try:
            # Crossref API v1 ê²½ë¡œ ì‚¬ìš©
            url = "https://api.crossref.org/v1/works"
            params = {
                "query": query,
                "rows": max_results,
                "sort": "relevance",
                "select": "DOI,title,author,abstract,URL,published-print,published-online"
            }
            
            if filter:
                params["filter"] = filter
            
            # ì‚¬ìš©ì ì—ì´ì „íŠ¸ í—¤ë” ì¶”ê°€ (Crossref ê¶Œì¥)
            headers = {"User-Agent": "ResearchAgent/1.0 (mailto:example@example.com)"}
            
            response = requests.get(url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            results = []
            if "message" in data and "items" in data["message"]:
                for item in data["message"]["items"]:
                    # ë°ì´í„° ì¶”ì¶œ ë¡œì§ - ê¸°ì¡´ê³¼ ë™ì¼
                    title = item.get("title", [""])[0] if "title" in item and len(item["title"]) > 0 else ""
                    
                    # ì €ì ì •ë³´ ì¶”ì¶œ
                    authors = []
                    if "author" in item:
                        for author in item["author"]:
                            name_parts = []
                            if "given" in author:
                                name_parts.append(author["given"])
                            if "family" in author:
                                name_parts.append(author["family"])
                            
                            if name_parts:
                                authors.append(" ".join(name_parts))
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    published_date = ""
                    if "published-print" in item and "date-parts" in item["published-print"]:
                        date_parts = item["published-print"]["date-parts"][0]
                        if len(date_parts) >= 1:
                            published_date = str(date_parts[0])
                    elif "published-online" in item and "date-parts" in item["published-online"]:
                        date_parts = item["published-online"]["date-parts"][0]
                        if len(date_parts) >= 1:
                            published_date = str(date_parts[0])
                    
                    # URL ë° DOI ì¶”ì¶œ
                    url = item.get("URL", "")
                    doi = item.get("DOI", "")
                    
                    # PDF URL ì¶”ì •
                    pdf_url = f"https://doi.org/{doi}" if doi else url
                    
                    # ì¶”ìƒ ì¶”ì¶œ
                    abstract = ""
                    if "abstract" in item:
                        abstract = item["abstract"]
                    
                    result = {
                        'title': title,
                        'authors': authors,
                        'abstract': abstract,
                        'url': url,
                        'doi': doi,
                        'published_date': published_date,
                        'pdf_url': pdf_url,
                        'source': 'Crossref'
                    }
                    
                    results.append(result)
            
            logger.info(f"Crossref ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ë…¼ë¬¸ ì°¾ìŒ")
            return results
            
        except Exception as e:
            logger.error(f"Crossref ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def format_results_as_markdown(self, results: Dict[str, Any]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        """
        if not results or not results.get("results"):
            return "# ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        markdown = f"# '{results['query']}' ê²€ìƒ‰ ê²°ê³¼\n\n"
        markdown += f"**ì‚¬ìš©ëœ ì†ŒìŠ¤**: {', '.join(results['sources_used'])}\n\n"
        
        for i, item in enumerate(results["results"], 1):
            source_name = {
                "google_scholar": "Google Scholar",
                "openalex": "OpenAlex",
                "google": "Google",
                "arxiv": "arXiv",
                "crossref": "Crossref"
            }.get(item["source"], item["source"])
            
            markdown += f"## {i}. {item['title']}\n\n"
            markdown += f"**ì¶œì²˜**: {source_name}\n\n"
            
            if item.get("authors"):
                if isinstance(item["authors"], list):
                    authors_text = ", ".join(item["authors"])
                else:
                    authors_text = str(item["authors"])
                markdown += f"**ì €ì**: {authors_text}\n\n"
            
            if item.get("year"):
                markdown += f"**ì¶œíŒ ì—°ë„**: {item['year']}\n\n"
            
            if item.get("journal"):
                markdown += f"**ì €ë„**: {item['journal']}\n\n"
            
            if item.get("abstract"):
                markdown += f"**ìš”ì•½**:\n\n> {item['abstract']}\n\n"
            
            if item.get("link"):
                markdown += f"**ë§í¬**: [{item['link']}]({item['link']})\n\n"
            
            markdown += "---\n\n"
        
        return markdown
    
    # search.pyì˜ test_academic_search í•¨ìˆ˜ë„ í†µí•©
    def test_academic_search(self, query: str, max_results: int = 5) -> str:
        """
        í•™ìˆ  ê²€ìƒ‰ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ í•¨ìˆ˜
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            str: í¬ë§·íŒ…ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        # í†µí•© ê²€ìƒ‰ ì‹¤í–‰
        results = self.search(
            query=query,
            source="all",
            limit=max_results
        )
        
        # ê²°ê³¼ í¬ë§·íŒ… (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)
        formatted_results = self.format_results_as_markdown(results)
        
        return formatted_results

    # ê¸°ì¡´ ë©”ì„œë“œ ì™¸ì— í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ ì¶”ê°€
    def google_search_compat(self, query: str, num_results: int = 10, language: str = 'en') -> list:
        """
        êµ¬ê¸€ ê²€ìƒ‰ì„ ìˆ˜í–‰ (í˜¸í™˜ì„± ìœ ì§€ìš©)
        """
        logger.info(f"google_search_compat ë©”ì„œë“œ í˜¸ì¶œ: query={query}, num_results={num_results}, language={language}")
        
        # í˜¸ì¶œ ìŠ¤íƒ ì¶”ì ì„ ìœ„í•œ ë¡œê¹…
        import traceback
        call_stack = traceback.format_stack()
        logger.debug(f"google_search_compat í˜¸ì¶œ ìŠ¤íƒ:\n{''.join(call_stack)}")
        
        try:
            # ë¬´í•œ ì¬ê·€ ë°©ì§€: search ë©”ì„œë“œ ëŒ€ì‹  ì§ì ‘ google_search ë©”ì„œë“œ í˜¸ì¶œ
            results = self.google_search(
                query=query,
                num_results=num_results,
                language=language
            )
            
            # ì›ë˜ ë°˜í™˜ í˜•ì‹ì— ë§ê²Œ ì¡°ì •
            logger.info(f"google_search_compat ê²°ê³¼: {len(results)}ê°œ í•­ëª© ë°˜í™˜")
            return results
        except Exception as e:
            logger.error(f"google_search_compat ì˜¤ë¥˜: {str(e)}")
            logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
            return []

    def search_with_fallback(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‹œë„í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹¨
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ê° ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            Dict: í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        logger.info(f"search_with_fallback ë©”ì„œë“œ í˜¸ì¶œ: query={query}, limit={limit}")
        
        # ê²°ê³¼ ì´ˆê¸°í™”
        combined_results = {
            "query": query,
            "sources_used": [],
            "results": []
        }
        
        # ëª¨ë“  ì†ŒìŠ¤ ë…ë¦½ì ìœ¼ë¡œ ì‹œë„
        sources_to_try = ["google", "openalex", "arxiv", "crossref"]
        
        for source in sources_to_try:
            try:
                logger.info(f"{source} ê²€ìƒ‰ ì‹œë„ ì¤‘...")
                source_results = self.search(query=query, source=source, limit=limit)
                
                # ê²°ê³¼ê°€ ìˆìœ¼ë©´ í†µí•©
                if source_results.get("results"):
                    logger.info(f"{source} ê²€ìƒ‰ ê²°ê³¼: {len(source_results['results'])}ê°œ í•­ëª©")
                    combined_results["sources_used"].extend(source_results["sources_used"])
                    combined_results["results"].extend(source_results["results"])
                else:
                    logger.info(f"{source} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            except Exception as e:
                logger.error(f"{source} ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                logger.error(f"ì˜¤ë¥˜ ìƒì„¸ ì •ë³´: {traceback.format_exc()}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë‹¤ìŒ ì†ŒìŠ¤ë¡œ ê³„ì† ì§„í–‰
        
        # ì¤‘ë³µ ì†ŒìŠ¤ ì œê±°
        combined_results["sources_used"] = list(set(combined_results["sources_used"]))
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¡œê·¸ ë‚¨ê¹€
        if not combined_results["results"]:
            logger.warning(f"ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}")
        
        return combined_results

    def _optimize_query(self, query: str) -> str:
        """
        ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•˜ì—¬ API í˜¸ì¶œì— ì í•©í•˜ê²Œ ë³€í™˜
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            
        Returns:
            ìµœì í™”ëœ ì¿¼ë¦¬
        """
        # ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¸¸ë©´ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
        if len(query) > 150:
            logger.info(f"ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(query)}ì). ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            
            # 1. ë¶ˆí•„ìš”í•œ ì§€ì‹œë¬¸ì´ë‚˜ ì„¤ëª… ì œê±°
            patterns_to_remove = [
                r"When summarizing as an academic review,.*",
                r"In particular, organize the discussion.*",
                r"cite the references used.*",
                r"include a bibliography.*",
                r"This search query would.*",
                r"Please provide.*",
                r"I need information about.*",
                r"I'm looking for.*",
                r"Can you find.*",
                r"I want to learn about.*"
            ]
            
            optimized = query
            for pattern in patterns_to_remove:
                optimized = re.sub(pattern, "", optimized, flags=re.IGNORECASE)
            
            # 2. ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ë²• ê°œì„ 
            important_keywords = []
            
            # 2.1 ëª…ì‚¬êµ¬ ì¶”ì¶œ (2-3ë‹¨ì–´ë¡œ êµ¬ì„±ëœ êµ¬ë¬¸)
            noun_phrases = re.findall(r'\b[A-Za-z][\w-]*(?:\s+[A-Za-z][\w-]*){1,2}\b', optimized)
            
            # 2.2 ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë‚˜ ë”°ì˜´í‘œ ì•ˆì˜ êµ¬ë¬¸ (ê¸°ì¡´ ë°©ì‹)
            capitalized_words = re.findall(r'\b[A-Z][a-zA-Z]*\b', optimized)
            quoted_phrases = re.findall(r'"([^"]*)"', optimized)
            
            # 2.3 ìˆ«ìê°€ í¬í•¨ëœ ì¤‘ìš” ì‹ë³„ì (ë²„ì „ ë²ˆí˜¸ ë“±)
            identifiers = re.findall(r'\b[A-Za-z]+[\d]+[\w]*\b', optimized)
            
            # 2.4 í•˜ì´í”ˆìœ¼ë¡œ ì—°ê²°ëœ ë³µí•©ì–´
            compound_words = re.findall(r'\b[\w]+-[\w]+\b', optimized)
            
            # ëª¨ë“  ì¶”ì¶œ ê²°ê³¼ í•©ì¹˜ê¸°
            important_keywords.extend(noun_phrases)
            important_keywords.extend(capitalized_words)
            important_keywords.extend(quoted_phrases)
            important_keywords.extend(identifiers)
            important_keywords.extend(compound_words)
            
            # 3. í•™ìˆ  ê²€ìƒ‰ì— ì¤‘ìš”í•œ íŠ¹ì • í‚¤ì›Œë“œ ì¶”ê°€
            academic_terms = [
                "model", "models", "comparison", "analysis", "review", 
                "topic modeling", "LLM", "LLMs", "large language model",
                "algorithm", "method", "approach", "framework",
                "research", "study", "experiment", "evaluation",
                "dataset", "data", "results", "findings",
                "neural", "network", "deep learning", "machine learning",
                "artificial intelligence", "AI", "NLP", "natural language processing",
                "transformer", "attention", "embedding", "fine-tuning",
                "pre-training", "training", "inference", "performance",
                "accuracy", "precision", "recall", "F1",
                "benchmark", "state-of-the-art", "SOTA"
            ]
            
            for term in academic_terms:
                if term.lower() in optimized.lower() and term not in important_keywords:
                    important_keywords.append(term)
            
            # 4. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ì¤‘ë³µ ì œê±°
            unique_keywords = []
            lowercase_set = set()
            
            for keyword in important_keywords:
                if keyword.lower() not in lowercase_set and len(keyword) > 2:  # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ ì œì™¸
                    unique_keywords.append(keyword)
                    lowercase_set.add(keyword.lower())
            
            # 5. í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì§€ì • (ë” ê¸´ êµ¬ë¬¸ ìš°ì„ )
            unique_keywords.sort(key=len, reverse=True)
            
            # 6. ìµœì¢… ì¿¼ë¦¬ ìƒì„± (ë„ˆë¬´ ë§ì€ í‚¤ì›Œë“œëŠ” ì œí•œ)
            if unique_keywords:
                # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œë¡œ ì œí•œ
                final_keywords = unique_keywords[:10]
                final_query = " ".join(final_keywords)
                logger.info(f"ìµœì í™”ëœ ì¿¼ë¦¬: {final_query}")
                return final_query
        
        # ì¿¼ë¦¬ê°€ ì¶©ë¶„íˆ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return query 